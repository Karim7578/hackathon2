{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Module Core - core_modules.py"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    \"\"\"Structure pour les r√©sultats de pr√©diction\"\"\"\n",
        "    text: str\n",
        "    predicted_label: str\n",
        "    confidence: float\n",
        "    all_scores: Dict[str, float]\n",
        "    context: Optional[List[str]] = None\n",
        "    processing_time: float = 0.0\n",
        "\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralis√©e\"\"\"\n",
        "    def __init__(self):\n",
        "        self.model_name = \"distilbert-base-uncased\"\n",
        "        self.max_length = 256\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 2e-4\n",
        "        self.epochs = 3\n",
        "        self.lora_r = 16\n",
        "        self.lora_alpha = 32\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            'model_name': self.model_name,\n",
        "            'max_length': self.max_length,\n",
        "            'batch_size': self.batch_size,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'epochs': self.epochs,\n",
        "            'device': str(self.device),\n",
        "            'lora_config': {'r': self.lora_r, 'alpha': self.lora_alpha}\n",
        "        }"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c7aa45-eabc-4632-fbbb-8e20d4d56006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Module Data Processing - data_modules.py"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "\n",
        "# data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Gestion centralis√©e du traitement des donn√©es avec gestion robuste des erreurs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        \"\"\"D√©tection automatique des colonnes texte et label avec validation\"\"\"\n",
        "        print(f\"üîç D√©tection des colonnes sur {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
        "        print(f\"üìã Colonnes disponibles: {list(df.columns)}\")\n",
        "\n",
        "        text_keywords = ['self_text', 'text', 'content', 'message', 'comment', 'body', 'description']\n",
        "        label_keywords = ['comment_sentiment', 'sentiment', 'label', 'category', 'class', 'target']\n",
        "\n",
        "        # Recherche intelligente\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        # Recherche par mots-cl√©s\n",
        "        for col in df.columns:\n",
        "            col_lower = str(col).lower()\n",
        "\n",
        "            # Recherche colonne texte\n",
        "            if not text_col:\n",
        "                for keyword in text_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        text_col = col\n",
        "                        break\n",
        "\n",
        "            # Recherche colonne label\n",
        "            if not label_col:\n",
        "                for keyword in label_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        label_col = col\n",
        "                        break\n",
        "\n",
        "        # Fallback intelligent pour la colonne texte\n",
        "        if not text_col:\n",
        "            string_cols = []\n",
        "            for col in df.columns:\n",
        "                try:\n",
        "                    # V√©rifier si la colonne contient principalement du texte\n",
        "                    sample = df[col].dropna().head(100)\n",
        "                    if len(sample) > 0:\n",
        "                        # Convertir en string et calculer la longueur moyenne\n",
        "                        sample_str = sample.astype(str)\n",
        "                        avg_length = sample_str.str.len().mean()\n",
        "                        if avg_length > 10:  # Textes probablement plus longs que 10 caract√®res\n",
        "                            string_cols.append((col, avg_length))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if string_cols:\n",
        "                # Prendre la colonne avec le texte le plus long en moyenne\n",
        "                text_col = max(string_cols, key=lambda x: x[1])[0]\n",
        "            else:\n",
        "                # Last resort: premi√®re colonne object\n",
        "                object_cols = df.select_dtypes(include=['object']).columns\n",
        "                if len(object_cols) > 0:\n",
        "                    text_col = object_cols[0]\n",
        "\n",
        "        # Fallback pour la colonne label\n",
        "        if not label_col:\n",
        "            # Chercher une colonne avec peu de valeurs uniques (potentiel label)\n",
        "            for col in df.columns:\n",
        "                if col != text_col:\n",
        "                    try:\n",
        "                        unique_count = df[col].nunique()\n",
        "                        total_count = len(df[col].dropna())\n",
        "                        if total_count > 0 and unique_count < min(20, total_count * 0.1):\n",
        "                            label_col = col\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Si toujours pas trouv√©, prendre la derni√®re colonne\n",
        "            if not label_col:\n",
        "                label_col = df.columns[-1]\n",
        "\n",
        "        print(f\"‚úÖ Colonnes d√©tect√©es: Text='{text_col}', Label='{label_col}'\")\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text_column(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Nettoyage robuste d'une colonne texte\"\"\"\n",
        "        try:\n",
        "            # Convertir en string d'abord\n",
        "            cleaned = series.astype(str)\n",
        "\n",
        "            # Remplacer les valeurs probl√©matiques\n",
        "            cleaned = cleaned.replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "            # Supprimer les espaces\n",
        "            cleaned = cleaned.str.strip()\n",
        "\n",
        "            # Remplacer les cha√Ænes vides par NaN\n",
        "            cleaned = cleaned.replace('', pd.NA)\n",
        "\n",
        "            return cleaned\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur nettoyage texte: {e}\")\n",
        "            # Fallback: conversion simple\n",
        "            return series.astype(str)\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        \"\"\"Pr√©paration des datasets avec validation robuste\"\"\"\n",
        "\n",
        "        print(f\"üìä Pr√©paration des datasets - Taille originale: {df.shape}\")\n",
        "\n",
        "        # D√©tection des colonnes\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "\n",
        "        if not self.text_col or not self.label_col:\n",
        "            raise ValueError(f\"‚ùå Impossible de d√©tecter les colonnes: text='{self.text_col}', label='{self.label_col}'\")\n",
        "\n",
        "        # Extraction et copie des colonnes n√©cessaires\n",
        "        try:\n",
        "            df_work = df[[self.text_col, self.label_col]].copy()\n",
        "        except KeyError as e:\n",
        "            print(f\"‚ùå Colonnes manquantes: {e}\")\n",
        "            print(f\"Colonnes disponibles: {list(df.columns)}\")\n",
        "            raise\n",
        "\n",
        "        # Renommer les colonnes\n",
        "        df_work.columns = ['text', 'label']\n",
        "\n",
        "        print(f\"üìã Avant nettoyage: {len(df_work)} lignes\")\n",
        "\n",
        "        # Nettoyage robuste des donn√©es\n",
        "        # 1. Nettoyage de la colonne texte\n",
        "        df_work['text'] = self.clean_text_column(df_work['text'])\n",
        "\n",
        "        # 2. Nettoyage de la colonne label\n",
        "        df_work['label'] = df_work['label'].astype(str).str.strip()\n",
        "        df_work['label'] = df_work['label'].replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "        # 3. Suppression des lignes avec des valeurs manquantes\n",
        "        initial_size = len(df_work)\n",
        "        df_work = df_work.dropna()\n",
        "        print(f\"üßπ Apr√®s suppression des NaN: {len(df_work)} lignes (supprim√©: {initial_size - len(df_work)})\")\n",
        "\n",
        "        # 4. Filtrage des textes trop courts (de mani√®re s√©curis√©e)\n",
        "        try:\n",
        "            # V√©rifier que nous avons bien des strings\n",
        "            df_work['text'] = df_work['text'].astype(str)\n",
        "\n",
        "            # Filtrer les textes trop courts\n",
        "            mask = df_work['text'].str.len() > 5\n",
        "            df_work = df_work[mask]\n",
        "            print(f\"üìù Apr√®s filtrage textes courts: {len(df_work)} lignes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du filtrage des textes: {e}\")\n",
        "            # Continuer sans filtrage si erreur\n",
        "\n",
        "        # V√©rification finale\n",
        "        if len(df_work) == 0:\n",
        "            raise ValueError(\"‚ùå Aucune donn√©e valide apr√®s nettoyage!\")\n",
        "\n",
        "        # 5. √âchantillonnage si n√©cessaire\n",
        "        if len(df_work) > sample_size:\n",
        "            df_work = df_work.sample(n=sample_size, random_state=42)\n",
        "            print(f\"üéØ √âchantillonnage √† {sample_size} lignes\")\n",
        "\n",
        "        # 6. Mapping des labels\n",
        "        unique_labels = sorted(df_work['label'].unique())\n",
        "        print(f\"üè∑Ô∏è Labels uniques trouv√©s: {unique_labels}\")\n",
        "\n",
        "        self.label_mapping = {str(label): idx for idx, label in enumerate(unique_labels)}\n",
        "        df_work['label_id'] = df_work['label'].astype(str).map(self.label_mapping)\n",
        "\n",
        "        # V√©rification du mapping\n",
        "        if df_work['label_id'].isna().any():\n",
        "            print(\"‚ö†Ô∏è Probl√®me de mapping des labels d√©tect√©\")\n",
        "            print(f\"Labels non mapp√©s: {df_work[df_work['label_id'].isna()]['label'].unique()}\")\n",
        "\n",
        "        print(f\"üìä Mapping des labels: {self.label_mapping}\")\n",
        "\n",
        "        # 7. Splits stratifi√©s\n",
        "        try:\n",
        "            # V√©rifier si on peut faire une stratification\n",
        "            if len(unique_labels) > 1 and all(df_work['label_id'].value_counts() >= 2):\n",
        "                stratify_col = df_work['label_id']\n",
        "                print(\"‚úÖ Stratification activ√©e\")\n",
        "            else:\n",
        "                stratify_col = None\n",
        "                print(\"‚ö†Ô∏è Pas de stratification (pas assez d'exemples par classe)\")\n",
        "\n",
        "            # Premier split: train vs (val + test)\n",
        "            train_df, temp_df = train_test_split(\n",
        "                df_work,\n",
        "                test_size=0.4,\n",
        "                random_state=42,\n",
        "                stratify=stratify_col if stratify_col is not None else None\n",
        "            )\n",
        "\n",
        "            # Deuxi√®me split: val vs test\n",
        "            if stratify_col is not None:\n",
        "                temp_stratify = temp_df['label_id']\n",
        "            else:\n",
        "                temp_stratify = None\n",
        "\n",
        "            val_df, test_df = train_test_split(\n",
        "                temp_df,\n",
        "                test_size=0.5,\n",
        "                random_state=42,\n",
        "                stratify=temp_stratify if temp_stratify is not None else None\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du split: {e}\")\n",
        "            # Fallback: split simple\n",
        "            train_size = int(0.6 * len(df_work))\n",
        "            val_size = int(0.2 * len(df_work))\n",
        "\n",
        "            train_df = df_work[:train_size]\n",
        "            val_df = df_work[train_size:train_size+val_size]\n",
        "            test_df = df_work[train_size+val_size:]\n",
        "\n",
        "        print(f\"üìä Splits finaux: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "        # 8. Conversion en Dataset\n",
        "        try:\n",
        "            train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']].reset_index(drop=True))\n",
        "\n",
        "            print(\"‚úÖ Datasets cr√©√©s avec succ√®s\")\n",
        "\n",
        "            return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation des datasets: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques du processeur de donn√©es\"\"\"\n",
        "        return {\n",
        "            \"text_column\": self.text_col,\n",
        "            \"label_column\": self.label_col,\n",
        "            \"label_mapping\": self.label_mapping,\n",
        "            \"num_labels\": len(self.label_mapping)\n",
        "        }\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Validation d'un DataFrame\"\"\"\n",
        "        try:\n",
        "            if df is None or df.empty:\n",
        "                print(\"‚ùå DataFrame vide ou None\")\n",
        "                return False\n",
        "\n",
        "            if len(df.columns) < 2:\n",
        "                print(\"‚ùå DataFrame doit avoir au moins 2 colonnes\")\n",
        "                return False\n",
        "\n",
        "            print(f\"‚úÖ DataFrame valide: {df.shape}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur validation DataFrame: {e}\")\n",
        "            return False"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72fb45e-0cae-45e9-ddf3-ac151e37757a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Module Mod√®le - model_modules.py"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "# model_modules.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from typing import Dict, Any\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "class ModelManager:\n",
        "    \"\"\"Gestion du mod√®le et de l'entra√Ænement avec logs avanc√©s\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.peft_model = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        \"\"\"Configuration du tokenizer\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        return self.tokenizer\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        \"\"\"Configuration du mod√®le avec LoRA\"\"\"\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=self.config.lora_r,\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_lin\", \"v_lin\"] if \"distilbert\" in self.config.model_name.lower() else [\"query\", \"value\"]\n",
        "        )\n",
        "\n",
        "        self.peft_model = get_peft_model(base_model, lora_config)\n",
        "        return self.peft_model\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"Tokenisation des exemples\"\"\"\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.config.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def setup_training_args(self, output_dir=\"outputs/runs\"):\n",
        "        \"\"\"Configuration compl√®te des TrainingArguments avec logs\"\"\"\n",
        "        return TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            logging_dir=output_dir + \"/logs\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"accuracy\",\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            gradient_checkpointing=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=2,\n",
        "            logging_steps=50,\n",
        "            eval_steps=1,\n",
        "            save_steps=1\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        \"\"\"Configuration du trainer avec arguments optimis√©s\"\"\"\n",
        "        training_args = self.setup_training_args()\n",
        "\n",
        "        self.trainer = Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics\n",
        "        )\n",
        "        return self.trainer\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_metrics(eval_pred):\n",
        "        \"\"\"Calcul des m√©triques\"\"\"\n",
        "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = torch.argmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, predictions),\n",
        "            \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
        "            \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
        "            \"precision\": precision_score(labels, predictions, average=\"weighted\"),\n",
        "            \"recall\": recall_score(labels, predictions, average=\"weighted\")\n",
        "        }"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c4a219-9f54-4e97-e0fe-8d65fc3cf04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. visualization_modules.py"
      ],
      "metadata": {
        "id": "qYJhFun3Mws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestion des visualisations d'entra√Ænement et d'√©valuation\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str):\n",
        "        \"\"\"Affiche les courbes d'entra√Ænement depuis les logs\"\"\"\n",
        "        try:\n",
        "            # Chemin vers le fichier de logs\n",
        "            log_file = Path(log_dir) / \"trainer_state.json\"\n",
        "\n",
        "            if not log_file.exists():\n",
        "                st.warning(\"Fichier de logs non trouv√©\")\n",
        "                return\n",
        "\n",
        "            # Chargement des logs\n",
        "            with open(log_file, 'r') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            # Extraction des m√©triques\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"Aucune donn√©e d'entra√Ænement trouv√©e\")\n",
        "                return\n",
        "\n",
        "            # Pr√©paration des donn√©es\n",
        "            epochs = []\n",
        "            train_loss = []\n",
        "            eval_loss = []\n",
        "            eval_accuracy = []\n",
        "            learning_rates = []\n",
        "\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_accuracy.append(entry.get('eval_accuracy', 0))\n",
        "                    learning_rates.append(entry.get('learning_rate', 0))\n",
        "                elif 'loss' in entry:\n",
        "                    train_loss.append(entry.get('loss', 0))\n",
        "\n",
        "            # Cr√©ation des graphiques\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle('üìà √âvolution de l\\'entra√Ænement', fontsize=16)\n",
        "\n",
        "            # Loss\n",
        "            if train_loss:\n",
        "                axes[0, 0].plot(range(len(train_loss)), train_loss, 'b-', label='Train Loss', marker='o')\n",
        "            if eval_loss:\n",
        "                axes[0, 0].plot(epochs[:len(eval_loss)], eval_loss, 'r-', label='Eval Loss', marker='s')\n",
        "            axes[0, 0].set_title('Perte (Loss)')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Accuracy\n",
        "            if eval_accuracy:\n",
        "                axes[0, 1].plot(epochs[:len(eval_accuracy)], eval_accuracy, 'g-', label='Accuracy', marker='^')\n",
        "            axes[0, 1].set_title('Pr√©cision')\n",
        "            axes[0, 1].set_xlabel('Epoch')\n",
        "            axes[0, 1].set_ylabel('Accuracy')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # Learning Rate\n",
        "            if learning_rates:\n",
        "                axes[1, 0].plot(epochs[:len(learning_rates)], learning_rates, 'orange', marker='d')\n",
        "            axes[1, 0].set_title('Learning Rate')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('LR')\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # R√©sum√©\n",
        "            if eval_accuracy:\n",
        "                axes[1, 1].text(0.1, 0.5,\n",
        "                               f\"Derni√®re pr√©cision: {eval_accuracy[-1]:.4f}\\n\"\n",
        "                               f\"Meilleure pr√©cision: {max(eval_accuracy):.4f}\\n\"\n",
        "                               f\"Epoch: {len(epochs)}\",\n",
        "                               fontsize=12, verticalalignment='center')\n",
        "            axes[1, 1].set_title('R√©sum√©')\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur lors de l'affichage des courbes: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names):\n",
        "        \"\"\"Affiche la matrice de confusion\"\"\"\n",
        "        try:\n",
        "            # Pr√©dictions\n",
        "            preds_output = trainer.predict(test_dataset)\n",
        "            preds = preds_output.predictions.argmax(axis=1)\n",
        "            labels = preds_output.label_ids\n",
        "\n",
        "            # Calcul de la matrice\n",
        "            cm = confusion_matrix(labels, preds)\n",
        "\n",
        "            # Affichage avec seaborn\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax)\n",
        "            ax.set_title('Matrice de Confusion')\n",
        "            ax.set_xlabel('Pr√©dictions')\n",
        "            ax.set_ylabel('Vraies √©tiquettes')\n",
        "\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Rapport de classification\n",
        "            st.subheader(\"üìä Rapport de Classification\")\n",
        "            report = classification_report(labels, preds, target_names=label_names, output_dict=True)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.dataframe(report_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur matrice de confusion: {e}\")"
      ],
      "metadata": {
        "id": "oMZt-WwIMwez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. qa_modules.py"
      ],
      "metadata": {
        "id": "1HSXVYNkNc-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import streamlit as st\n",
        "\n",
        "class QAModule:\n",
        "    \"\"\"Module de recherche et Q&A bas√© sur sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"Initialisation avec mod√®le d'embedding\"\"\"\n",
        "        try:\n",
        "            self.encoder = SentenceTransformer(model_name)\n",
        "            self.corpus_embeddings = None\n",
        "            self.corpus_texts = []\n",
        "            self.labels = []\n",
        "            self.model_name = model_name\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur chargement mod√®le Q&A: {e}\")\n",
        "            # Fallback\n",
        "            self.encoder = None\n",
        "            self.model_name = \"fallback\"\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        \"\"\"Indexe le dataset pour la recherche\"\"\"\n",
        "        if self.encoder is None:\n",
        "            st.warning(\"Module Q&A non disponible\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.corpus_texts = [item['text'] for item in dataset]\n",
        "            self.labels = [item['label_id'] for item in dataset]\n",
        "\n",
        "            with st.spinner(\"üìä Indexation des donn√©es pour la recherche...\"):\n",
        "                self.corpus_embeddings = self.encoder.encode(\n",
        "                    self.corpus_texts,\n",
        "                    convert_to_tensor=False,\n",
        "                    show_progress_bar=True\n",
        "                )\n",
        "            st.success(f\"‚úÖ {len(self.corpus_texts)} √©l√©ments index√©s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur indexation Q&A: {e}\")\n",
        "\n",
        "    def query(self, question: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Recherche les textes les plus similaires √† la question\"\"\"\n",
        "        if self.encoder is None or self.corpus_embeddings is None:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            question_embedding = self.encoder.encode([question], convert_to_tensor=False)\n",
        "            similarities = cosine_similarity(question_embedding, self.corpus_embeddings)[0]\n",
        "\n",
        "            # Top K indices\n",
        "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "            results = []\n",
        "            for idx in top_indices:\n",
        "                results.append({\n",
        "                    \"text\": self.corpus_texts[idx],\n",
        "                    \"label_id\": int(self.labels[idx]),\n",
        "                    \"score\": float(similarities[idx]),\n",
        "                    \"rank\": len(results) + 1\n",
        "                })\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur recherche Q&A: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Statistiques du module Q&A\"\"\"\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"indexed_items\": len(self.corpus_texts),\n",
        "            \"embedding_dim\": len(self.corpus_embeddings[0]) if self.corpus_embeddings else 0\n",
        "        }"
      ],
      "metadata": {
        "id": "CdRC_3s7Ncy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Module Knowledge Base - knowledge_modules.py"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le r√©chauffement climatique est principalement caus√© par les √©missions de gaz √† effet de serre d'origine humaine.\",\n",
        "            \"Les √©nergies renouvelables comme le solaire et l'√©olien sont essentielles pour d√©carboner notre √©conomie.\",\n",
        "            \"La d√©forestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports repr√©sente environ 24% des √©missions mondiales de gaz √† effet de serre.\",\n",
        "            \"L'am√©lioration de l'efficacit√© √©nerg√©tique des b√¢timents peut r√©duire jusqu'√† 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et r√©g√©n√©ratrice peut s√©questrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les oc√©ans absorbent 25% du CO2 atmosph√©rique mais s'acidifient, mena√ßant les √©cosyst√®mes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises √† r√©duire leurs √©missions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'att√©nuation des √©missions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralit√© carbone.\"\n",
        "        ]\n",
        "        print(\"‚úÖ Base de connaissances initialis√©e avec recherche par mots-cl√©s\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarit√© textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarit√© bas√© sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score d√©croissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"‚úÖ Nouvelle connaissance ajout√©e: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa60614e-84cf-4779-a197-d888766a67e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5Ô∏è‚É£ Module Streamlit - streamlit_app.py"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "\n",
        "# streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Import des modules\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig, PredictionResult\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from knowledge_modules import KnowledgeBase\n",
        "from visualization_modules import VisualizationManager\n",
        "from qa_modules import QAModule\n",
        "\n",
        "# Configuration Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"üåç Climate Analyzer - Pipeline Complet\",\n",
        "    page_icon=\"üåç\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# CSS personnalis√©\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        padding: 2rem;\n",
        "        border-radius: 15px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "        box-shadow: 0 10px 30px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .metric-card {\n",
        "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 12px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        margin: 1rem 0;\n",
        "        box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n",
        "    }\n",
        "    .stage-card {\n",
        "        background: #f8f9fa;\n",
        "        border-left: 4px solid #007bff;\n",
        "        padding: 1rem;\n",
        "        border-radius: 8px;\n",
        "        margin: 1rem 0;\n",
        "    }\n",
        "    .success-box {\n",
        "        background: #d4edda;\n",
        "        border: 1px solid #c3e6cb;\n",
        "        border-radius: 8px;\n",
        "        padding: 1rem;\n",
        "        margin: 1rem 0;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    \"\"\"Application Streamlit principale\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "        self.knowledge_base = KnowledgeBase()\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.qa_module = QAModule()\n",
        "        self.trained = False\n",
        "        self.trainer = None\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Ex√©cution principale de l'application\"\"\"\n",
        "\n",
        "        # Header\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"main-header\">\n",
        "            <h1>üåç Climate Sentiment Analyzer</h1>\n",
        "            <h3>Pipeline Complet avec Visualisations et Q&A</h3>\n",
        "            <p>Analyse de sentiment climatique avec entra√Ænement, courbes et recherche intelligente</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Sidebar\n",
        "        st.sidebar.header(\"üéõÔ∏è Navigation\")\n",
        "\n",
        "        # Navigation principale\n",
        "        app_mode = st.sidebar.selectbox(\n",
        "            \"Mode d'application\",\n",
        "            [\"üöÄ Pipeline Complet\", \"üìä Data Processing\", \"ü§ñ Mod√®le\", \"üîç Analyse\", \"‚ùì Q&A\", \"üìà Visualisations\"]\n",
        "        )\n",
        "\n",
        "        # Configuration rapide\n",
        "        with st.sidebar.expander(\"‚öôÔ∏è Configuration\"):\n",
        "            sample_size = st.slider(\"Taille √©chantillon\", 1000, 10000, 4000)\n",
        "            epochs = st.slider(\"Epochs\", 1, 5, 3)\n",
        "            show_details = st.checkbox(\"D√©tails d'ex√©cution\", True)\n",
        "\n",
        "        # Ex√©cution selon le mode\n",
        "        if app_mode == \"üöÄ Pipeline Complet\":\n",
        "            self.run_complete_pipeline(sample_size, epochs, show_details)\n",
        "        elif app_mode == \"üìä Data Processing\":\n",
        "            self.run_data_processing()\n",
        "        elif app_mode == \"ü§ñ Mod√®le\":\n",
        "            self.run_model_management()\n",
        "        elif app_mode == \"üîç Analyse\":\n",
        "            self.run_analysis()\n",
        "        elif app_mode == \"‚ùì Q&A\":\n",
        "            self.run_qa_interface()\n",
        "        elif app_mode == \"üìà Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self, sample_size, epochs, show_details):\n",
        "        \"\"\"Pipeline complet avec toutes les fonctionnalit√©s\"\"\"\n",
        "\n",
        "        st.header(\"üöÄ Pipeline Complet avec Entra√Ænement R√©el\")\n",
        "\n",
        "        # Configuration des epochs\n",
        "        self.config.epochs = epochs\n",
        "\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Choisissez un fichier CSV\",\n",
        "            type=['csv'],\n",
        "            key=\"pipeline_upload\"\n",
        "        )\n",
        "\n",
        "        if uploaded_file:\n",
        "            try:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "                st.success(f\"‚úÖ Fichier charg√©: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
        "\n",
        "                with st.expander(\"üëÄ Aper√ßu des donn√©es\"):\n",
        "                    st.dataframe(df.head())\n",
        "\n",
        "                if st.button(\"üöÄ Lancer l'entra√Ænement complet\", type=\"primary\"):\n",
        "                    self.run_real_training(df, sample_size)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå Erreur lors du chargement: {e}\")\n",
        "\n",
        "    def run_real_training(self, df, sample_size):\n",
        "        \"\"\"Entra√Ænement r√©el avec toutes les fonctionnalit√©s\"\"\"\n",
        "\n",
        "        progress_bar = st.progress(0)\n",
        "        status_text = st.empty()\n",
        "\n",
        "        try:\n",
        "            # √âtape 1: Data Processing\n",
        "            status_text.text(\"üìä Pr√©paration des donn√©es...\")\n",
        "            train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "            progress_bar.progress(20)\n",
        "\n",
        "            # √âtape 2: Configuration mod√®le\n",
        "            status_text.text(\"ü§ñ Configuration du mod√®le...\")\n",
        "            self.model_manager.setup_tokenizer()\n",
        "            self.model_manager.setup_model(len(self.data_processor.label_mapping))\n",
        "            progress_bar.progress(40)\n",
        "\n",
        "            # √âtape 3: Entra√Ænement\n",
        "            status_text.text(\"üéØ Entra√Ænement en cours...\")\n",
        "            self.trainer = self.model_manager.setup_trainer(train_ds, val_ds)\n",
        "            self.trainer.train()\n",
        "            progress_bar.progress(70)\n",
        "\n",
        "            # √âtape 4: √âvaluation\n",
        "            status_text.text(\"üìä √âvaluation du mod√®le...\")\n",
        "            metrics = self.trainer.evaluate(test_ds)\n",
        "            progress_bar.progress(85)\n",
        "\n",
        "            # √âtape 5: Configuration Q&A\n",
        "            status_text.text(\"üîç Configuration Q&A...\")\n",
        "            self.qa_module.fit(train_ds)\n",
        "            progress_bar.progress(100)\n",
        "\n",
        "            # Affichage des r√©sultats\n",
        "            self.display_training_results(metrics, test_ds)\n",
        "            self.trained = True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur dans le pipeline: {str(e)}\")\n",
        "            st.code(str(e))\n",
        "\n",
        "    def display_training_results(self, metrics, test_dataset):\n",
        "        \"\"\"Affichage des r√©sultats d'entra√Ænement\"\"\"\n",
        "\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"success-box\">\n",
        "            <h3>üéâ Entra√Ænement termin√© avec succ√®s!</h3>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # M√©triques principales\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "        with col1:\n",
        "            st.metric(\"üìä Accuracy\", f\"{metrics['eval_accuracy']:.4f}\")\n",
        "        with col2:\n",
        "            st.metric(\"üéØ F1-Score\", f\"{metrics['eval_f1_weighted']:.4f}\")\n",
        "        with col3:\n",
        "            st.metric(\"üìè Precision\", f\"{metrics['eval_precision']:.4f}\")\n",
        "        with col4:\n",
        "            st.metric(\"üîç Recall\", f\"{metrics['eval_recall']:.4f}\")\n",
        "\n",
        "        # Visualisations\n",
        "        st.subheader(\"üìà Courbes d'entra√Ænement\")\n",
        "        self.visualizer.plot_training_curves(\"outputs/runs\")\n",
        "\n",
        "        # Matrice de confusion\n",
        "        label_names = list(self.data_processor.label_mapping.keys())\n",
        "        st.subheader(\"üîç Matrice de Confusion\")\n",
        "        self.visualizer.show_confusion_matrix(self.trainer, test_dataset, label_names)\n",
        "\n",
        "        # Sauvegarde\n",
        "        model_path = \"outputs/final_model\"\n",
        "        self.trainer.save_model(model_path)\n",
        "        st.success(f\"üì¶ Mod√®le sauvegard√© dans `{model_path}`\")\n",
        "\n",
        "    def run_qa_interface(self):\n",
        "        \"\"\"Interface de recherche Q&A\"\"\"\n",
        "        st.header(\"‚ùì Module Questions-R√©ponses\")\n",
        "\n",
        "        if not self.trained:\n",
        "            st.warning(\"‚ö†Ô∏è Veuillez d'abord entra√Æner un mod√®le pour utiliser la recherche Q&A\")\n",
        "            return\n",
        "\n",
        "        question = st.text_input(\n",
        "            \"Posez votre question sur le climat :\",\n",
        "            placeholder=\"Ex: Quels sont les impacts du r√©chauffement climatique ?\"\n",
        "        )\n",
        "\n",
        "        if question:\n",
        "            results = self.qa_module.query(question, top_k=5)\n",
        "\n",
        "            if results:\n",
        "                st.markdown(\"### üîç R√©sultats les plus pertinents :\")\n",
        "                for i, res in enumerate(results, 1):\n",
        "                    st.markdown(f\"\"\"\n",
        "                    <div class=\"stage-card\">\n",
        "                        <h4>{i}. Score: {res['score']:.3f}</h4>\n",
        "                        <p><strong>Texte:</strong> {res['text'][:300]}...</p>\n",
        "                        <p><strong>Label:</strong> {res['label_id']}</p>\n",
        "                    </div>\n",
        "                    \"\"\", unsafe_allow_html=True)\n",
        "            else:\n",
        "                st.info(\"Aucun r√©sultat trouv√©. Essayez avec des mots-cl√©s diff√©rents.\")\n",
        "\n",
        "    def run_data_processing(self):\n",
        "        \"\"\"Interface de data processing\"\"\"\n",
        "        st.header(\"üìä Module Data Processing\")\n",
        "\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Choisissez un fichier CSV\",\n",
        "            type=['csv'],\n",
        "            key=\"data_upload\"\n",
        "        )\n",
        "\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"üìä Donn√©es charg√©es: {df.shape}\")\n",
        "\n",
        "            # D√©tection automatique\n",
        "            text_col, label_col = self.data_processor.detect_columns(df)\n",
        "            st.info(f\"üîç Colonnes d√©tect√©es: Texte='{text_col}', Label='{label_col}'\")\n",
        "\n",
        "            # Pr√©visualisation\n",
        "            with st.expander(\"üëÄ Aper√ßu des donn√©es\"):\n",
        "                st.dataframe(df.head(10))\n",
        "\n",
        "            if st.button(\"üìä Pr√©parer les donn√©es\"):\n",
        "                with st.spinner(\"Pr√©paration en cours...\"):\n",
        "                    train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df)\n",
        "\n",
        "                    col1, col2, col3 = st.columns(3)\n",
        "                    with col1:\n",
        "                        st.metric(\"üéØ Train\", len(train_ds))\n",
        "                    with col2:\n",
        "                        st.metric(\"üîç Validation\", len(val_ds))\n",
        "                    with col3:\n",
        "                        st.metric(\"üìä Test\", len(test_ds))\n",
        "\n",
        "                    # Distribution des labels\n",
        "                    st.subheader(\"üìà Distribution des labels\")\n",
        "                    label_dist = pd.Series([item['label_id'] for item in train_ds]).value_counts()\n",
        "                    st.bar_chart(label_dist)\n",
        "\n",
        "    def run_model_management(self):\n",
        "        \"\"\"Interface de gestion du mod√®le\"\"\"\n",
        "        st.header(\"ü§ñ Module Gestion Mod√®le\")\n",
        "\n",
        "        if st.button(\"üîß Configurer le mod√®le\"):\n",
        "            with st.spinner(\"Configuration...\"):\n",
        "                self.model_manager.setup_tokenizer()\n",
        "\n",
        "                st.success(\"‚úÖ Tokenizer configur√©!\")\n",
        "\n",
        "                col1, col2 = st.columns(2)\n",
        "\n",
        "                with col1:\n",
        "                    st.subheader(\"üìã Configuration\")\n",
        "                    config_dict = self.config.to_dict()\n",
        "                    st.json(config_dict)\n",
        "\n",
        "                with col2:\n",
        "                    st.subheader(\"üîç Informations\")\n",
        "                    st.info(\"ü§ñ Mod√®le: \" + self.config.model_name)\n",
        "                    st.info(\"üìè Longueur max: \" + str(self.config.max_length))\n",
        "                    st.info(\"üéØ Batch size: \" + str(self.config.batch_size))\n",
        "\n",
        "    def run_analysis(self):\n",
        "        \"\"\"Interface d'analyse\"\"\"\n",
        "        st.header(\"üîç Module Analyse\")\n",
        "\n",
        "        if not self.trained:\n",
        "            st.warning(\"‚ö†Ô∏è Veuillez d'abord entra√Æner un mod√®le\")\n",
        "            return\n",
        "\n",
        "        # Charger le mod√®le sauvegard√©\n",
        "        model_path = \"outputs/final_model\"\n",
        "        if os.path.exists(model_path):\n",
        "            self.model_manager.peft_model = self.model_manager.setup_model(len(self.data_processor.label_mapping))\n",
        "            self.model_manager.peft_model.load_adapter(model_path)\n",
        "\n",
        "        text_input = st.text_area(\n",
        "            \"Texte √† analyser:\",\n",
        "            height=100,\n",
        "            placeholder=\"Entrez votre texte climatique ici...\",\n",
        "            value=\"Le r√©chauffement climatique est un probl√®me urgent qui n√©cessite une action imm√©diate.\"\n",
        "        )\n",
        "\n",
        "        if st.button(\"üîç Pr√©dire\", type=\"primary\"):\n",
        "            if text_input.strip():\n",
        "                with st.spinner(\"Analyse en cours...\"):\n",
        "                    result = self.predict_with_saved_model(text_input)\n",
        "                    self.display_analysis_result(result)\n",
        "\n",
        "    def predict_with_saved_model(self, text: str) -> PredictionResult:\n",
        "        \"\"\"Pr√©diction avec le mod√®le entra√Æn√©\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Tokenisation\n",
        "        inputs = self.model_manager.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "        # Pr√©diction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_manager.peft_model(**inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "            confidence = probs[0][predicted_class].item()\n",
        "\n",
        "        # Mapping inverse\n",
        "        inv_label_map = {v: k for k, v in self.data_processor.label_mapping.items()}\n",
        "        predicted_label = inv_label_map.get(predicted_class, f\"Classe {predicted_class}\")\n",
        "\n",
        "        # Tous les scores\n",
        "        all_scores = {inv_label_map.get(i, f\"Class {i}\"): float(prob) for i, prob in enumerate(probs[0])}\n",
        "\n",
        "        # Contexte\n",
        "        context = self.knowledge_base.find_context(text)\n",
        "\n",
        "        return PredictionResult(\n",
        "            text=text,\n",
        "            predicted_label=predicted_label,\n",
        "            confidence=confidence,\n",
        "            all_scores=all_scores,\n",
        "            context=context,\n",
        "            processing_time=time.time() - start_time\n",
        "        )\n",
        "\n",
        "    def display_analysis_result(self, result: PredictionResult):\n",
        "        \"\"\"Affichage des r√©sultats d'analyse\"\"\"\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>üéØ Sentiment</h3>\n",
        "                <h2>{result.predicted_label.title()}</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        with col2:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>üìä Confiance</h3>\n",
        "                <h2>{result.confidence:.1%}</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        with col3:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>‚è±Ô∏è Temps</h3>\n",
        "                <h2>{result.processing_time:.2f}s</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Scores d√©taill√©s\n",
        "        st.subheader(\"üìä Scores d√©taill√©s\")\n",
        "        scores_df = pd.DataFrame([\n",
        "            {\"Sentiment\": k.title(), \"Score\": f\"{v:.2%}\"}\n",
        "            for k, v in result.all_scores.items()\n",
        "        ])\n",
        "        st.dataframe(scores_df, use_container_width=True)\n",
        "\n",
        "        # Contexte\n",
        "        if result.context:\n",
        "            st.subheader(\"üí° Contexte Pertinent\")\n",
        "            for i, ctx in enumerate(result.context, 1):\n",
        "                st.markdown(f\"**{i}.** {ctx}\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        \"\"\"Interface de visualisations\"\"\"\n",
        "        st.header(\"üìà Module Visualisations\")\n",
        "\n",
        "        if not self.trained:\n",
        "            st.warning(\"‚ö†Ô∏è Veuillez d'abord entra√Æner un mod√®le pour voir les visualisations\")\n",
        "            return\n",
        "\n",
        "        # Visualisations disponibles\n",
        "        viz_option = st.selectbox(\n",
        "            \"Choisir une visualisation\",\n",
        "            [\"üìà Courbes d'entra√Ænement\", \"üîç Matrice de confusion\", \"üìä M√©triques\"]\n",
        "        )\n",
        "\n",
        "        if viz_option == \"üìà Courbes d'entra√Ænement\":\n",
        "            self.visualizer.plot_training_curves(\"outputs/runs\")\n",
        "        elif viz_option == \"üîç Matrice de confusion\":\n",
        "            label_names = list(self.data_processor.label_mapping.keys())\n",
        "            self.visualizer.show_confusion_matrix(self.trainer, self.trainer.eval_dataset, label_names)\n",
        "        elif viz_option == \"üìä M√©triques\":\n",
        "            if self.trainer:\n",
        "                metrics = self.trainer.evaluate()\n",
        "                st.json(metrics)\n",
        "\n",
        "# ========== ORCHESTRATEUR PRINCIPAL ==========\n",
        "class PipelineOrchestrator:\n",
        "    \"\"\"Orchestrateur principal du pipeline\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.app = ClimateAnalyzerApp()\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Lancement de l'application\"\"\"\n",
        "        self.app.run()\n",
        "\n",
        "# ========== UTILISATION ==========\n",
        "if __name__ == \"__main__\":\n",
        "    orchestrator = PipelineOrchestrator()\n",
        "    orchestrator.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1fb5d3e-89bc-4983-9094-ba579282ea67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6Ô∏è‚É£ Script d'Installation - setup_pipeline.py"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_pipeline.py\n",
        "# setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installation compl√®te des d√©pendances\"\"\"\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"‚úÖ {package} install√©\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur avec {package}: {e}\")\n",
        "\n",
        "    print(\"‚úÖ Installation compl√®te termin√©e!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d738f21a-a5b4-48a6-ae75-a647f38d51d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ transformers==4.36.2 install√©\n",
            "‚úÖ datasets==2.16.1 install√©\n",
            "‚úÖ torch==2.1.2 install√©\n",
            "‚úÖ peft==0.7.1 install√©\n",
            "‚úÖ sentence-transformers==2.2.2 install√©\n",
            "‚úÖ faiss-cpu==1.7.4 install√©\n",
            "‚úÖ streamlit==1.29.0 install√©\n",
            "‚úÖ plotly==5.17.0 install√©\n",
            "‚úÖ scikit-learn==1.3.2 install√©\n",
            "‚úÖ Installation termin√©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c98b80f-19ab-46c7-95cf-66b2f5377cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/setup_pipeline.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MEnnZWsOuL5",
        "outputId": "7e81f95f-dc2f-4fe4-8107-5a4e48ff4d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.29.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.11.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.11/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.8)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.3.1)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.35.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.19.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3wskx_fZLEp",
        "outputId": "e4fb3cc2-a4d3-4053-f784-3b81330ab160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Lancement Streamlit + ngrok (version corrig√©e)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1Ô∏è‚É£ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2Ô∏è‚É£ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Attendre et cr√©er le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üöÄ Interface Streamlit disponible √† :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFyKJzjISjWG",
        "outputId": "fe9e2a2d-b05d-449f-f7b8-a3e9a0053512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "üöÄ Interface Streamlit disponible √† :\n",
            "NgrokTunnel: \"https://640c5f983024.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}