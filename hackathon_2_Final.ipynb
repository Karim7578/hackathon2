{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Module Core - core_modules.py"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py - Configuration optimis√©e\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralis√©e et optimis√©e\"\"\"\n",
        "    model_name: str = \"distilbert-base-uncased\"\n",
        "    max_length: int = 128\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 3\n",
        "    learning_rate: float = 2e-5\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    output_dir: str = \"outputs/final_model\"\n",
        "\n",
        "    # Configuration Q&A\n",
        "    qa_model: str = \"all-MiniLM-L6-v2\"\n",
        "    similarity_threshold: float = 0.3\n",
        "    max_results: int = 5"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b36eb8a-9a19-44fe-94b6-2313caa6354a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Module Data Processing - data_modules.py"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Tuple\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "        self.reverse_label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        text_keywords = ['text', 'content', 'message', 'comment', 'body', 'description', 'self_text']\n",
        "        label_keywords = ['label', 'sentiment', 'category', 'class', 'target', 'comment_sentiment']\n",
        "        text_col = next((c for c in df.columns if any(k in str(c).lower() for k in text_keywords)), None)\n",
        "        label_col = next((c for c in df.columns if any(k in str(c).lower() for k in label_keywords)), None)\n",
        "        if not text_col:\n",
        "            text_col = df.select_dtypes(include=['object']).columns[0]\n",
        "        if not label_col:\n",
        "            label_col = df.columns[-1]\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text) or str(text).strip().lower() in ['nan', 'none', '', 'null']:\n",
        "            return None\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r'&gt;|&lt;|&amp;', lambda m: {'&gt;': '>', '&lt;': '<', '&amp;': '&'}[m.group()], text)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip() if text.strip() else None\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "        df_clean = df[[self.text_col, self.label_col]].copy()\n",
        "        df_clean.columns = ['text', 'label']\n",
        "        df_clean['text'] = df_clean['text'].apply(self.clean_text)\n",
        "        df_clean['label'] = df_clean['label'].astype(str)\n",
        "        df_clean = df_clean.dropna().reset_index(drop=True)\n",
        "        df_clean = df_clean[df_clean['text'].str.len() >= 10]\n",
        "\n",
        "        if len(df_clean) > sample_size:\n",
        "            df_clean = df_clean.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        unique_labels = sorted(df_clean['label'].unique())\n",
        "        self.label_mapping = {str(l): i for i, l in enumerate(unique_labels)}\n",
        "        df_clean['label_id'] = df_clean['label'].map(self.label_mapping)\n",
        "\n",
        "        # Nettoyage final NaN\n",
        "        df_clean = df_clean.dropna(subset=['label_id'])\n",
        "        df_clean['label_id'] = df_clean['label_id'].astype(int)\n",
        "\n",
        "        if df_clean.empty:\n",
        "            raise ValueError(\"‚ùå Aucune donn√©e valide apr√®s nettoyage.\")\n",
        "\n",
        "        train_df, temp = train_test_split(df_clean, test_size=0.4, stratify=df_clean['label_id'], random_state=42)\n",
        "        val_df, test_df = train_test_split(temp, test_size=0.5, stratify=temp['label_id'], random_state=42)\n",
        "\n",
        "        return (\n",
        "            Dataset.from_pandas(train_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(val_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(test_df[['text', 'label_id']])\n",
        "        )"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45fbc009-7d81-4d1c-f16f-828dd598750b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Module Mod√®le - model_modules.py"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        return self.tokenizer\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float32,\n",
        "            problem_type=\"single_label_classification\"\n",
        "        )\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=self.config.lora_r,\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_lin\", \"v_lin\"],\n",
        "            bias=\"none\",\n",
        "        )\n",
        "        self.peft_model = get_peft_model(base_model, lora_config)\n",
        "        return self.peft_model\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.config.max_length,\n",
        "        )\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        preds = np.argmax(predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "        }\n",
        "\n",
        "    def setup_training_args(self):\n",
        "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.config.output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            save_steps=500,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "            fp16=False,\n",
        "            bf16=False,\n",
        "            fp16_full_eval=False,\n",
        "            bf16_full_eval=False,\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        return Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=self.setup_training_args(),\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "        )"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca21fdff-fd12-4140-c30c-4c204298b5e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. visualization_modules.py"
      ],
      "metadata": {
        "id": "qYJhFun3Mws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# --- NLTK / BLEU / ROUGE ---\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# T√©l√©chargement silencieux des ressources NLTK\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "plt.style.use('default')\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestionnaire de visualisations pour Climate Analyzer.\"\"\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Outils internes BLEU / ROUGE\n",
        "    # --------------------------------------------------\n",
        "    _smoothie = SmoothingFunction().method4\n",
        "    _rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _bleu(ref: str, hyp: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        ref_tok = nltk.word_tokenize(ref.lower())\n",
        "        hyp_tok = nltk.word_tokenize(hyp.lower())\n",
        "        return sentence_bleu([ref_tok], hyp_tok, smoothing_function=VisualizationManager._smoothie)\n",
        "\n",
        "    @staticmethod\n",
        "    def _rouge_score(ref: str, hyp: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        scores = VisualizationManager._rouge_scorer.score(ref.lower(), hyp.lower())\n",
        "        return {'rouge-1': scores['rouge1'].fmeasure,\n",
        "                'rouge-l': scores['rougeL'].fmeasure}\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1) Courbes d‚Äôentra√Ænement\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str = \"outputs/final_model\"):\n",
        "        try:\n",
        "            log_file = os.path.join(log_dir, \"trainer_state.json\")\n",
        "            if not os.path.exists(log_file):\n",
        "                st.warning(\"üìÑ Aucun log d'entra√Ænement trouv√©.\")\n",
        "                return\n",
        "\n",
        "            with open(log_file, 'r', encoding='utf-8') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"üìâ Aucune donn√©e d'historique trouv√©e.\")\n",
        "                return\n",
        "\n",
        "            epochs, train_loss, eval_loss, eval_acc, eval_f1 = [], [], [], [], []\n",
        "\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_acc.append(entry.get('eval_accuracy', 0))\n",
        "                    eval_f1.append(entry.get('eval_f1_weighted', 0))\n",
        "                elif 'train_loss' in entry:\n",
        "                    train_loss.append(entry.get('train_loss', 0))\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(\"üìà √âvolution de l'entra√Ænement\", fontsize=16)\n",
        "\n",
        "            if train_loss and eval_loss:\n",
        "                train_steps = np.linspace(0, max(epochs) if epochs else 1, len(train_loss))\n",
        "                axes[0, 0].plot(train_steps, train_loss, 'b-', label='Train Loss', alpha=0.7)\n",
        "                axes[0, 0].plot(epochs[:len(eval_loss)], eval_loss, 'r-o', label='Eval Loss', markersize=4)\n",
        "                axes[0, 0].set_title('Loss Evolution')\n",
        "                axes[0, 0].legend()\n",
        "                axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc:\n",
        "                axes[0, 1].plot(epochs[:len(eval_acc)], eval_acc, 'g-o', label='Accuracy', markersize=4)\n",
        "                axes[0, 1].set_title('Accuracy Evolution')\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_f1:\n",
        "                axes[1, 0].plot(epochs[:len(eval_f1)], eval_f1, 'm-o', label='F1-Weighted', markersize=4)\n",
        "                axes[1, 0].set_title('F1-Score Evolution')\n",
        "                axes[1, 0].legend()\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc and eval_f1:\n",
        "                final_metrics = ['Accuracy', 'F1-Score']\n",
        "                final_values = [eval_acc[-1], eval_f1[-1]]\n",
        "                bars = axes[1, 1].bar(final_metrics, final_values, color=['green', 'purple'], alpha=0.7)\n",
        "                axes[1, 1].set_title('Final Metrics')\n",
        "                axes[1, 1].set_ylim(0, 1)\n",
        "                for bar, value in zip(bars, final_values):\n",
        "                    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                                   f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'affichage des courbes : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2) Matrice de confusion\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names: List[str]):\n",
        "        try:\n",
        "            predictions_output = trainer.predict(test_dataset)\n",
        "            predictions = predictions_output.predictions.argmax(axis=1)\n",
        "            true_labels = predictions_output.label_ids\n",
        "\n",
        "            cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax1)\n",
        "            ax1.set_title(\"Matrice de confusion\")\n",
        "            ax1.set_xlabel(\"Pr√©dictions\")\n",
        "            ax1.set_ylabel(\"Vraies valeurs\")\n",
        "\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax2)\n",
        "            ax2.set_title(\"Matrice de confusion (normalis√©e)\")\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            report = classification_report(true_labels, predictions,\n",
        "                                         target_names=label_names,\n",
        "                                         output_dict=True, zero_division=0)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.subheader(\"üìä Rapport de classification\")\n",
        "            st.dataframe(report_df.round(3))\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'affichage de la matrice de confusion : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3) Distribution des classes\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(labels, label_names: List[str] = None, title: str = \"Distribution des classes\"):\n",
        "        try:\n",
        "            if hasattr(labels, 'tolist'):\n",
        "                labels = labels.tolist()\n",
        "            labels = [int(x) for x in labels]\n",
        "            label_counts = Counter(labels)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            if label_names:\n",
        "                x_labels = [label_names[i] if i < len(label_names) else f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "            else:\n",
        "                x_labels = [f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "\n",
        "            bars = ax.bar(range(len(x_labels)), counts, color=plt.cm.Set3(np.linspace(0, 1, len(x_labels))))\n",
        "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "            ax.set_xlabel(\"Classes\")\n",
        "            ax.set_ylabel(\"Nombre d'√©chantillons\")\n",
        "            ax.set_xticks(range(len(x_labels)))\n",
        "            ax.set_xticklabels(x_labels, rotation=45 if max(map(len, x_labels)) > 10 else 0)\n",
        "\n",
        "            for bar, count in zip(bars, counts):\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
        "                       str(count), ha='center', va='bottom')\n",
        "            total = sum(counts)\n",
        "            ax.text(0.02, 0.98, f\"Total: {total}\\nClasses: {len(x_labels)}\",\n",
        "                   transform=ax.transAxes, va='top', ha='left',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.7))\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'affichage de la distribution : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 4) Analyse des r√©sultats Q&A\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_qa_results_analysis(qa_results: List[Dict], question: str):\n",
        "        if not qa_results:\n",
        "            st.info(\"Aucun r√©sultat √† analyser\")\n",
        "            return\n",
        "        try:\n",
        "            scores = [r['score'] for r in qa_results]\n",
        "            ranks = [r['rank'] for r in qa_results]\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(f\"Analyse des r√©sultats pour: '{question[:50]}...'\", fontsize=14)\n",
        "\n",
        "            axes[0, 0].hist(scores, bins=min(10, len(scores)), alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            axes[0, 0].set_title(\"Distribution des scores de similarit√©\")\n",
        "            axes[0, 0].axvline(np.mean(scores), color='red', linestyle='--', label=f'Moyenne: {np.mean(scores):.3f}')\n",
        "            axes[0, 0].legend()\n",
        "\n",
        "            axes[0, 1].bar(ranks, scores, color='lightcoral', alpha=0.7)\n",
        "            axes[0, 1].set_title(\"Scores par rang\")\n",
        "            axes[0, 1].set_xlabel(\"Rang\")\n",
        "\n",
        "            text_lengths = [len(r['text']) for r in qa_results]\n",
        "            axes[1, 0].scatter(text_lengths, scores, alpha=0.6, color='green')\n",
        "            axes[1, 0].set_title(\"Score vs Longueur du texte\")\n",
        "            axes[1, 0].set_xlabel(\"Longueur du texte\")\n",
        "\n",
        "            top_scores = scores[:min(5, len(scores))]\n",
        "            top_ranks = ranks[:min(5, len(ranks))]\n",
        "            axes[1, 1].barh(range(len(top_scores)), top_scores, color='purple', alpha=0.7)\n",
        "            axes[1, 1].set_title(\"Top 5 des scores\")\n",
        "            axes[1, 1].set_yticks(range(len(top_scores)))\n",
        "            axes[1, 1].set_yticklabels([f\"Rang {r}\" for r in top_ranks])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            st.subheader(\"üìà Statistiques d√©taill√©es\")\n",
        "            stats_df = pd.DataFrame({\n",
        "                \"M√©trique\": [\"Score moyen\", \"Score m√©dian\", \"Score max\", \"Score min\", \"√âcart-type\"],\n",
        "                \"Valeur\": [np.mean(scores), np.median(scores), np.max(scores), np.min(scores), np.std(scores)]\n",
        "            })\n",
        "            st.dataframe(stats_df.round(4))\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'analyse des r√©sultats Q&A : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 5) M√©thodes BLEU / ROUGE manquantes\n",
        "    # --------------------------------------------------\n",
        "    def calculate_bleu_score(self, reference: str, candidate: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        return self._bleu(reference, candidate)\n",
        "\n",
        "    def calculate_rouge_score(self, reference: str, candidate: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        return self._rouge_score(reference, candidate)\n",
        "\n",
        "    def visualize_bleu_rouge_scores(self, qa_results, references):\n",
        "        \"\"\"Visualisation BLEU & ROUGE pour chaque paire (ref, r√©sultat).\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for ref, res in zip(references, qa_results):\n",
        "            bleus.append(self._bleu(ref, res['text']))\n",
        "            r1s.append(self._rouge_score(ref, res['text'])['rouge-1'])\n",
        "            rls.append(self._rouge_score(ref, res['text'])['rouge-l'])\n",
        "\n",
        "        x = list(range(1, len(bleus)+1))\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar([i-0.2 for i in x], bleus, 0.4, label='BLEU')\n",
        "        plt.bar([i+0.2 for i in x], r1s, 0.4, label='ROUGE-1')\n",
        "        plt.xlabel('Rang')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('BLEU & ROUGE vs r√©f√©rences')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(plt.gcf())\n",
        "\n",
        "    def evaluate_qa_performance(self, qa_module, questions, references):\n",
        "        \"\"\"√âvaluation compl√®te Q-A avec scores BLEU/ROUGE.\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for q, ref in zip(questions, references):\n",
        "            res = qa_module.query_with_fallback(q, top_k=1)\n",
        "            if res:\n",
        "                cand = res[0]['text']\n",
        "                bleus.append(self._bleu(ref, cand))\n",
        "                r1s.append(self._rouge_score(ref, cand)['rouge-1'])\n",
        "                rls.append(self._rouge_score(ref, cand)['rouge-l'])\n",
        "\n",
        "        st.write(\"### üìä Global Q-A metrics\")\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        col1.metric(\"Avg BLEU\", f\"{np.mean(bleus):.4f}\")\n",
        "        col2.metric(\"Avg ROUGE-1\", f\"{np.mean(r1s):.4f}\")\n",
        "        col3.metric(\"Avg ROUGE-L\", f\"{np.mean(rls):.4f}\")"
      ],
      "metadata": {
        "id": "oMZt-WwIMwez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "317787a1-d66a-4b0b-84e4-d4cf6a62f95c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting visualization_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. qa_modules.py"
      ],
      "metadata": {
        "id": "1HSXVYNkNc-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py - Version corrig√©e et optimis√©e\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "import logging\n",
        "from functools import lru_cache\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "class OptimizedQAModule:\n",
        "    \"\"\"Version optimis√©e du module Q&A avec caching et performance am√©lior√©e\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        try:\n",
        "            self.model = SentenceTransformer(model_name, device='cpu')\n",
        "            self.similarity_threshold = 0.3\n",
        "\n",
        "            # Structures optimis√©es\n",
        "            self.corpus_texts: List[str] = []\n",
        "            self.corpus_labels: List[int] = []\n",
        "            self.corpus_embeddings: Optional[np.ndarray] = None\n",
        "            self.corpus_index: Optional[faiss.Index] = None\n",
        "\n",
        "            # Base de connaissances optimis√©e\n",
        "            self.knowledge_base: List[Dict[str, Any]] = []\n",
        "            self.kb_embeddings: Optional[np.ndarray] = None\n",
        "            self.kb_index: Optional[faiss.Index] = None\n",
        "\n",
        "            # Cache pour les requ√™tes fr√©quentes\n",
        "            self._query_cache: Dict[str, List[Dict]] = {}\n",
        "\n",
        "            self._setup_knowledge_base()\n",
        "            logging.info(\"‚úÖ Module Q&A initialis√© avec succ√®s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur initialisation Q&A: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _setup_knowledge_base(self):\n",
        "        \"\"\"Initialisation optimis√©e de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            {\n",
        "                \"text\": \"Le r√©chauffement climatique est principalement caus√© par les √©missions de CO2 humaines.\",\n",
        "                \"category\": \"causes\",\n",
        "                \"keywords\": [\"CO2\", \"√©missions\", \"humaines\", \"causes\"],\n",
        "                \"weight\": 1.0\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Les √©nergies renouvelables (solaire, √©olien, hydro) r√©duisent drastiquement les √©missions.\",\n",
        "                \"category\": \"solutions\",\n",
        "                \"keywords\": [\"renouvelables\", \"solaire\", \"√©olien\", \"hydro\"],\n",
        "                \"weight\": 1.2\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"La d√©forestation est responsable de 15% des √©missions mondiales de CO2.\",\n",
        "                \"category\": \"causes\",\n",
        "                \"keywords\": [\"d√©forestation\", \"for√™ts\", \"15%\"],\n",
        "                \"weight\": 0.9\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Le transport repr√©sente 24% des √©missions mondiales de gaz √† effet de serre.\",\n",
        "                \"category\": \"secteurs\",\n",
        "                \"keywords\": [\"transport\", \"24%\", \"v√©hicules\"],\n",
        "                \"weight\": 1.1\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"L'isolation thermique peut r√©duire la consommation √©nerg√©tique jusqu'√† 50%.\",\n",
        "                \"category\": \"solutions\",\n",
        "                \"keywords\": [\"isolation\", \"thermique\", \"50%\"],\n",
        "                \"weight\": 1.0\n",
        "            }\n",
        "        ]\n",
        "        self._rebuild_kb_index()\n",
        "\n",
        "    def _rebuild_kb_index(self):\n",
        "        \"\"\"Reconstruction optimis√©e de l'index FAISS\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            texts = [item[\"text\"] for item in self.knowledge_base]\n",
        "            self.kb_embeddings = self.model.encode(\n",
        "                texts,\n",
        "                normalize_embeddings=True,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "\n",
        "            # Index FAISS optimis√©\n",
        "            d = self.kb_embeddings.shape[1]\n",
        "            self.kb_index = faiss.IndexFlatIP(d)\n",
        "            self.kb_index.add(self.kb_embeddings.astype(np.float32))\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur reconstruction index: {e}\")\n",
        "\n",
        "    def fit(self, dataset: List[Dict[str, Any]]) -> bool:\n",
        "        \"\"\"Indexation optimis√©e du corpus d'entra√Ænement\"\"\"\n",
        "        try:\n",
        "            if not dataset:\n",
        "                logging.warning(\"‚ö†Ô∏è Dataset vide, rien √† indexer\")\n",
        "                return False\n",
        "\n",
        "            self.corpus_texts = [d[\"text\"] for d in dataset]\n",
        "            self.corpus_labels = [d.get(\"label_id\", 0) for d in dataset]\n",
        "\n",
        "            # Encodage optimis√© avec batch processing\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(self.corpus_texts), batch_size):\n",
        "                batch = self.corpus_texts[i:i+batch_size]\n",
        "                batch_embeddings = self.model.encode(\n",
        "                    batch,\n",
        "                    normalize_embeddings=True,\n",
        "                    show_progress_bar=False,\n",
        "                    convert_to_numpy=True\n",
        "                )\n",
        "                embeddings.append(batch_embeddings)\n",
        "\n",
        "            self.corpus_embeddings = np.vstack(embeddings)\n",
        "\n",
        "            # Index FAISS\n",
        "            d = self.corpus_embeddings.shape[1]\n",
        "            self.corpus_index = faiss.IndexFlatIP(d)\n",
        "            self.corpus_index.add(self.corpus_embeddings.astype(np.float32))\n",
        "\n",
        "            # Mise √† jour du cache\n",
        "            self._query_cache.clear()\n",
        "\n",
        "            logging.info(f\"‚úÖ {len(dataset)} documents index√©s avec succ√®s\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur indexation: {e}\")\n",
        "            return False\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def _get_query_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Cache des embeddings de requ√™tes fr√©quentes\"\"\"\n",
        "        return self.model.encode([query], normalize_embeddings=True)[0]\n",
        "\n",
        "    def _search_index(self, query: str, index: faiss.Index, texts: List[str],\n",
        "                     source: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimis√©e dans un index FAISS\"\"\"\n",
        "        if index is None or not texts:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_embedding = self._get_query_embedding(query)\n",
        "            scores, indices = index.search(\n",
        "                query_embedding.reshape(1, -1).astype(np.float32),\n",
        "                min(top_k, len(texts))\n",
        "            )\n",
        "\n",
        "            results = []\n",
        "            for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
        "                if idx < len(texts) and score > self.similarity_threshold:\n",
        "                    result = {\n",
        "                        \"text\": texts[idx],\n",
        "                        \"score\": float(score),\n",
        "                        \"rank\": rank,\n",
        "                        \"source\": source\n",
        "                    }\n",
        "\n",
        "                    # Ajout des m√©tadonn√©es si disponible\n",
        "                    if source == \"knowledge_base\" and idx < len(self.knowledge_base):\n",
        "                        result.update({\n",
        "                            \"category\": self.knowledge_base[idx][\"category\"],\n",
        "                            \"keywords\": self.knowledge_base[idx][\"keywords\"]\n",
        "                        })\n",
        "                    elif source == \"training_corpus\" and idx < len(self.corpus_labels):\n",
        "                        result[\"label_id\"] = int(self.corpus_labels[idx])\n",
        "\n",
        "                    results.append(result)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur recherche index: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_knowledge_base(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimis√©e dans la base de connaissances\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return []\n",
        "        return self._search_index(query, self.kb_index,\n",
        "                                [item[\"text\"] for item in self.knowledge_base],\n",
        "                                \"knowledge_base\", top_k)\n",
        "\n",
        "    def query_corpus(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimis√©e dans le corpus d'entra√Ænement\"\"\"\n",
        "        if not self.corpus_texts:\n",
        "            return []\n",
        "        return self._search_index(query, self.corpus_index, self.corpus_texts,\n",
        "                                \"training_corpus\", top_k)\n",
        "\n",
        "    def keyword_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par mots-cl√©s optimis√©e avec scoring avanc√©\"\"\"\n",
        "        if not query:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
        "            if not query_words:\n",
        "                return []\n",
        "\n",
        "            scored = []\n",
        "\n",
        "            # Recherche dans la base de connaissances\n",
        "            for item in self.knowledge_base:\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', item[\"text\"].lower()))\n",
        "                keyword_words = set(item[\"keywords\"])\n",
        "\n",
        "                # Score combin√© TF-IDF like\n",
        "                text_score = len(query_words & text_words) / max(len(text_words), 1)\n",
        "                keyword_score = len(query_words & keyword_words) / max(len(keyword_words), 1)\n",
        "                combined_score = (text_score * 0.7 + keyword_score * 0.3) * item.get(\"weight\", 1.0)\n",
        "\n",
        "                if combined_score > 0.1:\n",
        "                    scored.append({\n",
        "                        \"text\": item[\"text\"],\n",
        "                        \"category\": item[\"category\"],\n",
        "                        \"keywords\": item[\"keywords\"],\n",
        "                        \"score\": combined_score,\n",
        "                        \"source\": \"knowledge_base\"\n",
        "                    })\n",
        "\n",
        "            # Recherche dans le corpus\n",
        "            for i, text in enumerate(self.corpus_texts):\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "                score = len(query_words & text_words) / max(len(text_words), 1)\n",
        "\n",
        "                if score > 0.1:\n",
        "                    scored.append({\n",
        "                        \"text\": text,\n",
        "                        \"label_id\": int(self.corpus_labels[i]),\n",
        "                        \"score\": score,\n",
        "                        \"source\": \"training_corpus\"\n",
        "                    })\n",
        "\n",
        "            scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "            for i, result in enumerate(scored[:top_k], 1):\n",
        "                result[\"rank\"] = i\n",
        "\n",
        "            return scored[:top_k]\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur recherche mots-cl√©s: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_with_fallback(self, question: str, top_k: int = 5,\n",
        "                          search_mode: str = \"hybrid\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche avec fallback optimis√©\"\"\"\n",
        "        # Cl√© de cache\n",
        "        cache_key = f\"{question}_{search_mode}_{top_k}\"\n",
        "        if cache_key in self._query_cache:\n",
        "            return self._query_cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            # S√©lection du mode de recherche\n",
        "            if search_mode == \"knowledge_only\":\n",
        "                results = self.query_knowledge_base(question, top_k)\n",
        "            elif search_mode == \"corpus_only\":\n",
        "                results = self.query_corpus(question, top_k)\n",
        "            elif search_mode == \"keywords\":\n",
        "                results = self.keyword_search(question, top_k)\n",
        "            else:  # hybrid\n",
        "                kb_results = self.query_knowledge_base(question, top_k // 2 + 1)\n",
        "                corpus_results = self.query_corpus(question, top_k // 2 + 1)\n",
        "\n",
        "                # Fusion et d√©duplication\n",
        "                seen_texts = set()\n",
        "                results = []\n",
        "\n",
        "                for item in kb_results + corpus_results:\n",
        "                    if item[\"text\"] not in seen_texts:\n",
        "                        seen_texts.add(item[\"text\"])\n",
        "                        results.append(item)\n",
        "\n",
        "                results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                results = results[:top_k]\n",
        "\n",
        "            # Fallback si n√©cessaire\n",
        "            if not results or (results and max(r[\"score\"] for r in results) < self.similarity_threshold):\n",
        "                fallback = self.keyword_search(question, top_k)\n",
        "                seen = {r[\"text\"] for r in results}\n",
        "                for item in fallback:\n",
        "                    if item[\"text\"] not in seen:\n",
        "                        results.append(item)\n",
        "                results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                results = results[:top_k]\n",
        "\n",
        "            # Mise en cache\n",
        "            self._query_cache[cache_key] = results\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur recherche avec fallback: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_by_category(self, category: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par cat√©gorie optimis√©e\"\"\"\n",
        "        try:\n",
        "            results = [\n",
        "                {\n",
        "                    \"text\": item[\"text\"],\n",
        "                    \"category\": item[\"category\"],\n",
        "                    \"keywords\": item[\"keywords\"],\n",
        "                    \"source\": \"knowledge_base\",\n",
        "                    \"rank\": i + 1,\n",
        "                    \"score\": 1.0\n",
        "                }\n",
        "                for i, item in enumerate([\n",
        "                    it for it in self.knowledge_base\n",
        "                    if it[\"category\"].lower() == category.lower()\n",
        "                ][:top_k])\n",
        "            ]\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur recherche cat√©gorie: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, text: str, category: str = \"custom\",\n",
        "                     keywords: List[str] = None) -> bool:\n",
        "        \"\"\"Ajout optimis√© de nouvelles connaissances\"\"\"\n",
        "        try:\n",
        "            if not text or not isinstance(text, str):\n",
        "                return False\n",
        "\n",
        "            # V√©rification des doublons\n",
        "            if any(item[\"text\"].strip() == text.strip() for item in self.knowledge_base):\n",
        "                return False\n",
        "\n",
        "            new_item = {\n",
        "                \"text\": text.strip(),\n",
        "                \"category\": category,\n",
        "                \"keywords\": keywords or [],\n",
        "                \"weight\": 1.0\n",
        "            }\n",
        "\n",
        "            self.knowledge_base.append(new_item)\n",
        "            self._rebuild_kb_index()\n",
        "            self._query_cache.clear()\n",
        "\n",
        "            logging.info(f\"‚úÖ Nouvelle connaissance ajout√©e: {text[:50]}...\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur ajout connaissance: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_categories(self) -> List[str]:\n",
        "        \"\"\"Retourne les cat√©gories disponibles\"\"\"\n",
        "        try:\n",
        "            return list({item[\"category\"] for item in self.knowledge_base})\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Statistiques d√©taill√©es et optimis√©es\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                \"knowledge_base_size\": len(self.knowledge_base),\n",
        "                \"training_corpus_size\": len(self.corpus_texts),\n",
        "                \"total_documents\": len(self.knowledge_base) + len(self.corpus_texts),\n",
        "                \"categories\": self.get_categories(),\n",
        "                \"avg_kb_length\": np.mean([len(item[\"text\"]) for item in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "                \"avg_corpus_length\": np.mean([len(t) for t in self.corpus_texts]) if self.corpus_texts else 0,\n",
        "                \"cache_size\": len(self._query_cache)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur stats: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Nettoyage manuel du cache\"\"\"\n",
        "        self._query_cache.clear()\n",
        "        logging.info(\"üóëÔ∏è Cache vid√©\")"
      ],
      "metadata": {
        "id": "CdRC_3s7Ncy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f34118e-03a4-45d7-e301-ec0bc620ff90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting qa_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Module Knowledge Base - knowledge_modules.py"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le r√©chauffement climatique est principalement caus√© par les √©missions de gaz √† effet de serre d'origine humaine.\",\n",
        "            \"Les √©nergies renouvelables comme le solaire et l'√©olien sont essentielles pour d√©carboner notre √©conomie.\",\n",
        "            \"La d√©forestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports repr√©sente environ 24% des √©missions mondiales de gaz √† effet de serre.\",\n",
        "            \"L'am√©lioration de l'efficacit√© √©nerg√©tique des b√¢timents peut r√©duire jusqu'√† 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et r√©g√©n√©ratrice peut s√©questrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les oc√©ans absorbent 25% du CO2 atmosph√©rique mais s'acidifient, mena√ßant les √©cosyst√®mes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises √† r√©duire leurs √©missions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'att√©nuation des √©missions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralit√© carbone.\"\n",
        "        ]\n",
        "        print(\"‚úÖ Base de connaissances initialis√©e avec recherche par mots-cl√©s\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarit√© textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarit√© bas√© sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score d√©croissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"‚úÖ Nouvelle connaissance ajout√©e: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701ca9bc-448e-4100-e91c-8fe5e3ecbb78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "update_kb_rss.py"
      ],
      "metadata": {
        "id": "duM3V0Q02w8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile update_kb_rss.py\n",
        "import feedparser\n",
        "from qa_modules import OptimizedQAModule\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "import schedule\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Configuration des URLs RSS valides\n",
        "RSS_FEEDS = [\n",
        "    \"https://www.carbonbrief.org/feed/\",\n",
        "    \"https://climate.nasa.gov/news/rss.xml\",\n",
        "    \"https://unfccc.int/news/rss.xml\"\n",
        "]\n",
        "\n",
        "class RSSUpdater:\n",
        "    def __init__(self, qa_module):\n",
        "        self.qa = qa_module\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=400,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\n\", \". \", \"? \", \"! \"]\n",
        "        )\n",
        "\n",
        "    def fetch_and_process_feed(self, feed_url, max_entries=5):\n",
        "        \"\"\"R√©cup√®re et traite un flux RSS\"\"\"\n",
        "        try:\n",
        "            feed = feedparser.parse(feed_url)\n",
        "            if feed.bozo:\n",
        "                logging.warning(f\"‚ö†Ô∏è Flux RSS mal format√©: {feed_url}\")\n",
        "                return 0\n",
        "\n",
        "            processed = 0\n",
        "            for entry in feed.entries[:max_entries]:\n",
        "                try:\n",
        "                    # Extraction des informations\n",
        "                    title = entry.title if hasattr(entry, 'title') else \"Sans titre\"\n",
        "                    summary = entry.summary if hasattr(entry, 'summary') else \"\"\n",
        "                    link = entry.link if hasattr(entry, 'link') else \"Lien manquant\"\n",
        "\n",
        "                    # Nettoyage du texte\n",
        "                    content = f\"{title}. {summary}\".strip()\n",
        "                    if len(content) < 50:  # Skip trop courts\n",
        "                        continue\n",
        "\n",
        "                    # Cr√©ation du document\n",
        "                    doc = Document(\n",
        "                        page_content=content,\n",
        "                        metadata={\n",
        "                            \"url\": link,\n",
        "                            \"title\": title,\n",
        "                            \"source\": \"rss_feed\",\n",
        "                            \"date\": datetime.datetime.now().isoformat()\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                    # D√©coupage en chunks\n",
        "                    chunks = self.text_splitter.split_documents([doc])\n",
        "\n",
        "                    # Ajout √† la base de connaissances\n",
        "                    for chunk in chunks:\n",
        "                        success = self.qa.add_knowledge(\n",
        "                            text=chunk.page_content,\n",
        "                            category=\"rss_news\",\n",
        "                            keywords=[\"rss\", \"news\", \"climate\", \"update\"]\n",
        "                        )\n",
        "                        if success:\n",
        "                            processed += 1\n",
        "                            logging.info(f\"‚úÖ Ajout√©: {title[:60]}...\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"‚ùå Erreur traitement article: {e}\")\n",
        "                    continue\n",
        "\n",
        "            return processed\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur flux RSS {feed_url}: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def update_all_feeds(self):\n",
        "        \"\"\"Mise √† jour de tous les flux RSS\"\"\"\n",
        "        total_added = 0\n",
        "        logging.info(f\"üîÑ D√©but mise √† jour RSS - {datetime.datetime.now()}\")\n",
        "\n",
        "        for feed_url in RSS_FEEDS:\n",
        "            added = self.fetch_and_process_feed(feed_url)\n",
        "            total_added += added\n",
        "            logging.info(f\"üìä {feed_url}: {added} articles ajout√©s\")\n",
        "\n",
        "        logging.info(f\"‚úÖ Mise √† jour termin√©e - Total: {total_added} nouveaux articles\")\n",
        "        return total_added\n",
        "\n",
        "    def start_scheduler(self):\n",
        "        \"\"\"D√©marre le planificateur RSS\"\"\"\n",
        "        # Planification quotidienne √† 9h\n",
        "        schedule.every().day.at(\"09:00\").do(self.update_all_feeds)\n",
        "\n",
        "        # Test imm√©diat\n",
        "        self.update_all_feeds()\n",
        "\n",
        "        logging.info(\"üìÖ Planificateur RSS d√©marr√© - mise √† jour quotidienne √† 09:00\")\n",
        "\n",
        "        # Boucle d'ex√©cution\n",
        "        while True:\n",
        "            schedule.run_pending()\n",
        "            time.sleep(3600)  # V√©rification toutes les heures\n",
        "\n",
        "# Fonction utilitaire pour Streamlit\n",
        "def init_rss_updater(qa_module):\n",
        "    \"\"\"Initialise le RSS updater pour Streamlit\"\"\"\n",
        "    updater = RSSUpdater(qa_module)\n",
        "    return updater\n",
        "\n",
        "def manual_rss_update(qa_module):\n",
        "    \"\"\"Mise √† jour manuelle via Streamlit\"\"\"\n",
        "    try:\n",
        "        updater = RSSUpdater(qa_module)\n",
        "        return updater.update_all_feeds()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Erreur mise √† jour RSS: {e}\")\n",
        "        return 0"
      ],
      "metadata": {
        "id": "L726nrJs2wq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55aee367-6fc1-4bd4-952c-5372fd88de7a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting update_kb_rss.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5Ô∏è‚É£ Module Streamlit - streamlit_app.py"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "# streamlit_app_fusion.py - Version fusionn√©e avec optimisations et RSS\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from contextlib import contextmanager\n",
        "from qa_modules import OptimizedQAModule  # Module optimis√©\n",
        "\n",
        "# Configuration optimis√©e de Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"üåç Climate Analyzer Pro\",\n",
        "    page_icon=\"üåç\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Barre de progression persistante\n",
        "# ---------------------------------------------------------\n",
        "@contextmanager\n",
        "def st_progress(title=\"Progress\", max_value=100):\n",
        "    bar = st.progress(0, text=title)\n",
        "    try:\n",
        "        yield bar\n",
        "    finally:\n",
        "        bar.empty()\n",
        "\n",
        "# Cache optimis√© pour le module Q&A\n",
        "@st.cache_resource\n",
        "def get_optimized_qa():\n",
        "    \"\"\"Cache du module Q&A optimis√©\"\"\"\n",
        "    return OptimizedQAModule()\n",
        "\n",
        "# Persistance session_state - Initialisation correcte\n",
        "if \"trainer\" not in st.session_state:\n",
        "    st.session_state.trainer = None\n",
        "if \"label_names\" not in st.session_state:\n",
        "    st.session_state.label_names = None\n",
        "if \"test_ds\" not in st.session_state:\n",
        "    st.session_state.test_ds = None\n",
        "if \"training\" not in st.session_state:\n",
        "    st.session_state.training = False\n",
        "if \"raw_train_data\" not in st.session_state:\n",
        "    st.session_state.raw_train_data = None\n",
        "if \"qa_module\" not in st.session_state:\n",
        "    st.session_state.qa_module = get_optimized_qa()  # Utilisation du module optimis√©\n",
        "\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from visualization_modules import VisualizationManager\n",
        "\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "\n",
        "        # Fusion des modules Q&A\n",
        "        if st.session_state.qa_module is None:\n",
        "            st.session_state.qa_module = get_optimized_qa()\n",
        "\n",
        "        self.qa_module = st.session_state.qa_module\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.load_saved_model()\n",
        "\n",
        "    def load_saved_model(self):\n",
        "        \"\"\"Chargement automatique du mod√®le si d√©j√† pr√©sent\"\"\"\n",
        "        if st.session_state.trainer is None and os.path.exists(\"outputs/final_model/config.json\"):\n",
        "            try:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                num_labels = len(self.data_processor.label_mapping) or 2\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                trainer = self.model_manager.setup_trainer(None, None)\n",
        "                trainer.model = trainer.model.from_pretrained(\"outputs/final_model\")\n",
        "                st.session_state.trainer = trainer\n",
        "                st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "                st.success(\"‚úÖ Mod√®le charg√© depuis le disque.\")\n",
        "            except Exception as e:\n",
        "                st.warning(f\"‚ö†Ô∏è Chargement impossible : {e}\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Menu principal de l'application\"\"\"\n",
        "        st.title(\"üåç Climate Sentiment Analyzer Pro\")\n",
        "\n",
        "        # Sidebar optimis√©e avec statistiques Q&A\n",
        "        with st.sidebar:\n",
        "            st.markdown(\"### üìä Statistiques Syst√®me\")\n",
        "\n",
        "            stats = self.qa_module.get_stats()\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                st.metric(\"Base de connaissances\", stats[\"knowledge_base_size\"])\n",
        "            with col2:\n",
        "                st.metric(\"Total documents\", stats[\"total_documents\"])\n",
        "\n",
        "            # Contr√¥les rapides\n",
        "            if st.button(\"üóëÔ∏è Vider le cache Q&A\"):\n",
        "                self.qa_module.clear_cache()\n",
        "                st.success(\"Cache vid√©!\")\n",
        "                st.rerun()\n",
        "\n",
        "            # Navigation principale\n",
        "            mode = st.selectbox(\n",
        "                \"Mode\",\n",
        "                [\"üöÄ Pipeline Complet\", \"‚ùì Q&A Avanc√©e\", \"üìö Gestion des Connaissances\",\n",
        "                 \"üì∞ Mise √† jour RSS\", \"üìà Visualisations\"]\n",
        "            )\n",
        "\n",
        "        # Routage vers les diff√©rentes sections\n",
        "        if mode == \"üöÄ Pipeline Complet\":\n",
        "            self.run_complete_pipeline()\n",
        "        elif mode == \"‚ùì Q&A Avanc√©e\":\n",
        "            self.run_advanced_qa_interface()\n",
        "        elif mode == \"üìö Gestion des Connaissances\":\n",
        "            self.run_knowledge_management()\n",
        "        elif mode == \"üì∞ Mise √† jour RSS\":\n",
        "            self.run_rss_integration()\n",
        "        elif mode == \"üìà Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Pipeline complet d'entra√Ænement (inchang√© mais optimis√©)\"\"\"\n",
        "        st.header(\"üöÄ Pipeline Complet\")\n",
        "\n",
        "        uploaded_file = st.file_uploader(\"T√©l√©chargez votre CSV\", type=[\"csv\"])\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.dataframe(df.head())\n",
        "\n",
        "            # SLIDERS avec valeurs par d√©faut optimis√©es\n",
        "            sample_size = st.slider(\"Taille √©chantillon\", 1000, 10000, value=4000)\n",
        "            self.config.epochs = st.slider(\"Epochs\", 1, 5, value=3)\n",
        "\n",
        "            is_training = st.session_state.get(\"training\", False)\n",
        "\n",
        "            if st.button(\n",
        "                \"üöÄ Lancer l'entra√Ænement\",\n",
        "                type=\"primary\",\n",
        "                disabled=bool(is_training)\n",
        "            ):\n",
        "                st.session_state.training = True\n",
        "                try:\n",
        "                    self.train_pipeline(df, sample_size)\n",
        "                finally:\n",
        "                    st.session_state.training = False\n",
        "\n",
        "    def train_pipeline(self, df: pd.DataFrame, sample_size: int):\n",
        "        \"\"\"Processus d'entra√Ænement optimis√©\"\"\"\n",
        "        try:\n",
        "            # 1/4 ‚Äî Analyse des donn√©es avec cache\n",
        "            with st_progress(\"1/4  Analyse des donn√©es ‚Ä¶\") as bar:\n",
        "                train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "                bar.progress(25)\n",
        "\n",
        "            # Sauvegarder les donn√©es brutes\n",
        "            raw_train_data = [{\"text\": item[\"text\"], \"label_id\": item[\"label_id\"]}\n",
        "                            for item in train_ds]\n",
        "            st.session_state.raw_train_data = raw_train_data\n",
        "\n",
        "            # 2/4 ‚Äî Tokenizer\n",
        "            with st_progress(\"2/4  Chargement du tokenizer ‚Ä¶\") as bar:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                bar.progress(50)\n",
        "\n",
        "            # 3/4 ‚Äî Mod√®le\n",
        "            with st_progress(\"3/4  Initialisation du mod√®le ‚Ä¶\") as bar:\n",
        "                num_labels = len(self.data_processor.label_mapping)\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                bar.progress(75)\n",
        "\n",
        "            # 4/4 ‚Äî Tokenisation optimis√©e\n",
        "            def prep(ds):\n",
        "                with st_progress(\"4/4  Tokenisation ‚Ä¶\") as bar:\n",
        "                    ds = ds.map(\n",
        "                        self.model_manager.tokenize_function,\n",
        "                        batched=True,\n",
        "                        desc=\"Tokenisation\"\n",
        "                    )\n",
        "                    ds = ds.rename_column(\"label_id\", \"labels\")\n",
        "                    keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
        "                    for col in list(ds.column_names):\n",
        "                        if col not in keep:\n",
        "                            ds = ds.remove_columns(col)\n",
        "                    ds.set_format(type=\"torch\", columns=list(keep))\n",
        "                    bar.progress(100)\n",
        "                    return ds\n",
        "\n",
        "            train_ds_processed, val_ds_processed, test_ds_processed = map(prep, (train_ds, val_ds, test_ds))\n",
        "\n",
        "            trainer = self.model_manager.setup_trainer(train_ds_processed, val_ds_processed)\n",
        "\n",
        "            with st.spinner(\"Entra√Ænement en cours ‚Ä¶\"):\n",
        "                trainer.train()\n",
        "\n",
        "            trainer.save_model(\"outputs/final_model\")\n",
        "            trainer.state.save_to_json(\"outputs/final_model/trainer_state.json\")\n",
        "\n",
        "            # Indexer les donn√©es d'entra√Ænement dans le module Q&A optimis√©\n",
        "            if st.session_state.raw_train_data:\n",
        "                self.qa_module.fit(st.session_state.raw_train_data)\n",
        "                st.session_state.qa_module = self.qa_module\n",
        "\n",
        "            st.session_state.trainer = trainer\n",
        "            st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "            st.session_state.test_ds = test_ds_processed\n",
        "            st.success(\"üéâ Entra√Ænement termin√© !\")\n",
        "            st.balloons()\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur : {e}\")\n",
        "            import traceback\n",
        "            st.error(f\"D√©tail: {traceback.format_exc()}\")\n",
        "            st.session_state.training = False\n",
        "\n",
        "    def run_advanced_qa_interface(self):\n",
        "        \"\"\"Interface Q&A avanc√©e fusionn√©e avec optimisations\"\"\"\n",
        "        st.header(\"‚ùì Interface Q&A Avanc√©e\")\n",
        "\n",
        "        # Configuration de recherche avec colonnes optimis√©es\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "\n",
        "        with col1:\n",
        "            question = st.text_input(\n",
        "                \"Posez votre question sur le climat :\",\n",
        "                placeholder=\"Ex: Quelles sont les causes du r√©chauffement climatique ?\"\n",
        "            )\n",
        "\n",
        "        with col2:\n",
        "            search_mode = st.selectbox(\n",
        "                \"Mode\",\n",
        "                [\"hybrid\", \"knowledge_only\", \"corpus_only\", \"keywords\"],\n",
        "                format_func=lambda x: {\n",
        "                    \"hybrid\": \"üîÄ Hybride\",\n",
        "                    \"knowledge_only\": \"üìö Base de connaissances\",\n",
        "                    \"corpus_only\": \"üìä Corpus d'entra√Ænement\",\n",
        "                    \"keywords\": \"üîç Mots-cl√©s\"\n",
        "                }[x]\n",
        "            )\n",
        "\n",
        "        # Param√®tres avanc√©s dans l'expandeur\n",
        "        with st.expander(\"‚öôÔ∏è Param√®tres de recherche\", expanded=False):\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                top_k = st.slider(\"Nombre de r√©sultats\", 1, 10, value=5)\n",
        "            with col2:\n",
        "                show_details = st.checkbox(\"Afficher les d√©tails\", True)\n",
        "\n",
        "        # Recherche principale avec spinner optimis√©\n",
        "        if question:\n",
        "            try:\n",
        "                with st.spinner(\"üîç Recherche intelligente en cours...\"):\n",
        "                    results = self.qa_module.query_with_fallback(question, top_k, search_mode)\n",
        "\n",
        "                self.display_qa_results(results, show_details, f\"R√©sultats pour: '{question}'\")\n",
        "\n",
        "                # Analyse des r√©sultats dans un expander\n",
        "                if results and len(results) > 1:\n",
        "                    with st.expander(\"üìä Analyse des r√©sultats\", expanded=False):\n",
        "                        self.visualizer.plot_qa_results_analysis(results, question)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå Erreur : {str(e)}\")\n",
        "                if st.checkbox(\"Afficher les d√©tails techniques\"):\n",
        "                    st.exception(e)\n",
        "\n",
        "        # Questions sugg√©r√©es avec boutons\n",
        "        st.markdown(\"### üí° Questions sugg√©r√©es\")\n",
        "        suggested_questions = [\n",
        "            \"Quelles sont les principales causes du r√©chauffement climatique ?\",\n",
        "            \"Comment les √©nergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la d√©forestation sur le climat ?\",\n",
        "            \"Comment r√©duire les √©missions de gaz √† effet de serre ?\"\n",
        "        ]\n",
        "\n",
        "        cols = st.columns(2)\n",
        "        for i, suggestion in enumerate(suggested_questions[:4]):\n",
        "            if cols[i % 2].button(\n",
        "                suggestion[:50] + \"...\" if len(suggestion) > 50 else suggestion,\n",
        "                key=f\"suggestion_{i}\",\n",
        "                use_container_width=True\n",
        "            ):\n",
        "                st.session_state[\"question\"] = suggestion\n",
        "                st.rerun()\n",
        "\n",
        "    def display_qa_results(self, results: list, show_details: bool, title: str):\n",
        "        \"\"\"Affichage optimis√© des r√©sultats Q&A\"\"\"\n",
        "        if not results:\n",
        "            st.info(\"üîç Aucun r√©sultat trouv√©.\")\n",
        "            return\n",
        "\n",
        "        st.markdown(f\"### {title}\")\n",
        "        st.caption(f\"{len(results)} r√©sultat(s) trouv√©(s)\")\n",
        "\n",
        "        for result in results:\n",
        "            # Emoji selon la source\n",
        "            emoji = {\n",
        "                \"knowledge_base\": \"üìö\",\n",
        "                \"training_corpus\": \"üìä\",\n",
        "                \"keywords\": \"üîç\"\n",
        "            }.get(result.get(\"source\"), \"üìÑ\")\n",
        "\n",
        "            # Couleur selon le score\n",
        "            score = result.get(\"score\", 0)\n",
        "            score_color = \"üü¢\" if score > 0.7 else \"üü°\" if score > 0.4 else \"üî¥\"\n",
        "\n",
        "            with st.expander(\n",
        "                f\"{emoji} Score: {score_color} {score:.3f} - {result.get('source', 'source').replace('_', ' ').title()}\",\n",
        "                expanded=(result.get(\"rank\", 0) == 1)\n",
        "            ):\n",
        "                st.write(\"**Texte:**\")\n",
        "                st.write(result[\"text\"])\n",
        "\n",
        "                if show_details:\n",
        "                    st.divider()\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    with col1:\n",
        "                        if \"category\" in result:\n",
        "                            st.caption(f\"**Cat√©gorie:** `{result['category']}`\")\n",
        "                        if \"keywords\" in result and result[\"keywords\"]:\n",
        "                            st.caption(\"**Mots-cl√©s:** \" + \" \".join([f\"`{kw}`\" for kw in result[\"keywords\"][:3]]))\n",
        "\n",
        "                    with col2:\n",
        "                        st.caption(f\"**Rang:** {result.get('rank', '?')}\")\n",
        "\n",
        "    def run_knowledge_management(self):\n",
        "        \"\"\"Interface de gestion optimis√©e de la base de connaissances\"\"\"\n",
        "        st.header(\"üìö Gestion des Connaissances\")\n",
        "\n",
        "        tab1, tab2, tab3 = st.tabs([\"üìñ Consulter\", \"‚ûï Ajouter\", \"üìä Statistiques\"])\n",
        "\n",
        "        with tab1:\n",
        "            self._render_knowledge_browser()\n",
        "\n",
        "        with tab2:\n",
        "            self._render_knowledge_adder()\n",
        "\n",
        "        with tab3:\n",
        "            self._render_knowledge_stats()\n",
        "\n",
        "    def _render_knowledge_browser(self):\n",
        "        \"\"\"Sous-composant pour naviguer dans la base de connaissances\"\"\"\n",
        "        st.subheader(\"üìñ Consultation\")\n",
        "\n",
        "        # Filtres avec colonnes\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            categories = self.qa_module.get_categories()\n",
        "            filter_category = st.selectbox(\"Filtrer par\", [\"Toutes\"] + categories)\n",
        "        with col2:\n",
        "            search_text = st.text_input(\"Rechercher\", placeholder=\"Mots-cl√©s...\")\n",
        "\n",
        "        # Filtrage et affichage\n",
        "        knowledge_items = []\n",
        "        for idx, item in enumerate(self.qa_module.knowledge_base):\n",
        "            if filter_category == \"Toutes\" or item[\"category\"] == filter_category:\n",
        "                if not search_text or search_text.lower() in item[\"text\"].lower():\n",
        "                    knowledge_items.append((idx, item))\n",
        "\n",
        "        st.info(f\"üìÑ {len(knowledge_items)} document(s) trouv√©(s)\")\n",
        "\n",
        "        # Affichage pagin√© pour performance\n",
        "        items_per_page = 5\n",
        "        page = st.number_input(\"Page\", min_value=1, max_value=max(1, len(knowledge_items)//items_per_page + 1), value=1)\n",
        "\n",
        "        start_idx = (page - 1) * items_per_page\n",
        "        end_idx = min(start_idx + items_per_page, len(knowledge_items))\n",
        "\n",
        "        for idx, (orig_idx, item) in enumerate(knowledge_items[start_idx:end_idx], start=1):\n",
        "            with st.expander(f\"üìÑ {item['category'].upper()} - {item['text'][:80]}...\"):\n",
        "                st.write(item[\"text\"])\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    st.caption(f\"ID: `{orig_idx}`\")\n",
        "                with col2:\n",
        "                    if item[\"keywords\"]:\n",
        "                        st.caption(\"Keywords: \" + \", \".join(item[\"keywords\"][:3]))\n",
        "\n",
        "    def _render_knowledge_adder(self):\n",
        "        \"\"\"Sous-composant pour ajouter des connaissances\"\"\"\n",
        "        st.subheader(\"‚ûï Ajouter une connaissance\")\n",
        "\n",
        "        with st.form(\"add_knowledge_form\"):\n",
        "            new_text = st.text_area(\n",
        "                \"Texte\",\n",
        "                placeholder=\"Entrez le texte de la nouvelle connaissance...\",\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                categories = self.qa_module.get_categories()\n",
        "                category_choice = st.selectbox(\"Cat√©gorie\", [\"Nouvelle\"] + categories)\n",
        "\n",
        "                if category_choice == \"Nouvelle\":\n",
        "                    new_category = st.text_input(\"Nouvelle cat√©gorie\", placeholder=\"technologies\")\n",
        "                    final_category = new_category\n",
        "                else:\n",
        "                    final_category = category_choice\n",
        "\n",
        "            with col2:\n",
        "                keywords = st.text_input(\n",
        "                    \"Mots-cl√©s\",\n",
        "                    placeholder=\"tech, innovation, futur\"\n",
        "                )\n",
        "\n",
        "            submitted = st.form_submit_button(\"‚úÖ Ajouter\", use_container_width=True)\n",
        "\n",
        "            if submitted and new_text and final_category:\n",
        "                keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
        "\n",
        "                if self.qa_module.add_knowledge(new_text, final_category, keyword_list):\n",
        "                    st.success(\"‚úÖ Ajout√© avec succ√®s!\")\n",
        "                    st.rerun()\n",
        "                else:\n",
        "                    st.error(\"‚ùå Erreur lors de l'ajout\")\n",
        "\n",
        "    def _render_knowledge_stats(self):\n",
        "        \"\"\"Sous-composant pour les statistiques\"\"\"\n",
        "        st.subheader(\"üìä Statistiques\")\n",
        "        stats = self.qa_module.get_stats()\n",
        "\n",
        "        # M√©triques principales\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        col1.metric(\"Total\", stats[\"total_documents\"])\n",
        "        col2.metric(\"KB\", stats[\"knowledge_base_size\"])\n",
        "        col3.metric(\"Corpus\", stats[\"training_corpus_size\"])\n",
        "        col4.metric(\"Cat√©gories\", len(stats[\"categories\"]))\n",
        "\n",
        "        # Graphiques si des donn√©es existent\n",
        "        if self.qa_module.knowledge_base:\n",
        "            # Distribution par cat√©gorie\n",
        "            category_counts = {}\n",
        "            for item in self.qa_module.knowledge_base:\n",
        "                cat = item[\"category\"]\n",
        "                category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "            df_categories = pd.DataFrame([\n",
        "                {\"Cat√©gorie\": cat, \"Nombre\": count}\n",
        "                for cat, count in category_counts.items()\n",
        "            ])\n",
        "\n",
        "            st.bar_chart(df_categories.set_index(\"Cat√©gorie\"))\n",
        "\n",
        "    def run_rss_integration(self):\n",
        "        \"\"\"Section RSS dans l'interface Streamlit\"\"\"\n",
        "        st.header(\"üì∞ Mise √† jour RSS Automatique\")\n",
        "\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "\n",
        "        with col1:\n",
        "            st.info(\"üì° Flux RSS configur√©s:\")\n",
        "            for url in [\n",
        "                \"Carbon Brief\",\n",
        "                \"NASA Climate\",\n",
        "                \"UNFCCC News\"\n",
        "            ]:\n",
        "                st.write(f\"‚Ä¢ {url}\")\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"üîÑ Mise √† jour manuelle\", type=\"primary\"):\n",
        "                with st.spinner(\"Mise √† jour en cours...\"):\n",
        "                    try:\n",
        "                        from update_kb_rss import manual_rss_update\n",
        "                        added = manual_rss_update(self.qa_module)\n",
        "                        if added > 0:\n",
        "                            st.success(f\"‚úÖ {added} articles ajout√©s\")\n",
        "                            st.rerun()\n",
        "                        else:\n",
        "                            st.info(\"Aucun nouvel article trouv√©\")\n",
        "                    except ImportError:\n",
        "                        st.error(\"‚ùå Module update_kb_rss non trouv√©\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"‚ùå Erreur lors de la mise √† jour : {str(e)}\")\n",
        "\n",
        "        # Param√®tres avanc√©s\n",
        "        with st.expander(\"‚öôÔ∏è Param√®tres RSS\"):\n",
        "            st.write(\"Mise √† jour automatique activ√©e\")\n",
        "            st.write(\"Fr√©quence: Quotidienne √† 09:00\")\n",
        "            st.write(\"Sources: Carbon Brief, NASA Climate, UNFCCC\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        \"\"\"Interface des visualisations (inchang√©e mais avec optimisations)\"\"\"\n",
        "        st.header(\"üìà Visualisations\")\n",
        "\n",
        "        viz = st.selectbox(\n",
        "            \"Choisir une visualisation\",\n",
        "            [\"Distribution des classes\", \"Matrice de confusion\", \"Courbes d'entra√Ænement\",\n",
        "             \"üìä M√©triques BLEU/ROUGE\", \"üîç √âvaluation Q&A\", \"üìö Analyse Knowledge Base\"]\n",
        "        )\n",
        "\n",
        "        test_ds = st.session_state.get(\"test_ds\")\n",
        "        label_names = st.session_state.get(\"label_names\")\n",
        "\n",
        "        try:\n",
        "            if viz == \"Distribution des classes\" and test_ds:\n",
        "                self.visualizer.plot_class_distribution(test_ds[\"labels\"], label_names)\n",
        "            elif viz == \"Matrice de confusion\" and test_ds and st.session_state.trainer:\n",
        "                self.visualizer.show_confusion_matrix(st.session_state.trainer, test_ds, label_names)\n",
        "            elif viz == \"Courbes d'entra√Ænement\":\n",
        "                self.visualizer.plot_training_curves(\"outputs/final_model\")\n",
        "            elif viz == \"üìä M√©triques BLEU/ROUGE\":\n",
        "                self.run_bleu_rouge_analysis()\n",
        "            elif viz == \"üîç √âvaluation Q&A\":\n",
        "                self.run_qa_evaluation()\n",
        "            elif viz == \"üìö Analyse Knowledge Base\":\n",
        "                self.run_knowledge_base_analysis()\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur : {e}\")\n",
        "            if st.checkbox(\"D√©tails techniques\"):\n",
        "                st.exception(e)\n",
        "\n",
        "    def run_knowledge_base_analysis(self):\n",
        "        \"\"\"Analyse optimis√©e de la base de connaissances\"\"\"\n",
        "        st.subheader(\"üìö Analyse de la Base de Connaissances\")\n",
        "\n",
        "        if not self.qa_module.knowledge_base:\n",
        "            st.info(\"La base de connaissances est vide.\")\n",
        "            return\n",
        "\n",
        "        # Mots-cl√©s fr√©quents\n",
        "        all_keywords = [kw for item in self.qa_module.knowledge_base for kw in item[\"keywords\"]]\n",
        "\n",
        "        if all_keywords:\n",
        "            from collections import Counter\n",
        "            top_keywords = Counter(all_keywords).most_common(10)\n",
        "            df_keywords = pd.DataFrame(top_keywords, columns=[\"Mot-cl√©\", \"Fr√©quence\"])\n",
        "            st.bar_chart(df_keywords.set_index(\"Mot-cl√©\"))\n",
        "\n",
        "        # Longueurs de texte\n",
        "        lengths = [len(item[\"text\"]) for item in self.qa_module.knowledge_base]\n",
        "\n",
        "        if lengths:\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Min\", f\"{min(lengths)}\")\n",
        "            col2.metric(\"Moyenne\", f\"{np.mean(lengths):.0f}\")\n",
        "            col3.metric(\"Max\", f\"{max(lengths)}\")\n",
        "\n",
        "    def run_bleu_rouge_analysis(self):\n",
        "        \"\"\"Analyse BLEU/ROUGE (conserv√©e avec optimisations)\"\"\"\n",
        "        st.subheader(\"üìä Analyse des m√©triques BLEU et ROUGE\")\n",
        "\n",
        "        # Code inchang√© mais avec gestion d'erreurs am√©lior√©e\n",
        "        try:\n",
        "            # Section test personnalis√©\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                reference_text = st.text_area(\n",
        "                    \"Texte de r√©f√©rence:\",\n",
        "                    \"Le r√©chauffement climatique est un ph√©nom√®ne global caus√© par les activit√©s humaines.\",\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            with col2:\n",
        "                candidate_text = st.text_area(\n",
        "                    \"Texte candidat:\",\n",
        "                    \"Le changement climatique est un probl√®me mondial d√ª aux actions humaines.\",\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            if st.button(\"Calculer les scores\", use_container_width=True):\n",
        "                bleu_score = self.visualizer.calculate_bleu_score(reference_text, candidate_text)\n",
        "                rouge_scores = self.visualizer.calculate_rouge_score(reference_text, candidate_text)\n",
        "\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                col1.metric(\"BLEU\", f\"{bleu_score:.4f}\")\n",
        "                col2.metric(\"ROUGE-1\", f\"{rouge_scores['rouge-1']:.4f}\")\n",
        "                col3.metric(\"ROUGE-L\", f\"{rouge_scores['rouge-l']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur dans l'analyse BLEU/ROUGE : {e}\")\n",
        "\n",
        "    def run_qa_evaluation(self):\n",
        "        \"\"\"√âvaluation Q&A avec optimisations\"\"\"\n",
        "        st.subheader(\"üîç √âvaluation du syst√®me Q&A\")\n",
        "\n",
        "        # Questions de test avec suggestions\n",
        "        default_questions = [\n",
        "            \"Quelles sont les principales causes du r√©chauffement climatique ?\",\n",
        "            \"Comment les √©nergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la d√©forestation sur le climat ?\"\n",
        "        ]\n",
        "\n",
        "        if st.checkbox(\"Utiliser questions par d√©faut\", value=True):\n",
        "            test_questions = default_questions\n",
        "        else:\n",
        "            test_questions = st.text_area(\n",
        "                \"Entrez vos questions (une par ligne):\",\n",
        "                height=100\n",
        "            ).split(\"\\n\")\n",
        "\n",
        "        if test_questions and st.button(\"üöÄ Lancer l'√©valuation\", type=\"primary\"):\n",
        "            with st.spinner(\"√âvaluation en cours...\"):\n",
        "                # G√©n√©ration des r√©f√©rences\n",
        "                reference_answers = []\n",
        "                for question in test_questions:\n",
        "                    kb_results = self.qa_module.query_knowledge_base(question, top_k=1)\n",
        "                    ref = kb_results[0]['text'] if kb_results else \"R√©f√©rence manquante\"\n",
        "                    reference_answers.append(ref)\n",
        "\n",
        "                self.visualizer.evaluate_qa_performance(\n",
        "                    self.qa_module,\n",
        "                    test_questions,\n",
        "                    reference_answers\n",
        "                )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = ClimateAnalyzerApp()\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96527f4-daab-40af-da57-79e983601aa1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6Ô∏è‚É£ Script d'Installation - setup_pipeline.py"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\",\n",
        "        \"rouge-score\",\n",
        "        \"nltk\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "            print(f\"‚úÖ {package} install√©\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur avec {package}: {e}\")\n",
        "\n",
        "    # T√©l√©chargement des ressources NLTK\n",
        "    import nltk\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"‚úÖ Ressources NLTK pr√™tes\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465431a7-4073-42f7-edf8-ac9ca8860e48"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492b1426-23d6-462d-bd98-a1250159b930"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "‚úÖ transformers>=4.36.0 install√©\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "‚úÖ datasets>=2.16.0 install√©\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "‚úÖ torch>=2.1.0 install√©\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "‚úÖ peft>=0.7.0 install√©\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "‚úÖ sentence-transformers>=2.2.0 install√©\n",
            "Requirement already satisfied: faiss-cpu>=1.7.0 in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "‚úÖ faiss-cpu>=1.7.0 install√©\n",
            "Requirement already satisfied: streamlit>=1.29.0 in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "‚úÖ streamlit>=1.29.0 install√©\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "‚úÖ plotly>=5.17.0 install√©\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "‚úÖ scikit-learn>=1.3.0 install√©\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "‚úÖ matplotlib>=3.7.0 install√©\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "‚úÖ seaborn>=0.12.0 install√©\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "‚úÖ pandas>=1.5.0 install√©\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "‚úÖ numpy>=1.24.0 install√©\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "‚úÖ rouge-score install√©\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "‚úÖ nltk install√©\n",
            "‚úÖ Ressources NLTK pr√™tes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "-MEnnZWsOuL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94cd4e5e-f754-4bcd-a0d0-3ed3cf51b6f5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "Y3wskx_fZLEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b25e9c5-38df-4afa-b58c-3fc8461d11b0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K3WFMzkb6p9",
        "outputId": "e12a2e04-772e-4a4b-9d12-3d21f5da7277"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Lancement Streamlit + ngrok (version corrig√©e)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1Ô∏è‚É£ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2Ô∏è‚É£ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Attendre et cr√©er le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üöÄ Interface Streamlit disponible √† :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "IFyKJzjISjWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049b9cbf-d40e-4ae7-aee2-d06d3e9c7f41"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "üöÄ Interface Streamlit disponible √† :\n",
            "NgrokTunnel: \"https://0c1c5f32419e.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}