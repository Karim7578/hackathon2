{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Module Core - core_modules.py"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py - Configuration optimisée\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralisée et optimisée\"\"\"\n",
        "    model_name: str = \"distilbert-base-uncased\"\n",
        "    max_length: int = 128\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 3\n",
        "    learning_rate: float = 2e-5\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    output_dir: str = \"outputs/final_model\"\n",
        "\n",
        "    # Configuration Q&A\n",
        "    qa_model: str = \"all-MiniLM-L6-v2\"\n",
        "    similarity_threshold: float = 0.3\n",
        "    max_results: int = 5"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b36eb8a-9a19-44fe-94b6-2313caa6354a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Module Data Processing - data_modules.py"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Tuple\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "        self.reverse_label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        text_keywords = ['text', 'content', 'message', 'comment', 'body', 'description', 'self_text']\n",
        "        label_keywords = ['label', 'sentiment', 'category', 'class', 'target', 'comment_sentiment']\n",
        "        text_col = next((c for c in df.columns if any(k in str(c).lower() for k in text_keywords)), None)\n",
        "        label_col = next((c for c in df.columns if any(k in str(c).lower() for k in label_keywords)), None)\n",
        "        if not text_col:\n",
        "            text_col = df.select_dtypes(include=['object']).columns[0]\n",
        "        if not label_col:\n",
        "            label_col = df.columns[-1]\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text) or str(text).strip().lower() in ['nan', 'none', '', 'null']:\n",
        "            return None\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r'&gt;|&lt;|&amp;', lambda m: {'&gt;': '>', '&lt;': '<', '&amp;': '&'}[m.group()], text)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip() if text.strip() else None\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "        df_clean = df[[self.text_col, self.label_col]].copy()\n",
        "        df_clean.columns = ['text', 'label']\n",
        "        df_clean['text'] = df_clean['text'].apply(self.clean_text)\n",
        "        df_clean['label'] = df_clean['label'].astype(str)\n",
        "        df_clean = df_clean.dropna().reset_index(drop=True)\n",
        "        df_clean = df_clean[df_clean['text'].str.len() >= 10]\n",
        "\n",
        "        if len(df_clean) > sample_size:\n",
        "            df_clean = df_clean.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        unique_labels = sorted(df_clean['label'].unique())\n",
        "        self.label_mapping = {str(l): i for i, l in enumerate(unique_labels)}\n",
        "        df_clean['label_id'] = df_clean['label'].map(self.label_mapping)\n",
        "\n",
        "        # Nettoyage final NaN\n",
        "        df_clean = df_clean.dropna(subset=['label_id'])\n",
        "        df_clean['label_id'] = df_clean['label_id'].astype(int)\n",
        "\n",
        "        if df_clean.empty:\n",
        "            raise ValueError(\"❌ Aucune donnée valide après nettoyage.\")\n",
        "\n",
        "        train_df, temp = train_test_split(df_clean, test_size=0.4, stratify=df_clean['label_id'], random_state=42)\n",
        "        val_df, test_df = train_test_split(temp, test_size=0.5, stratify=temp['label_id'], random_state=42)\n",
        "\n",
        "        return (\n",
        "            Dataset.from_pandas(train_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(val_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(test_df[['text', 'label_id']])\n",
        "        )"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45fbc009-7d81-4d1c-f16f-828dd598750b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3️⃣ Module Modèle - model_modules.py"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        return self.tokenizer\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float32,\n",
        "            problem_type=\"single_label_classification\"\n",
        "        )\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=self.config.lora_r,\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_lin\", \"v_lin\"],\n",
        "            bias=\"none\",\n",
        "        )\n",
        "        self.peft_model = get_peft_model(base_model, lora_config)\n",
        "        return self.peft_model\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.config.max_length,\n",
        "        )\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        preds = np.argmax(predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "        }\n",
        "\n",
        "    def setup_training_args(self):\n",
        "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.config.output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            save_steps=500,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "            fp16=False,\n",
        "            bf16=False,\n",
        "            fp16_full_eval=False,\n",
        "            bf16_full_eval=False,\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        return Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=self.setup_training_args(),\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "        )"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca21fdff-fd12-4140-c30c-4c204298b5e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. visualization_modules.py"
      ],
      "metadata": {
        "id": "qYJhFun3Mws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# --- NLTK / BLEU / ROUGE ---\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Téléchargement silencieux des ressources NLTK\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "plt.style.use('default')\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestionnaire de visualisations pour Climate Analyzer.\"\"\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Outils internes BLEU / ROUGE\n",
        "    # --------------------------------------------------\n",
        "    _smoothie = SmoothingFunction().method4\n",
        "    _rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _bleu(ref: str, hyp: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        ref_tok = nltk.word_tokenize(ref.lower())\n",
        "        hyp_tok = nltk.word_tokenize(hyp.lower())\n",
        "        return sentence_bleu([ref_tok], hyp_tok, smoothing_function=VisualizationManager._smoothie)\n",
        "\n",
        "    @staticmethod\n",
        "    def _rouge_score(ref: str, hyp: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        scores = VisualizationManager._rouge_scorer.score(ref.lower(), hyp.lower())\n",
        "        return {'rouge-1': scores['rouge1'].fmeasure,\n",
        "                'rouge-l': scores['rougeL'].fmeasure}\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1) Courbes d’entraînement\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str = \"outputs/final_model\"):\n",
        "        try:\n",
        "            log_file = os.path.join(log_dir, \"trainer_state.json\")\n",
        "            if not os.path.exists(log_file):\n",
        "                st.warning(\"📄 Aucun log d'entraînement trouvé.\")\n",
        "                return\n",
        "\n",
        "            with open(log_file, 'r', encoding='utf-8') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"📉 Aucune donnée d'historique trouvée.\")\n",
        "                return\n",
        "\n",
        "            epochs, train_loss, eval_loss, eval_acc, eval_f1 = [], [], [], [], []\n",
        "\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_acc.append(entry.get('eval_accuracy', 0))\n",
        "                    eval_f1.append(entry.get('eval_f1_weighted', 0))\n",
        "                elif 'train_loss' in entry:\n",
        "                    train_loss.append(entry.get('train_loss', 0))\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(\"📈 Évolution de l'entraînement\", fontsize=16)\n",
        "\n",
        "            if train_loss and eval_loss:\n",
        "                train_steps = np.linspace(0, max(epochs) if epochs else 1, len(train_loss))\n",
        "                axes[0, 0].plot(train_steps, train_loss, 'b-', label='Train Loss', alpha=0.7)\n",
        "                axes[0, 0].plot(epochs[:len(eval_loss)], eval_loss, 'r-o', label='Eval Loss', markersize=4)\n",
        "                axes[0, 0].set_title('Loss Evolution')\n",
        "                axes[0, 0].legend()\n",
        "                axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc:\n",
        "                axes[0, 1].plot(epochs[:len(eval_acc)], eval_acc, 'g-o', label='Accuracy', markersize=4)\n",
        "                axes[0, 1].set_title('Accuracy Evolution')\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_f1:\n",
        "                axes[1, 0].plot(epochs[:len(eval_f1)], eval_f1, 'm-o', label='F1-Weighted', markersize=4)\n",
        "                axes[1, 0].set_title('F1-Score Evolution')\n",
        "                axes[1, 0].legend()\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc and eval_f1:\n",
        "                final_metrics = ['Accuracy', 'F1-Score']\n",
        "                final_values = [eval_acc[-1], eval_f1[-1]]\n",
        "                bars = axes[1, 1].bar(final_metrics, final_values, color=['green', 'purple'], alpha=0.7)\n",
        "                axes[1, 1].set_title('Final Metrics')\n",
        "                axes[1, 1].set_ylim(0, 1)\n",
        "                for bar, value in zip(bars, final_values):\n",
        "                    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                                   f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage des courbes : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2) Matrice de confusion\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names: List[str]):\n",
        "        try:\n",
        "            predictions_output = trainer.predict(test_dataset)\n",
        "            predictions = predictions_output.predictions.argmax(axis=1)\n",
        "            true_labels = predictions_output.label_ids\n",
        "\n",
        "            cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax1)\n",
        "            ax1.set_title(\"Matrice de confusion\")\n",
        "            ax1.set_xlabel(\"Prédictions\")\n",
        "            ax1.set_ylabel(\"Vraies valeurs\")\n",
        "\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax2)\n",
        "            ax2.set_title(\"Matrice de confusion (normalisée)\")\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            report = classification_report(true_labels, predictions,\n",
        "                                         target_names=label_names,\n",
        "                                         output_dict=True, zero_division=0)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.subheader(\"📊 Rapport de classification\")\n",
        "            st.dataframe(report_df.round(3))\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage de la matrice de confusion : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3) Distribution des classes\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(labels, label_names: List[str] = None, title: str = \"Distribution des classes\"):\n",
        "        try:\n",
        "            if hasattr(labels, 'tolist'):\n",
        "                labels = labels.tolist()\n",
        "            labels = [int(x) for x in labels]\n",
        "            label_counts = Counter(labels)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            if label_names:\n",
        "                x_labels = [label_names[i] if i < len(label_names) else f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "            else:\n",
        "                x_labels = [f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "\n",
        "            bars = ax.bar(range(len(x_labels)), counts, color=plt.cm.Set3(np.linspace(0, 1, len(x_labels))))\n",
        "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "            ax.set_xlabel(\"Classes\")\n",
        "            ax.set_ylabel(\"Nombre d'échantillons\")\n",
        "            ax.set_xticks(range(len(x_labels)))\n",
        "            ax.set_xticklabels(x_labels, rotation=45 if max(map(len, x_labels)) > 10 else 0)\n",
        "\n",
        "            for bar, count in zip(bars, counts):\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
        "                       str(count), ha='center', va='bottom')\n",
        "            total = sum(counts)\n",
        "            ax.text(0.02, 0.98, f\"Total: {total}\\nClasses: {len(x_labels)}\",\n",
        "                   transform=ax.transAxes, va='top', ha='left',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.7))\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage de la distribution : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 4) Analyse des résultats Q&A\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_qa_results_analysis(qa_results: List[Dict], question: str):\n",
        "        if not qa_results:\n",
        "            st.info(\"Aucun résultat à analyser\")\n",
        "            return\n",
        "        try:\n",
        "            scores = [r['score'] for r in qa_results]\n",
        "            ranks = [r['rank'] for r in qa_results]\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(f\"Analyse des résultats pour: '{question[:50]}...'\", fontsize=14)\n",
        "\n",
        "            axes[0, 0].hist(scores, bins=min(10, len(scores)), alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            axes[0, 0].set_title(\"Distribution des scores de similarité\")\n",
        "            axes[0, 0].axvline(np.mean(scores), color='red', linestyle='--', label=f'Moyenne: {np.mean(scores):.3f}')\n",
        "            axes[0, 0].legend()\n",
        "\n",
        "            axes[0, 1].bar(ranks, scores, color='lightcoral', alpha=0.7)\n",
        "            axes[0, 1].set_title(\"Scores par rang\")\n",
        "            axes[0, 1].set_xlabel(\"Rang\")\n",
        "\n",
        "            text_lengths = [len(r['text']) for r in qa_results]\n",
        "            axes[1, 0].scatter(text_lengths, scores, alpha=0.6, color='green')\n",
        "            axes[1, 0].set_title(\"Score vs Longueur du texte\")\n",
        "            axes[1, 0].set_xlabel(\"Longueur du texte\")\n",
        "\n",
        "            top_scores = scores[:min(5, len(scores))]\n",
        "            top_ranks = ranks[:min(5, len(ranks))]\n",
        "            axes[1, 1].barh(range(len(top_scores)), top_scores, color='purple', alpha=0.7)\n",
        "            axes[1, 1].set_title(\"Top 5 des scores\")\n",
        "            axes[1, 1].set_yticks(range(len(top_scores)))\n",
        "            axes[1, 1].set_yticklabels([f\"Rang {r}\" for r in top_ranks])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            st.subheader(\"📈 Statistiques détaillées\")\n",
        "            stats_df = pd.DataFrame({\n",
        "                \"Métrique\": [\"Score moyen\", \"Score médian\", \"Score max\", \"Score min\", \"Écart-type\"],\n",
        "                \"Valeur\": [np.mean(scores), np.median(scores), np.max(scores), np.min(scores), np.std(scores)]\n",
        "            })\n",
        "            st.dataframe(stats_df.round(4))\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'analyse des résultats Q&A : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 5) Méthodes BLEU / ROUGE manquantes\n",
        "    # --------------------------------------------------\n",
        "    def calculate_bleu_score(self, reference: str, candidate: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        return self._bleu(reference, candidate)\n",
        "\n",
        "    def calculate_rouge_score(self, reference: str, candidate: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        return self._rouge_score(reference, candidate)\n",
        "\n",
        "    def visualize_bleu_rouge_scores(self, qa_results, references):\n",
        "        \"\"\"Visualisation BLEU & ROUGE pour chaque paire (ref, résultat).\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for ref, res in zip(references, qa_results):\n",
        "            bleus.append(self._bleu(ref, res['text']))\n",
        "            r1s.append(self._rouge_score(ref, res['text'])['rouge-1'])\n",
        "            rls.append(self._rouge_score(ref, res['text'])['rouge-l'])\n",
        "\n",
        "        x = list(range(1, len(bleus)+1))\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar([i-0.2 for i in x], bleus, 0.4, label='BLEU')\n",
        "        plt.bar([i+0.2 for i in x], r1s, 0.4, label='ROUGE-1')\n",
        "        plt.xlabel('Rang')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('BLEU & ROUGE vs références')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(plt.gcf())\n",
        "\n",
        "    def evaluate_qa_performance(self, qa_module, questions, references):\n",
        "        \"\"\"Évaluation complète Q-A avec scores BLEU/ROUGE.\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for q, ref in zip(questions, references):\n",
        "            res = qa_module.query_with_fallback(q, top_k=1)\n",
        "            if res:\n",
        "                cand = res[0]['text']\n",
        "                bleus.append(self._bleu(ref, cand))\n",
        "                r1s.append(self._rouge_score(ref, cand)['rouge-1'])\n",
        "                rls.append(self._rouge_score(ref, cand)['rouge-l'])\n",
        "\n",
        "        st.write(\"### 📊 Global Q-A metrics\")\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        col1.metric(\"Avg BLEU\", f\"{np.mean(bleus):.4f}\")\n",
        "        col2.metric(\"Avg ROUGE-1\", f\"{np.mean(r1s):.4f}\")\n",
        "        col3.metric(\"Avg ROUGE-L\", f\"{np.mean(rls):.4f}\")"
      ],
      "metadata": {
        "id": "oMZt-WwIMwez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "317787a1-d66a-4b0b-84e4-d4cf6a62f95c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting visualization_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. qa_modules.py"
      ],
      "metadata": {
        "id": "1HSXVYNkNc-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py - Version corrigée et optimisée\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "import logging\n",
        "from functools import lru_cache\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "class OptimizedQAModule:\n",
        "    \"\"\"Version optimisée du module Q&A avec caching et performance améliorée\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        try:\n",
        "            self.model = SentenceTransformer(model_name, device='cpu')\n",
        "            self.similarity_threshold = 0.3\n",
        "\n",
        "            # Structures optimisées\n",
        "            self.corpus_texts: List[str] = []\n",
        "            self.corpus_labels: List[int] = []\n",
        "            self.corpus_embeddings: Optional[np.ndarray] = None\n",
        "            self.corpus_index: Optional[faiss.Index] = None\n",
        "\n",
        "            # Base de connaissances optimisée\n",
        "            self.knowledge_base: List[Dict[str, Any]] = []\n",
        "            self.kb_embeddings: Optional[np.ndarray] = None\n",
        "            self.kb_index: Optional[faiss.Index] = None\n",
        "\n",
        "            # Cache pour les requêtes fréquentes\n",
        "            self._query_cache: Dict[str, List[Dict]] = {}\n",
        "\n",
        "            self._setup_knowledge_base()\n",
        "            logging.info(\"✅ Module Q&A initialisé avec succès\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur initialisation Q&A: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _setup_knowledge_base(self):\n",
        "        \"\"\"Initialisation optimisée de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            {\n",
        "                \"text\": \"Le réchauffement climatique est principalement causé par les émissions de CO2 humaines.\",\n",
        "                \"category\": \"causes\",\n",
        "                \"keywords\": [\"CO2\", \"émissions\", \"humaines\", \"causes\"],\n",
        "                \"weight\": 1.0\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Les énergies renouvelables (solaire, éolien, hydro) réduisent drastiquement les émissions.\",\n",
        "                \"category\": \"solutions\",\n",
        "                \"keywords\": [\"renouvelables\", \"solaire\", \"éolien\", \"hydro\"],\n",
        "                \"weight\": 1.2\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"La déforestation est responsable de 15% des émissions mondiales de CO2.\",\n",
        "                \"category\": \"causes\",\n",
        "                \"keywords\": [\"déforestation\", \"forêts\", \"15%\"],\n",
        "                \"weight\": 0.9\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Le transport représente 24% des émissions mondiales de gaz à effet de serre.\",\n",
        "                \"category\": \"secteurs\",\n",
        "                \"keywords\": [\"transport\", \"24%\", \"véhicules\"],\n",
        "                \"weight\": 1.1\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"L'isolation thermique peut réduire la consommation énergétique jusqu'à 50%.\",\n",
        "                \"category\": \"solutions\",\n",
        "                \"keywords\": [\"isolation\", \"thermique\", \"50%\"],\n",
        "                \"weight\": 1.0\n",
        "            }\n",
        "        ]\n",
        "        self._rebuild_kb_index()\n",
        "\n",
        "    def _rebuild_kb_index(self):\n",
        "        \"\"\"Reconstruction optimisée de l'index FAISS\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            texts = [item[\"text\"] for item in self.knowledge_base]\n",
        "            self.kb_embeddings = self.model.encode(\n",
        "                texts,\n",
        "                normalize_embeddings=True,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "\n",
        "            # Index FAISS optimisé\n",
        "            d = self.kb_embeddings.shape[1]\n",
        "            self.kb_index = faiss.IndexFlatIP(d)\n",
        "            self.kb_index.add(self.kb_embeddings.astype(np.float32))\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur reconstruction index: {e}\")\n",
        "\n",
        "    def fit(self, dataset: List[Dict[str, Any]]) -> bool:\n",
        "        \"\"\"Indexation optimisée du corpus d'entraînement\"\"\"\n",
        "        try:\n",
        "            if not dataset:\n",
        "                logging.warning(\"⚠️ Dataset vide, rien à indexer\")\n",
        "                return False\n",
        "\n",
        "            self.corpus_texts = [d[\"text\"] for d in dataset]\n",
        "            self.corpus_labels = [d.get(\"label_id\", 0) for d in dataset]\n",
        "\n",
        "            # Encodage optimisé avec batch processing\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(self.corpus_texts), batch_size):\n",
        "                batch = self.corpus_texts[i:i+batch_size]\n",
        "                batch_embeddings = self.model.encode(\n",
        "                    batch,\n",
        "                    normalize_embeddings=True,\n",
        "                    show_progress_bar=False,\n",
        "                    convert_to_numpy=True\n",
        "                )\n",
        "                embeddings.append(batch_embeddings)\n",
        "\n",
        "            self.corpus_embeddings = np.vstack(embeddings)\n",
        "\n",
        "            # Index FAISS\n",
        "            d = self.corpus_embeddings.shape[1]\n",
        "            self.corpus_index = faiss.IndexFlatIP(d)\n",
        "            self.corpus_index.add(self.corpus_embeddings.astype(np.float32))\n",
        "\n",
        "            # Mise à jour du cache\n",
        "            self._query_cache.clear()\n",
        "\n",
        "            logging.info(f\"✅ {len(dataset)} documents indexés avec succès\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur indexation: {e}\")\n",
        "            return False\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def _get_query_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Cache des embeddings de requêtes fréquentes\"\"\"\n",
        "        return self.model.encode([query], normalize_embeddings=True)[0]\n",
        "\n",
        "    def _search_index(self, query: str, index: faiss.Index, texts: List[str],\n",
        "                     source: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimisée dans un index FAISS\"\"\"\n",
        "        if index is None or not texts:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_embedding = self._get_query_embedding(query)\n",
        "            scores, indices = index.search(\n",
        "                query_embedding.reshape(1, -1).astype(np.float32),\n",
        "                min(top_k, len(texts))\n",
        "            )\n",
        "\n",
        "            results = []\n",
        "            for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
        "                if idx < len(texts) and score > self.similarity_threshold:\n",
        "                    result = {\n",
        "                        \"text\": texts[idx],\n",
        "                        \"score\": float(score),\n",
        "                        \"rank\": rank,\n",
        "                        \"source\": source\n",
        "                    }\n",
        "\n",
        "                    # Ajout des métadonnées si disponible\n",
        "                    if source == \"knowledge_base\" and idx < len(self.knowledge_base):\n",
        "                        result.update({\n",
        "                            \"category\": self.knowledge_base[idx][\"category\"],\n",
        "                            \"keywords\": self.knowledge_base[idx][\"keywords\"]\n",
        "                        })\n",
        "                    elif source == \"training_corpus\" and idx < len(self.corpus_labels):\n",
        "                        result[\"label_id\"] = int(self.corpus_labels[idx])\n",
        "\n",
        "                    results.append(result)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur recherche index: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_knowledge_base(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimisée dans la base de connaissances\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return []\n",
        "        return self._search_index(query, self.kb_index,\n",
        "                                [item[\"text\"] for item in self.knowledge_base],\n",
        "                                \"knowledge_base\", top_k)\n",
        "\n",
        "    def query_corpus(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimisée dans le corpus d'entraînement\"\"\"\n",
        "        if not self.corpus_texts:\n",
        "            return []\n",
        "        return self._search_index(query, self.corpus_index, self.corpus_texts,\n",
        "                                \"training_corpus\", top_k)\n",
        "\n",
        "    def keyword_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par mots-clés optimisée avec scoring avancé\"\"\"\n",
        "        if not query:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
        "            if not query_words:\n",
        "                return []\n",
        "\n",
        "            scored = []\n",
        "\n",
        "            # Recherche dans la base de connaissances\n",
        "            for item in self.knowledge_base:\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', item[\"text\"].lower()))\n",
        "                keyword_words = set(item[\"keywords\"])\n",
        "\n",
        "                # Score combiné TF-IDF like\n",
        "                text_score = len(query_words & text_words) / max(len(text_words), 1)\n",
        "                keyword_score = len(query_words & keyword_words) / max(len(keyword_words), 1)\n",
        "                combined_score = (text_score * 0.7 + keyword_score * 0.3) * item.get(\"weight\", 1.0)\n",
        "\n",
        "                if combined_score > 0.1:\n",
        "                    scored.append({\n",
        "                        \"text\": item[\"text\"],\n",
        "                        \"category\": item[\"category\"],\n",
        "                        \"keywords\": item[\"keywords\"],\n",
        "                        \"score\": combined_score,\n",
        "                        \"source\": \"knowledge_base\"\n",
        "                    })\n",
        "\n",
        "            # Recherche dans le corpus\n",
        "            for i, text in enumerate(self.corpus_texts):\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "                score = len(query_words & text_words) / max(len(text_words), 1)\n",
        "\n",
        "                if score > 0.1:\n",
        "                    scored.append({\n",
        "                        \"text\": text,\n",
        "                        \"label_id\": int(self.corpus_labels[i]),\n",
        "                        \"score\": score,\n",
        "                        \"source\": \"training_corpus\"\n",
        "                    })\n",
        "\n",
        "            scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "            for i, result in enumerate(scored[:top_k], 1):\n",
        "                result[\"rank\"] = i\n",
        "\n",
        "            return scored[:top_k]\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur recherche mots-clés: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_with_fallback(self, question: str, top_k: int = 5,\n",
        "                          search_mode: str = \"hybrid\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche avec fallback optimisé\"\"\"\n",
        "        # Clé de cache\n",
        "        cache_key = f\"{question}_{search_mode}_{top_k}\"\n",
        "        if cache_key in self._query_cache:\n",
        "            return self._query_cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            # Sélection du mode de recherche\n",
        "            if search_mode == \"knowledge_only\":\n",
        "                results = self.query_knowledge_base(question, top_k)\n",
        "            elif search_mode == \"corpus_only\":\n",
        "                results = self.query_corpus(question, top_k)\n",
        "            elif search_mode == \"keywords\":\n",
        "                results = self.keyword_search(question, top_k)\n",
        "            else:  # hybrid\n",
        "                kb_results = self.query_knowledge_base(question, top_k // 2 + 1)\n",
        "                corpus_results = self.query_corpus(question, top_k // 2 + 1)\n",
        "\n",
        "                # Fusion et déduplication\n",
        "                seen_texts = set()\n",
        "                results = []\n",
        "\n",
        "                for item in kb_results + corpus_results:\n",
        "                    if item[\"text\"] not in seen_texts:\n",
        "                        seen_texts.add(item[\"text\"])\n",
        "                        results.append(item)\n",
        "\n",
        "                results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                results = results[:top_k]\n",
        "\n",
        "            # Fallback si nécessaire\n",
        "            if not results or (results and max(r[\"score\"] for r in results) < self.similarity_threshold):\n",
        "                fallback = self.keyword_search(question, top_k)\n",
        "                seen = {r[\"text\"] for r in results}\n",
        "                for item in fallback:\n",
        "                    if item[\"text\"] not in seen:\n",
        "                        results.append(item)\n",
        "                results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                results = results[:top_k]\n",
        "\n",
        "            # Mise en cache\n",
        "            self._query_cache[cache_key] = results\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur recherche avec fallback: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_by_category(self, category: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par catégorie optimisée\"\"\"\n",
        "        try:\n",
        "            results = [\n",
        "                {\n",
        "                    \"text\": item[\"text\"],\n",
        "                    \"category\": item[\"category\"],\n",
        "                    \"keywords\": item[\"keywords\"],\n",
        "                    \"source\": \"knowledge_base\",\n",
        "                    \"rank\": i + 1,\n",
        "                    \"score\": 1.0\n",
        "                }\n",
        "                for i, item in enumerate([\n",
        "                    it for it in self.knowledge_base\n",
        "                    if it[\"category\"].lower() == category.lower()\n",
        "                ][:top_k])\n",
        "            ]\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur recherche catégorie: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, text: str, category: str = \"custom\",\n",
        "                     keywords: List[str] = None) -> bool:\n",
        "        \"\"\"Ajout optimisé de nouvelles connaissances\"\"\"\n",
        "        try:\n",
        "            if not text or not isinstance(text, str):\n",
        "                return False\n",
        "\n",
        "            # Vérification des doublons\n",
        "            if any(item[\"text\"].strip() == text.strip() for item in self.knowledge_base):\n",
        "                return False\n",
        "\n",
        "            new_item = {\n",
        "                \"text\": text.strip(),\n",
        "                \"category\": category,\n",
        "                \"keywords\": keywords or [],\n",
        "                \"weight\": 1.0\n",
        "            }\n",
        "\n",
        "            self.knowledge_base.append(new_item)\n",
        "            self._rebuild_kb_index()\n",
        "            self._query_cache.clear()\n",
        "\n",
        "            logging.info(f\"✅ Nouvelle connaissance ajoutée: {text[:50]}...\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur ajout connaissance: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_categories(self) -> List[str]:\n",
        "        \"\"\"Retourne les catégories disponibles\"\"\"\n",
        "        try:\n",
        "            return list({item[\"category\"] for item in self.knowledge_base})\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Statistiques détaillées et optimisées\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                \"knowledge_base_size\": len(self.knowledge_base),\n",
        "                \"training_corpus_size\": len(self.corpus_texts),\n",
        "                \"total_documents\": len(self.knowledge_base) + len(self.corpus_texts),\n",
        "                \"categories\": self.get_categories(),\n",
        "                \"avg_kb_length\": np.mean([len(item[\"text\"]) for item in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "                \"avg_corpus_length\": np.mean([len(t) for t in self.corpus_texts]) if self.corpus_texts else 0,\n",
        "                \"cache_size\": len(self._query_cache)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur stats: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Nettoyage manuel du cache\"\"\"\n",
        "        self._query_cache.clear()\n",
        "        logging.info(\"🗑️ Cache vidé\")"
      ],
      "metadata": {
        "id": "CdRC_3s7Ncy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f34118e-03a4-45d7-e301-ec0bc620ff90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting qa_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4️⃣ Module Knowledge Base - knowledge_modules.py"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le réchauffement climatique est principalement causé par les émissions de gaz à effet de serre d'origine humaine.\",\n",
        "            \"Les énergies renouvelables comme le solaire et l'éolien sont essentielles pour décarboner notre économie.\",\n",
        "            \"La déforestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports représente environ 24% des émissions mondiales de gaz à effet de serre.\",\n",
        "            \"L'amélioration de l'efficacité énergétique des bâtiments peut réduire jusqu'à 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et régénératrice peut séquestrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les océans absorbent 25% du CO2 atmosphérique mais s'acidifient, menaçant les écosystèmes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises à réduire leurs émissions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'atténuation des émissions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralité carbone.\"\n",
        "        ]\n",
        "        print(\"✅ Base de connaissances initialisée avec recherche par mots-clés\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarité textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarité basé sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score décroissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"✅ Nouvelle connaissance ajoutée: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701ca9bc-448e-4100-e91c-8fe5e3ecbb78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "update_kb_rss.py"
      ],
      "metadata": {
        "id": "duM3V0Q02w8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile update_kb_rss.py\n",
        "import feedparser\n",
        "from qa_modules import OptimizedQAModule\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "import schedule\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Configuration des URLs RSS valides\n",
        "RSS_FEEDS = [\n",
        "    \"https://www.carbonbrief.org/feed/\",\n",
        "    \"https://climate.nasa.gov/news/rss.xml\",\n",
        "    \"https://unfccc.int/news/rss.xml\"\n",
        "]\n",
        "\n",
        "class RSSUpdater:\n",
        "    def __init__(self, qa_module):\n",
        "        self.qa = qa_module\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=400,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\n\", \". \", \"? \", \"! \"]\n",
        "        )\n",
        "\n",
        "    def fetch_and_process_feed(self, feed_url, max_entries=5):\n",
        "        \"\"\"Récupère et traite un flux RSS\"\"\"\n",
        "        try:\n",
        "            feed = feedparser.parse(feed_url)\n",
        "            if feed.bozo:\n",
        "                logging.warning(f\"⚠️ Flux RSS mal formaté: {feed_url}\")\n",
        "                return 0\n",
        "\n",
        "            processed = 0\n",
        "            for entry in feed.entries[:max_entries]:\n",
        "                try:\n",
        "                    # Extraction des informations\n",
        "                    title = entry.title if hasattr(entry, 'title') else \"Sans titre\"\n",
        "                    summary = entry.summary if hasattr(entry, 'summary') else \"\"\n",
        "                    link = entry.link if hasattr(entry, 'link') else \"Lien manquant\"\n",
        "\n",
        "                    # Nettoyage du texte\n",
        "                    content = f\"{title}. {summary}\".strip()\n",
        "                    if len(content) < 50:  # Skip trop courts\n",
        "                        continue\n",
        "\n",
        "                    # Création du document\n",
        "                    doc = Document(\n",
        "                        page_content=content,\n",
        "                        metadata={\n",
        "                            \"url\": link,\n",
        "                            \"title\": title,\n",
        "                            \"source\": \"rss_feed\",\n",
        "                            \"date\": datetime.datetime.now().isoformat()\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                    # Découpage en chunks\n",
        "                    chunks = self.text_splitter.split_documents([doc])\n",
        "\n",
        "                    # Ajout à la base de connaissances\n",
        "                    for chunk in chunks:\n",
        "                        success = self.qa.add_knowledge(\n",
        "                            text=chunk.page_content,\n",
        "                            category=\"rss_news\",\n",
        "                            keywords=[\"rss\", \"news\", \"climate\", \"update\"]\n",
        "                        )\n",
        "                        if success:\n",
        "                            processed += 1\n",
        "                            logging.info(f\"✅ Ajouté: {title[:60]}...\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"❌ Erreur traitement article: {e}\")\n",
        "                    continue\n",
        "\n",
        "            return processed\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur flux RSS {feed_url}: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def update_all_feeds(self):\n",
        "        \"\"\"Mise à jour de tous les flux RSS\"\"\"\n",
        "        total_added = 0\n",
        "        logging.info(f\"🔄 Début mise à jour RSS - {datetime.datetime.now()}\")\n",
        "\n",
        "        for feed_url in RSS_FEEDS:\n",
        "            added = self.fetch_and_process_feed(feed_url)\n",
        "            total_added += added\n",
        "            logging.info(f\"📊 {feed_url}: {added} articles ajoutés\")\n",
        "\n",
        "        logging.info(f\"✅ Mise à jour terminée - Total: {total_added} nouveaux articles\")\n",
        "        return total_added\n",
        "\n",
        "    def start_scheduler(self):\n",
        "        \"\"\"Démarre le planificateur RSS\"\"\"\n",
        "        # Planification quotidienne à 9h\n",
        "        schedule.every().day.at(\"09:00\").do(self.update_all_feeds)\n",
        "\n",
        "        # Test immédiat\n",
        "        self.update_all_feeds()\n",
        "\n",
        "        logging.info(\"📅 Planificateur RSS démarré - mise à jour quotidienne à 09:00\")\n",
        "\n",
        "        # Boucle d'exécution\n",
        "        while True:\n",
        "            schedule.run_pending()\n",
        "            time.sleep(3600)  # Vérification toutes les heures\n",
        "\n",
        "# Fonction utilitaire pour Streamlit\n",
        "def init_rss_updater(qa_module):\n",
        "    \"\"\"Initialise le RSS updater pour Streamlit\"\"\"\n",
        "    updater = RSSUpdater(qa_module)\n",
        "    return updater\n",
        "\n",
        "def manual_rss_update(qa_module):\n",
        "    \"\"\"Mise à jour manuelle via Streamlit\"\"\"\n",
        "    try:\n",
        "        updater = RSSUpdater(qa_module)\n",
        "        return updater.update_all_feeds()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Erreur mise à jour RSS: {e}\")\n",
        "        return 0"
      ],
      "metadata": {
        "id": "L726nrJs2wq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55aee367-6fc1-4bd4-952c-5372fd88de7a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting update_kb_rss.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5️⃣ Module Streamlit - streamlit_app.py"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "# streamlit_app_fusion.py - Version fusionnée avec optimisations et RSS\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from contextlib import contextmanager\n",
        "from qa_modules import OptimizedQAModule  # Module optimisé\n",
        "\n",
        "# Configuration optimisée de Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"🌍 Climate Analyzer Pro\",\n",
        "    page_icon=\"🌍\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Barre de progression persistante\n",
        "# ---------------------------------------------------------\n",
        "@contextmanager\n",
        "def st_progress(title=\"Progress\", max_value=100):\n",
        "    bar = st.progress(0, text=title)\n",
        "    try:\n",
        "        yield bar\n",
        "    finally:\n",
        "        bar.empty()\n",
        "\n",
        "# Cache optimisé pour le module Q&A\n",
        "@st.cache_resource\n",
        "def get_optimized_qa():\n",
        "    \"\"\"Cache du module Q&A optimisé\"\"\"\n",
        "    return OptimizedQAModule()\n",
        "\n",
        "# Persistance session_state - Initialisation correcte\n",
        "if \"trainer\" not in st.session_state:\n",
        "    st.session_state.trainer = None\n",
        "if \"label_names\" not in st.session_state:\n",
        "    st.session_state.label_names = None\n",
        "if \"test_ds\" not in st.session_state:\n",
        "    st.session_state.test_ds = None\n",
        "if \"training\" not in st.session_state:\n",
        "    st.session_state.training = False\n",
        "if \"raw_train_data\" not in st.session_state:\n",
        "    st.session_state.raw_train_data = None\n",
        "if \"qa_module\" not in st.session_state:\n",
        "    st.session_state.qa_module = get_optimized_qa()  # Utilisation du module optimisé\n",
        "\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from visualization_modules import VisualizationManager\n",
        "\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "\n",
        "        # Fusion des modules Q&A\n",
        "        if st.session_state.qa_module is None:\n",
        "            st.session_state.qa_module = get_optimized_qa()\n",
        "\n",
        "        self.qa_module = st.session_state.qa_module\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.load_saved_model()\n",
        "\n",
        "    def load_saved_model(self):\n",
        "        \"\"\"Chargement automatique du modèle si déjà présent\"\"\"\n",
        "        if st.session_state.trainer is None and os.path.exists(\"outputs/final_model/config.json\"):\n",
        "            try:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                num_labels = len(self.data_processor.label_mapping) or 2\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                trainer = self.model_manager.setup_trainer(None, None)\n",
        "                trainer.model = trainer.model.from_pretrained(\"outputs/final_model\")\n",
        "                st.session_state.trainer = trainer\n",
        "                st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "                st.success(\"✅ Modèle chargé depuis le disque.\")\n",
        "            except Exception as e:\n",
        "                st.warning(f\"⚠️ Chargement impossible : {e}\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Menu principal de l'application\"\"\"\n",
        "        st.title(\"🌍 Climate Sentiment Analyzer Pro\")\n",
        "\n",
        "        # Sidebar optimisée avec statistiques Q&A\n",
        "        with st.sidebar:\n",
        "            st.markdown(\"### 📊 Statistiques Système\")\n",
        "\n",
        "            stats = self.qa_module.get_stats()\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                st.metric(\"Base de connaissances\", stats[\"knowledge_base_size\"])\n",
        "            with col2:\n",
        "                st.metric(\"Total documents\", stats[\"total_documents\"])\n",
        "\n",
        "            # Contrôles rapides\n",
        "            if st.button(\"🗑️ Vider le cache Q&A\"):\n",
        "                self.qa_module.clear_cache()\n",
        "                st.success(\"Cache vidé!\")\n",
        "                st.rerun()\n",
        "\n",
        "            # Navigation principale\n",
        "            mode = st.selectbox(\n",
        "                \"Mode\",\n",
        "                [\"🚀 Pipeline Complet\", \"❓ Q&A Avancée\", \"📚 Gestion des Connaissances\",\n",
        "                 \"📰 Mise à jour RSS\", \"📈 Visualisations\"]\n",
        "            )\n",
        "\n",
        "        # Routage vers les différentes sections\n",
        "        if mode == \"🚀 Pipeline Complet\":\n",
        "            self.run_complete_pipeline()\n",
        "        elif mode == \"❓ Q&A Avancée\":\n",
        "            self.run_advanced_qa_interface()\n",
        "        elif mode == \"📚 Gestion des Connaissances\":\n",
        "            self.run_knowledge_management()\n",
        "        elif mode == \"📰 Mise à jour RSS\":\n",
        "            self.run_rss_integration()\n",
        "        elif mode == \"📈 Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Pipeline complet d'entraînement (inchangé mais optimisé)\"\"\"\n",
        "        st.header(\"🚀 Pipeline Complet\")\n",
        "\n",
        "        uploaded_file = st.file_uploader(\"Téléchargez votre CSV\", type=[\"csv\"])\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.dataframe(df.head())\n",
        "\n",
        "            # SLIDERS avec valeurs par défaut optimisées\n",
        "            sample_size = st.slider(\"Taille échantillon\", 1000, 10000, value=4000)\n",
        "            self.config.epochs = st.slider(\"Epochs\", 1, 5, value=3)\n",
        "\n",
        "            is_training = st.session_state.get(\"training\", False)\n",
        "\n",
        "            if st.button(\n",
        "                \"🚀 Lancer l'entraînement\",\n",
        "                type=\"primary\",\n",
        "                disabled=bool(is_training)\n",
        "            ):\n",
        "                st.session_state.training = True\n",
        "                try:\n",
        "                    self.train_pipeline(df, sample_size)\n",
        "                finally:\n",
        "                    st.session_state.training = False\n",
        "\n",
        "    def train_pipeline(self, df: pd.DataFrame, sample_size: int):\n",
        "        \"\"\"Processus d'entraînement optimisé\"\"\"\n",
        "        try:\n",
        "            # 1/4 — Analyse des données avec cache\n",
        "            with st_progress(\"1/4  Analyse des données …\") as bar:\n",
        "                train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "                bar.progress(25)\n",
        "\n",
        "            # Sauvegarder les données brutes\n",
        "            raw_train_data = [{\"text\": item[\"text\"], \"label_id\": item[\"label_id\"]}\n",
        "                            for item in train_ds]\n",
        "            st.session_state.raw_train_data = raw_train_data\n",
        "\n",
        "            # 2/4 — Tokenizer\n",
        "            with st_progress(\"2/4  Chargement du tokenizer …\") as bar:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                bar.progress(50)\n",
        "\n",
        "            # 3/4 — Modèle\n",
        "            with st_progress(\"3/4  Initialisation du modèle …\") as bar:\n",
        "                num_labels = len(self.data_processor.label_mapping)\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                bar.progress(75)\n",
        "\n",
        "            # 4/4 — Tokenisation optimisée\n",
        "            def prep(ds):\n",
        "                with st_progress(\"4/4  Tokenisation …\") as bar:\n",
        "                    ds = ds.map(\n",
        "                        self.model_manager.tokenize_function,\n",
        "                        batched=True,\n",
        "                        desc=\"Tokenisation\"\n",
        "                    )\n",
        "                    ds = ds.rename_column(\"label_id\", \"labels\")\n",
        "                    keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
        "                    for col in list(ds.column_names):\n",
        "                        if col not in keep:\n",
        "                            ds = ds.remove_columns(col)\n",
        "                    ds.set_format(type=\"torch\", columns=list(keep))\n",
        "                    bar.progress(100)\n",
        "                    return ds\n",
        "\n",
        "            train_ds_processed, val_ds_processed, test_ds_processed = map(prep, (train_ds, val_ds, test_ds))\n",
        "\n",
        "            trainer = self.model_manager.setup_trainer(train_ds_processed, val_ds_processed)\n",
        "\n",
        "            with st.spinner(\"Entraînement en cours …\"):\n",
        "                trainer.train()\n",
        "\n",
        "            trainer.save_model(\"outputs/final_model\")\n",
        "            trainer.state.save_to_json(\"outputs/final_model/trainer_state.json\")\n",
        "\n",
        "            # Indexer les données d'entraînement dans le module Q&A optimisé\n",
        "            if st.session_state.raw_train_data:\n",
        "                self.qa_module.fit(st.session_state.raw_train_data)\n",
        "                st.session_state.qa_module = self.qa_module\n",
        "\n",
        "            st.session_state.trainer = trainer\n",
        "            st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "            st.session_state.test_ds = test_ds_processed\n",
        "            st.success(\"🎉 Entraînement terminé !\")\n",
        "            st.balloons()\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur : {e}\")\n",
        "            import traceback\n",
        "            st.error(f\"Détail: {traceback.format_exc()}\")\n",
        "            st.session_state.training = False\n",
        "\n",
        "    def run_advanced_qa_interface(self):\n",
        "        \"\"\"Interface Q&A avancée fusionnée avec optimisations\"\"\"\n",
        "        st.header(\"❓ Interface Q&A Avancée\")\n",
        "\n",
        "        # Configuration de recherche avec colonnes optimisées\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "\n",
        "        with col1:\n",
        "            question = st.text_input(\n",
        "                \"Posez votre question sur le climat :\",\n",
        "                placeholder=\"Ex: Quelles sont les causes du réchauffement climatique ?\"\n",
        "            )\n",
        "\n",
        "        with col2:\n",
        "            search_mode = st.selectbox(\n",
        "                \"Mode\",\n",
        "                [\"hybrid\", \"knowledge_only\", \"corpus_only\", \"keywords\"],\n",
        "                format_func=lambda x: {\n",
        "                    \"hybrid\": \"🔀 Hybride\",\n",
        "                    \"knowledge_only\": \"📚 Base de connaissances\",\n",
        "                    \"corpus_only\": \"📊 Corpus d'entraînement\",\n",
        "                    \"keywords\": \"🔍 Mots-clés\"\n",
        "                }[x]\n",
        "            )\n",
        "\n",
        "        # Paramètres avancés dans l'expandeur\n",
        "        with st.expander(\"⚙️ Paramètres de recherche\", expanded=False):\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                top_k = st.slider(\"Nombre de résultats\", 1, 10, value=5)\n",
        "            with col2:\n",
        "                show_details = st.checkbox(\"Afficher les détails\", True)\n",
        "\n",
        "        # Recherche principale avec spinner optimisé\n",
        "        if question:\n",
        "            try:\n",
        "                with st.spinner(\"🔍 Recherche intelligente en cours...\"):\n",
        "                    results = self.qa_module.query_with_fallback(question, top_k, search_mode)\n",
        "\n",
        "                self.display_qa_results(results, show_details, f\"Résultats pour: '{question}'\")\n",
        "\n",
        "                # Analyse des résultats dans un expander\n",
        "                if results and len(results) > 1:\n",
        "                    with st.expander(\"📊 Analyse des résultats\", expanded=False):\n",
        "                        self.visualizer.plot_qa_results_analysis(results, question)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"❌ Erreur : {str(e)}\")\n",
        "                if st.checkbox(\"Afficher les détails techniques\"):\n",
        "                    st.exception(e)\n",
        "\n",
        "        # Questions suggérées avec boutons\n",
        "        st.markdown(\"### 💡 Questions suggérées\")\n",
        "        suggested_questions = [\n",
        "            \"Quelles sont les principales causes du réchauffement climatique ?\",\n",
        "            \"Comment les énergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la déforestation sur le climat ?\",\n",
        "            \"Comment réduire les émissions de gaz à effet de serre ?\"\n",
        "        ]\n",
        "\n",
        "        cols = st.columns(2)\n",
        "        for i, suggestion in enumerate(suggested_questions[:4]):\n",
        "            if cols[i % 2].button(\n",
        "                suggestion[:50] + \"...\" if len(suggestion) > 50 else suggestion,\n",
        "                key=f\"suggestion_{i}\",\n",
        "                use_container_width=True\n",
        "            ):\n",
        "                st.session_state[\"question\"] = suggestion\n",
        "                st.rerun()\n",
        "\n",
        "    def display_qa_results(self, results: list, show_details: bool, title: str):\n",
        "        \"\"\"Affichage optimisé des résultats Q&A\"\"\"\n",
        "        if not results:\n",
        "            st.info(\"🔍 Aucun résultat trouvé.\")\n",
        "            return\n",
        "\n",
        "        st.markdown(f\"### {title}\")\n",
        "        st.caption(f\"{len(results)} résultat(s) trouvé(s)\")\n",
        "\n",
        "        for result in results:\n",
        "            # Emoji selon la source\n",
        "            emoji = {\n",
        "                \"knowledge_base\": \"📚\",\n",
        "                \"training_corpus\": \"📊\",\n",
        "                \"keywords\": \"🔍\"\n",
        "            }.get(result.get(\"source\"), \"📄\")\n",
        "\n",
        "            # Couleur selon le score\n",
        "            score = result.get(\"score\", 0)\n",
        "            score_color = \"🟢\" if score > 0.7 else \"🟡\" if score > 0.4 else \"🔴\"\n",
        "\n",
        "            with st.expander(\n",
        "                f\"{emoji} Score: {score_color} {score:.3f} - {result.get('source', 'source').replace('_', ' ').title()}\",\n",
        "                expanded=(result.get(\"rank\", 0) == 1)\n",
        "            ):\n",
        "                st.write(\"**Texte:**\")\n",
        "                st.write(result[\"text\"])\n",
        "\n",
        "                if show_details:\n",
        "                    st.divider()\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    with col1:\n",
        "                        if \"category\" in result:\n",
        "                            st.caption(f\"**Catégorie:** `{result['category']}`\")\n",
        "                        if \"keywords\" in result and result[\"keywords\"]:\n",
        "                            st.caption(\"**Mots-clés:** \" + \" \".join([f\"`{kw}`\" for kw in result[\"keywords\"][:3]]))\n",
        "\n",
        "                    with col2:\n",
        "                        st.caption(f\"**Rang:** {result.get('rank', '?')}\")\n",
        "\n",
        "    def run_knowledge_management(self):\n",
        "        \"\"\"Interface de gestion optimisée de la base de connaissances\"\"\"\n",
        "        st.header(\"📚 Gestion des Connaissances\")\n",
        "\n",
        "        tab1, tab2, tab3 = st.tabs([\"📖 Consulter\", \"➕ Ajouter\", \"📊 Statistiques\"])\n",
        "\n",
        "        with tab1:\n",
        "            self._render_knowledge_browser()\n",
        "\n",
        "        with tab2:\n",
        "            self._render_knowledge_adder()\n",
        "\n",
        "        with tab3:\n",
        "            self._render_knowledge_stats()\n",
        "\n",
        "    def _render_knowledge_browser(self):\n",
        "        \"\"\"Sous-composant pour naviguer dans la base de connaissances\"\"\"\n",
        "        st.subheader(\"📖 Consultation\")\n",
        "\n",
        "        # Filtres avec colonnes\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            categories = self.qa_module.get_categories()\n",
        "            filter_category = st.selectbox(\"Filtrer par\", [\"Toutes\"] + categories)\n",
        "        with col2:\n",
        "            search_text = st.text_input(\"Rechercher\", placeholder=\"Mots-clés...\")\n",
        "\n",
        "        # Filtrage et affichage\n",
        "        knowledge_items = []\n",
        "        for idx, item in enumerate(self.qa_module.knowledge_base):\n",
        "            if filter_category == \"Toutes\" or item[\"category\"] == filter_category:\n",
        "                if not search_text or search_text.lower() in item[\"text\"].lower():\n",
        "                    knowledge_items.append((idx, item))\n",
        "\n",
        "        st.info(f\"📄 {len(knowledge_items)} document(s) trouvé(s)\")\n",
        "\n",
        "        # Affichage paginé pour performance\n",
        "        items_per_page = 5\n",
        "        page = st.number_input(\"Page\", min_value=1, max_value=max(1, len(knowledge_items)//items_per_page + 1), value=1)\n",
        "\n",
        "        start_idx = (page - 1) * items_per_page\n",
        "        end_idx = min(start_idx + items_per_page, len(knowledge_items))\n",
        "\n",
        "        for idx, (orig_idx, item) in enumerate(knowledge_items[start_idx:end_idx], start=1):\n",
        "            with st.expander(f\"📄 {item['category'].upper()} - {item['text'][:80]}...\"):\n",
        "                st.write(item[\"text\"])\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    st.caption(f\"ID: `{orig_idx}`\")\n",
        "                with col2:\n",
        "                    if item[\"keywords\"]:\n",
        "                        st.caption(\"Keywords: \" + \", \".join(item[\"keywords\"][:3]))\n",
        "\n",
        "    def _render_knowledge_adder(self):\n",
        "        \"\"\"Sous-composant pour ajouter des connaissances\"\"\"\n",
        "        st.subheader(\"➕ Ajouter une connaissance\")\n",
        "\n",
        "        with st.form(\"add_knowledge_form\"):\n",
        "            new_text = st.text_area(\n",
        "                \"Texte\",\n",
        "                placeholder=\"Entrez le texte de la nouvelle connaissance...\",\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                categories = self.qa_module.get_categories()\n",
        "                category_choice = st.selectbox(\"Catégorie\", [\"Nouvelle\"] + categories)\n",
        "\n",
        "                if category_choice == \"Nouvelle\":\n",
        "                    new_category = st.text_input(\"Nouvelle catégorie\", placeholder=\"technologies\")\n",
        "                    final_category = new_category\n",
        "                else:\n",
        "                    final_category = category_choice\n",
        "\n",
        "            with col2:\n",
        "                keywords = st.text_input(\n",
        "                    \"Mots-clés\",\n",
        "                    placeholder=\"tech, innovation, futur\"\n",
        "                )\n",
        "\n",
        "            submitted = st.form_submit_button(\"✅ Ajouter\", use_container_width=True)\n",
        "\n",
        "            if submitted and new_text and final_category:\n",
        "                keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
        "\n",
        "                if self.qa_module.add_knowledge(new_text, final_category, keyword_list):\n",
        "                    st.success(\"✅ Ajouté avec succès!\")\n",
        "                    st.rerun()\n",
        "                else:\n",
        "                    st.error(\"❌ Erreur lors de l'ajout\")\n",
        "\n",
        "    def _render_knowledge_stats(self):\n",
        "        \"\"\"Sous-composant pour les statistiques\"\"\"\n",
        "        st.subheader(\"📊 Statistiques\")\n",
        "        stats = self.qa_module.get_stats()\n",
        "\n",
        "        # Métriques principales\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        col1.metric(\"Total\", stats[\"total_documents\"])\n",
        "        col2.metric(\"KB\", stats[\"knowledge_base_size\"])\n",
        "        col3.metric(\"Corpus\", stats[\"training_corpus_size\"])\n",
        "        col4.metric(\"Catégories\", len(stats[\"categories\"]))\n",
        "\n",
        "        # Graphiques si des données existent\n",
        "        if self.qa_module.knowledge_base:\n",
        "            # Distribution par catégorie\n",
        "            category_counts = {}\n",
        "            for item in self.qa_module.knowledge_base:\n",
        "                cat = item[\"category\"]\n",
        "                category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "            df_categories = pd.DataFrame([\n",
        "                {\"Catégorie\": cat, \"Nombre\": count}\n",
        "                for cat, count in category_counts.items()\n",
        "            ])\n",
        "\n",
        "            st.bar_chart(df_categories.set_index(\"Catégorie\"))\n",
        "\n",
        "    def run_rss_integration(self):\n",
        "        \"\"\"Section RSS dans l'interface Streamlit\"\"\"\n",
        "        st.header(\"📰 Mise à jour RSS Automatique\")\n",
        "\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "\n",
        "        with col1:\n",
        "            st.info(\"📡 Flux RSS configurés:\")\n",
        "            for url in [\n",
        "                \"Carbon Brief\",\n",
        "                \"NASA Climate\",\n",
        "                \"UNFCCC News\"\n",
        "            ]:\n",
        "                st.write(f\"• {url}\")\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"🔄 Mise à jour manuelle\", type=\"primary\"):\n",
        "                with st.spinner(\"Mise à jour en cours...\"):\n",
        "                    try:\n",
        "                        from update_kb_rss import manual_rss_update\n",
        "                        added = manual_rss_update(self.qa_module)\n",
        "                        if added > 0:\n",
        "                            st.success(f\"✅ {added} articles ajoutés\")\n",
        "                            st.rerun()\n",
        "                        else:\n",
        "                            st.info(\"Aucun nouvel article trouvé\")\n",
        "                    except ImportError:\n",
        "                        st.error(\"❌ Module update_kb_rss non trouvé\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"❌ Erreur lors de la mise à jour : {str(e)}\")\n",
        "\n",
        "        # Paramètres avancés\n",
        "        with st.expander(\"⚙️ Paramètres RSS\"):\n",
        "            st.write(\"Mise à jour automatique activée\")\n",
        "            st.write(\"Fréquence: Quotidienne à 09:00\")\n",
        "            st.write(\"Sources: Carbon Brief, NASA Climate, UNFCCC\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        \"\"\"Interface des visualisations (inchangée mais avec optimisations)\"\"\"\n",
        "        st.header(\"📈 Visualisations\")\n",
        "\n",
        "        viz = st.selectbox(\n",
        "            \"Choisir une visualisation\",\n",
        "            [\"Distribution des classes\", \"Matrice de confusion\", \"Courbes d'entraînement\",\n",
        "             \"📊 Métriques BLEU/ROUGE\", \"🔍 Évaluation Q&A\", \"📚 Analyse Knowledge Base\"]\n",
        "        )\n",
        "\n",
        "        test_ds = st.session_state.get(\"test_ds\")\n",
        "        label_names = st.session_state.get(\"label_names\")\n",
        "\n",
        "        try:\n",
        "            if viz == \"Distribution des classes\" and test_ds:\n",
        "                self.visualizer.plot_class_distribution(test_ds[\"labels\"], label_names)\n",
        "            elif viz == \"Matrice de confusion\" and test_ds and st.session_state.trainer:\n",
        "                self.visualizer.show_confusion_matrix(st.session_state.trainer, test_ds, label_names)\n",
        "            elif viz == \"Courbes d'entraînement\":\n",
        "                self.visualizer.plot_training_curves(\"outputs/final_model\")\n",
        "            elif viz == \"📊 Métriques BLEU/ROUGE\":\n",
        "                self.run_bleu_rouge_analysis()\n",
        "            elif viz == \"🔍 Évaluation Q&A\":\n",
        "                self.run_qa_evaluation()\n",
        "            elif viz == \"📚 Analyse Knowledge Base\":\n",
        "                self.run_knowledge_base_analysis()\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur : {e}\")\n",
        "            if st.checkbox(\"Détails techniques\"):\n",
        "                st.exception(e)\n",
        "\n",
        "    def run_knowledge_base_analysis(self):\n",
        "        \"\"\"Analyse optimisée de la base de connaissances\"\"\"\n",
        "        st.subheader(\"📚 Analyse de la Base de Connaissances\")\n",
        "\n",
        "        if not self.qa_module.knowledge_base:\n",
        "            st.info(\"La base de connaissances est vide.\")\n",
        "            return\n",
        "\n",
        "        # Mots-clés fréquents\n",
        "        all_keywords = [kw for item in self.qa_module.knowledge_base for kw in item[\"keywords\"]]\n",
        "\n",
        "        if all_keywords:\n",
        "            from collections import Counter\n",
        "            top_keywords = Counter(all_keywords).most_common(10)\n",
        "            df_keywords = pd.DataFrame(top_keywords, columns=[\"Mot-clé\", \"Fréquence\"])\n",
        "            st.bar_chart(df_keywords.set_index(\"Mot-clé\"))\n",
        "\n",
        "        # Longueurs de texte\n",
        "        lengths = [len(item[\"text\"]) for item in self.qa_module.knowledge_base]\n",
        "\n",
        "        if lengths:\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Min\", f\"{min(lengths)}\")\n",
        "            col2.metric(\"Moyenne\", f\"{np.mean(lengths):.0f}\")\n",
        "            col3.metric(\"Max\", f\"{max(lengths)}\")\n",
        "\n",
        "    def run_bleu_rouge_analysis(self):\n",
        "        \"\"\"Analyse BLEU/ROUGE (conservée avec optimisations)\"\"\"\n",
        "        st.subheader(\"📊 Analyse des métriques BLEU et ROUGE\")\n",
        "\n",
        "        # Code inchangé mais avec gestion d'erreurs améliorée\n",
        "        try:\n",
        "            # Section test personnalisé\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                reference_text = st.text_area(\n",
        "                    \"Texte de référence:\",\n",
        "                    \"Le réchauffement climatique est un phénomène global causé par les activités humaines.\",\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            with col2:\n",
        "                candidate_text = st.text_area(\n",
        "                    \"Texte candidat:\",\n",
        "                    \"Le changement climatique est un problème mondial dû aux actions humaines.\",\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            if st.button(\"Calculer les scores\", use_container_width=True):\n",
        "                bleu_score = self.visualizer.calculate_bleu_score(reference_text, candidate_text)\n",
        "                rouge_scores = self.visualizer.calculate_rouge_score(reference_text, candidate_text)\n",
        "\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                col1.metric(\"BLEU\", f\"{bleu_score:.4f}\")\n",
        "                col2.metric(\"ROUGE-1\", f\"{rouge_scores['rouge-1']:.4f}\")\n",
        "                col3.metric(\"ROUGE-L\", f\"{rouge_scores['rouge-l']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur dans l'analyse BLEU/ROUGE : {e}\")\n",
        "\n",
        "    def run_qa_evaluation(self):\n",
        "        \"\"\"Évaluation Q&A avec optimisations\"\"\"\n",
        "        st.subheader(\"🔍 Évaluation du système Q&A\")\n",
        "\n",
        "        # Questions de test avec suggestions\n",
        "        default_questions = [\n",
        "            \"Quelles sont les principales causes du réchauffement climatique ?\",\n",
        "            \"Comment les énergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la déforestation sur le climat ?\"\n",
        "        ]\n",
        "\n",
        "        if st.checkbox(\"Utiliser questions par défaut\", value=True):\n",
        "            test_questions = default_questions\n",
        "        else:\n",
        "            test_questions = st.text_area(\n",
        "                \"Entrez vos questions (une par ligne):\",\n",
        "                height=100\n",
        "            ).split(\"\\n\")\n",
        "\n",
        "        if test_questions and st.button(\"🚀 Lancer l'évaluation\", type=\"primary\"):\n",
        "            with st.spinner(\"Évaluation en cours...\"):\n",
        "                # Génération des références\n",
        "                reference_answers = []\n",
        "                for question in test_questions:\n",
        "                    kb_results = self.qa_module.query_knowledge_base(question, top_k=1)\n",
        "                    ref = kb_results[0]['text'] if kb_results else \"Référence manquante\"\n",
        "                    reference_answers.append(ref)\n",
        "\n",
        "                self.visualizer.evaluate_qa_performance(\n",
        "                    self.qa_module,\n",
        "                    test_questions,\n",
        "                    reference_answers\n",
        "                )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = ClimateAnalyzerApp()\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96527f4-daab-40af-da57-79e983601aa1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6️⃣ Script d'Installation - setup_pipeline.py"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\",\n",
        "        \"rouge-score\",\n",
        "        \"nltk\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "            print(f\"✅ {package} installé\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠️ Erreur avec {package}: {e}\")\n",
        "\n",
        "    # Téléchargement des ressources NLTK\n",
        "    import nltk\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"✅ Ressources NLTK prêtes\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465431a7-4073-42f7-edf8-ac9ca8860e48"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492b1426-23d6-462d-bd98-a1250159b930"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "✅ transformers>=4.36.0 installé\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "✅ datasets>=2.16.0 installé\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "✅ torch>=2.1.0 installé\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "✅ peft>=0.7.0 installé\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "✅ sentence-transformers>=2.2.0 installé\n",
            "Requirement already satisfied: faiss-cpu>=1.7.0 in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "✅ faiss-cpu>=1.7.0 installé\n",
            "Requirement already satisfied: streamlit>=1.29.0 in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "✅ streamlit>=1.29.0 installé\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "✅ plotly>=5.17.0 installé\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "✅ scikit-learn>=1.3.0 installé\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "✅ matplotlib>=3.7.0 installé\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "✅ seaborn>=0.12.0 installé\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "✅ pandas>=1.5.0 installé\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "✅ numpy>=1.24.0 installé\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "✅ rouge-score installé\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "✅ nltk installé\n",
            "✅ Ressources NLTK prêtes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "-MEnnZWsOuL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94cd4e5e-f754-4bcd-a0d0-3ed3cf51b6f5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "Y3wskx_fZLEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b25e9c5-38df-4afa-b58c-3fc8461d11b0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K3WFMzkb6p9",
        "outputId": "e12a2e04-772e-4a4b-9d12-3d21f5da7277"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Lancement Streamlit + ngrok (version corrigée)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1️⃣ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2️⃣ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3️⃣ Attendre et créer le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🚀 Interface Streamlit disponible à :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "IFyKJzjISjWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049b9cbf-d40e-4ae7-aee2-d06d3e9c7f41"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "🚀 Interface Streamlit disponible à :\n",
            "NgrokTunnel: \"https://0c1c5f32419e.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}