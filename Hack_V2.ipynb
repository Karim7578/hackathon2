{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SPHSAyjovWTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa11806-15ab-402c-e6a8-4726ddcb84b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install streamlit torch transformers datasets peft scikit-learn plotly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile climate_app.py\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import logging\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration centralis√©e de l'application\"\"\"\n",
        "    MODEL_NAME = \"t5-small\"\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    # Param√®tres optimis√©s pour l'√©chantillonnage\n",
        "    DEFAULT_PARAMS = {\n",
        "        \"sample_sizes\": {\n",
        "            \"test_rapide\": 30000,    # 10K par classe - test en 5 min\n",
        "            \"validation\": 75000,     # 25K par classe - validation en 15 min\n",
        "            \"production\": 150000,    # 50K par classe - mod√®le final en 30 min\n",
        "            \"maximum\": 300000        # 100K par classe - si n√©cessaire\n",
        "        },\n",
        "        \"chunk_size\": 20000,\n",
        "        \"test_size\": 0.2,\n",
        "        \"val_size\": 0.125,\n",
        "        \"max_input_length\": 256,\n",
        "        \"max_target_length\": 16,\n",
        "        \"train_batch_size\": 8,\n",
        "        \"eval_batch_size\": 8,\n",
        "        \"num_epochs\": 3,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"lora_r\": 16,\n",
        "        \"lora_alpha\": 32,\n",
        "        \"lora_dropout\": 0.05,\n",
        "        \"eval_steps\": 50,\n",
        "        \"logging_steps\": 25\n",
        "    }\n",
        "\n",
        "    LABEL_MAPPING = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "    LABEL_NAMES = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "class SmartSampler:\n",
        "    \"\"\"√âchantillonneur intelligent pour gros datasets\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def estimate_dataset_size(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Estime la taille et les caract√©ristiques du dataset\"\"\"\n",
        "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "\n",
        "        # Estimation du nombre de lignes bas√©e sur la taille du fichier\n",
        "        # R√®gle empirique : ~1KB par ligne pour du texte Reddit\n",
        "        estimated_lines = int(file_size_mb * 1000)\n",
        "\n",
        "        return {\n",
        "            \"file_size_mb\": file_size_mb,\n",
        "            \"estimated_lines\": estimated_lines,\n",
        "            \"processing_time_estimate\": self._estimate_processing_time(estimated_lines)\n",
        "        }\n",
        "\n",
        "    def _estimate_processing_time(self, lines: int) -> Dict[str, str]:\n",
        "        \"\"\"Estime les temps de traitement selon la taille\"\"\"\n",
        "        times = {}\n",
        "        for size_name, sample_size in self.config.DEFAULT_PARAMS[\"sample_sizes\"].items():\n",
        "            if sample_size >= lines:\n",
        "                times[size_name] = f\"{int(lines / 5000)} min (dataset complet)\"\n",
        "            else:\n",
        "                times[size_name] = f\"{int(sample_size / 5000)} min\"\n",
        "        return times\n",
        "\n",
        "    def create_stratified_sample(self, df: pd.DataFrame, target_size: int,\n",
        "                                progress_callback=None) -> pd.DataFrame:\n",
        "        \"\"\"Cr√©e un √©chantillon stratifi√© intelligent\"\"\"\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(\"üîç Analyse de la distribution des classes...\")\n",
        "\n",
        "        # Analyse de la distribution\n",
        "        class_counts = df['label'].value_counts().sort_index()\n",
        "        total_samples = len(df)\n",
        "\n",
        "        st.info(f\"üìä Distribution originale: {dict(class_counts)}\")\n",
        "\n",
        "        # Calcul des tailles par classe pour l'√©quilibrage\n",
        "        samples_per_class = target_size // 3  # 3 classes\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(f\"üéØ Objectif: {samples_per_class:,} √©chantillons par classe\")\n",
        "\n",
        "        balanced_samples = []\n",
        "\n",
        "        for class_label in [0, 1, 2]:  # negative, neutral, positive\n",
        "            class_data = df[df['label'] == class_label]\n",
        "            available_samples = len(class_data)\n",
        "\n",
        "            if available_samples == 0:\n",
        "                st.warning(f\"‚ö†Ô∏è Aucun √©chantillon trouv√© pour la classe {self.config.LABEL_NAMES[class_label]}\")\n",
        "                continue\n",
        "\n",
        "            # Prendre le minimum entre ce qui est disponible et ce qui est demand√©\n",
        "            n_samples = min(samples_per_class, available_samples)\n",
        "\n",
        "            if progress_callback:\n",
        "                progress_callback(f\"üìù √âchantillonnage classe {self.config.LABEL_NAMES[class_label]}: {n_samples:,} √©chantillons\")\n",
        "\n",
        "            # √âchantillonnage al√©atoire stratifi√©\n",
        "            if n_samples < available_samples:\n",
        "                sampled_class = class_data.sample(n=n_samples, random_state=42)\n",
        "            else:\n",
        "                sampled_class = class_data\n",
        "\n",
        "            balanced_samples.append(sampled_class)\n",
        "\n",
        "        # Combinaison et m√©lange final\n",
        "        if progress_callback:\n",
        "            progress_callback(\"üîÑ Combinaison des √©chantillons...\")\n",
        "\n",
        "        final_sample = pd.concat(balanced_samples, ignore_index=True)\n",
        "        final_sample = final_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        # Statistiques finales\n",
        "        final_class_counts = final_sample['label'].value_counts().sort_index()\n",
        "        st.success(f\"‚úÖ √âchantillon cr√©√©: {dict(final_class_counts)} (Total: {len(final_sample):,})\")\n",
        "\n",
        "        return final_sample\n",
        "\n",
        "class OptimizedDataProcessor:\n",
        "    \"\"\"Processeur de donn√©es optimis√© pour l'√©chantillonnage\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.sampler = SmartSampler(config)\n",
        "\n",
        "    def load_and_sample_data(self, file_path: str, target_sample_size: int) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Charge et √©chantillonne les donn√©es de mani√®re optimis√©e\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Estimation initiale\n",
        "            file_info = self.sampler.estimate_dataset_size(file_path)\n",
        "            st.info(f\"üìÅ Fichier: {file_info['file_size_mb']:.1f} MB (~{file_info['estimated_lines']:,} lignes estim√©es)\")\n",
        "\n",
        "            # Interface de progression\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "\n",
        "            def update_progress(message):\n",
        "                status_text.text(message)\n",
        "\n",
        "            # Strat√©gie de chargement bas√©e sur la taille\n",
        "            if file_info[\"file_size_mb\"] > 500:  # > 500MB\n",
        "                return self._load_large_file_with_sampling(\n",
        "                    file_path, target_sample_size, progress_bar, update_progress\n",
        "                )\n",
        "            else:\n",
        "                return self._load_and_sample_standard(\n",
        "                    file_path, target_sample_size, progress_bar, update_progress\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors du traitement: {str(e)}\")\n",
        "            logger.error(f\"Erreur traitement donn√©es: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _load_and_sample_standard(self, file_path: str, target_size: int,\n",
        "                                 progress_bar, update_progress) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Charge un fichier standard et l'√©chantillonne\"\"\"\n",
        "\n",
        "        update_progress(\"üìñ Lecture du fichier...\")\n",
        "        progress_bar.progress(0.2)\n",
        "\n",
        "        # Tentative de chargement avec diff√©rents encodages\n",
        "        df = None\n",
        "        for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
        "            try:\n",
        "                df = pd.read_csv(\n",
        "                    file_path,\n",
        "                    encoding=encoding,\n",
        "                    on_bad_lines='skip',\n",
        "                    engine='python',\n",
        "                    quoting=csv.QUOTE_MINIMAL\n",
        "                )\n",
        "                logger.info(f\"‚úÖ Fichier charg√© avec encoding {encoding}\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "\n",
        "        if df is None:\n",
        "            st.error(\"‚ùå Impossible de d√©coder le fichier CSV\")\n",
        "            return None\n",
        "\n",
        "        progress_bar.progress(0.4)\n",
        "        update_progress(\"üßπ Validation et nettoyage...\")\n",
        "\n",
        "        # Validation et nettoyage\n",
        "        cleaned_df = self._validate_and_clean_data(df)\n",
        "        if cleaned_df.empty:\n",
        "            return None\n",
        "\n",
        "        progress_bar.progress(0.6)\n",
        "\n",
        "        # √âchantillonnage intelligent\n",
        "        sampled_df = self.sampler.create_stratified_sample(\n",
        "            cleaned_df, target_size, update_progress\n",
        "        )\n",
        "\n",
        "        progress_bar.progress(1.0)\n",
        "        update_progress(f\"‚úÖ Traitement termin√©: {len(sampled_df):,} √©chantillons\")\n",
        "\n",
        "        return sampled_df\n",
        "\n",
        "    def _load_large_file_with_sampling(self, file_path: str, target_size: int,\n",
        "                                      progress_bar, update_progress) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Charge un gros fichier avec √©chantillonnage par chunks\"\"\"\n",
        "\n",
        "        update_progress(\"üîç Analyse du gros fichier...\")\n",
        "\n",
        "        # Premi√®re passe : estimation et √©chantillonnage des chunks\n",
        "        chunk_size = self.config.DEFAULT_PARAMS[\"chunk_size\"]\n",
        "        sampled_chunks = []\n",
        "        total_processed = 0\n",
        "\n",
        "        # Calcul du ratio d'√©chantillonnage approximatif\n",
        "        file_info = self.sampler.estimate_dataset_size(file_path)\n",
        "        if file_info[\"estimated_lines\"] > target_size:\n",
        "            chunk_sample_ratio = target_size / file_info[\"estimated_lines\"] * 2  # x2 pour avoir de la marge\n",
        "        else:\n",
        "            chunk_sample_ratio = 1.0\n",
        "\n",
        "        try:\n",
        "            # Lecture par chunks avec √©chantillonnage\n",
        "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
        "                try:\n",
        "                    chunk_reader = pd.read_csv(\n",
        "                        file_path,\n",
        "                        encoding=encoding,\n",
        "                        chunksize=chunk_size,\n",
        "                        on_bad_lines='skip',\n",
        "                        engine='python',\n",
        "                        quoting=csv.QUOTE_MINIMAL\n",
        "                    )\n",
        "\n",
        "                    for i, chunk in enumerate(chunk_reader):\n",
        "                        # Validation du premier chunk\n",
        "                        if i == 0:\n",
        "                            if not self._validate_columns(chunk):\n",
        "                                return None\n",
        "\n",
        "                        # Nettoyage du chunk\n",
        "                        cleaned_chunk = self._clean_chunk(chunk)\n",
        "                        if len(cleaned_chunk) == 0:\n",
        "                            continue\n",
        "\n",
        "                        # √âchantillonnage du chunk si n√©cessaire\n",
        "                        if chunk_sample_ratio < 1.0:\n",
        "                            n_samples = max(1, int(len(cleaned_chunk) * chunk_sample_ratio))\n",
        "                            cleaned_chunk = cleaned_chunk.sample(n=n_samples, random_state=42)\n",
        "\n",
        "                        sampled_chunks.append(cleaned_chunk)\n",
        "                        total_processed += len(chunk)\n",
        "\n",
        "                        # Mise √† jour de la progression\n",
        "                        progress = min(0.4 + (i * 0.4 / 100), 0.8)  # 40-80% pour le chargement\n",
        "                        progress_bar.progress(progress)\n",
        "                        update_progress(f\"üìä Chunks trait√©s: {i+1} - Lignes: {total_processed:,}\")\n",
        "\n",
        "                        # Arr√™t si on a assez de donn√©es\n",
        "                        total_samples = sum(len(chunk) for chunk in sampled_chunks)\n",
        "                        if total_samples >= target_size * 3:  # x3 pour avoir de la marge avant l'√©quilibrage\n",
        "                            break\n",
        "\n",
        "                    break  # Succ√®s avec cet encoding\n",
        "\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if not sampled_chunks:\n",
        "                st.error(\"‚ùå Aucune donn√©e valide trouv√©e\")\n",
        "                return None\n",
        "\n",
        "            # Combinaison des chunks\n",
        "            progress_bar.progress(0.85)\n",
        "            update_progress(\"üîÑ Assemblage des donn√©es...\")\n",
        "\n",
        "            combined_df = pd.concat(sampled_chunks, ignore_index=True)\n",
        "\n",
        "            # √âchantillonnage final stratifi√©\n",
        "            progress_bar.progress(0.9)\n",
        "            final_sample = self.sampler.create_stratified_sample(\n",
        "                combined_df, target_size, update_progress\n",
        "            )\n",
        "\n",
        "            progress_bar.progress(1.0)\n",
        "            update_progress(f\"‚úÖ Gros fichier trait√©: {len(final_sample):,} √©chantillons finaux\")\n",
        "\n",
        "            return final_sample\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors du traitement du gros fichier: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _validate_columns(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Valide la pr√©sence des colonnes requises\"\"\"\n",
        "        required_cols = [\"comment_sentiment\", \"post_title\", \"self_text\"]\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            st.error(f\"‚ùå Colonnes manquantes: {missing_cols}\")\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _validate_and_clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Valide et nettoie un DataFrame complet\"\"\"\n",
        "        if not self._validate_columns(df):\n",
        "            return pd.DataFrame()\n",
        "        return self._clean_data(df)\n",
        "\n",
        "    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Nettoie les donn√©es\"\"\"\n",
        "        initial_count = len(df)\n",
        "\n",
        "        # Filtrage des labels valides\n",
        "        valid_labels = set(self.config.LABEL_MAPPING.keys())\n",
        "        df = df[df[\"comment_sentiment\"].isin(valid_labels)]\n",
        "\n",
        "        # Suppression des valeurs manquantes\n",
        "        df = df.dropna(subset=[\"comment_sentiment\", \"post_title\", \"self_text\"])\n",
        "\n",
        "        # Cr√©ation des labels num√©riques\n",
        "        df[\"label\"] = df[\"comment_sentiment\"].map(self.config.LABEL_MAPPING).astype(int)\n",
        "\n",
        "        # Cr√©ation du texte combin√©\n",
        "        df[\"text\"] = (\n",
        "            df[\"post_title\"].fillna(\"\") + \" \" + df[\"self_text\"].fillna(\"\")\n",
        "        ).str.strip()\n",
        "\n",
        "        # Filtrage des textes vides\n",
        "        df = df[df[\"text\"].str.len() > 0]\n",
        "\n",
        "        final_count = len(df)\n",
        "        if initial_count > 0:\n",
        "            logger.info(f\"Donn√©es nettoy√©es: {initial_count} ‚Üí {final_count} √©chantillons ({final_count/initial_count*100:.1f}% conserv√©s)\")\n",
        "\n",
        "        return df[[\"text\", \"label\", \"comment_sentiment\"]]\n",
        "\n",
        "    def _clean_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Nettoie un chunk de donn√©es\"\"\"\n",
        "        return self._clean_data(chunk)\n",
        "\n",
        "    def split_data(self, df: pd.DataFrame, test_size: float, val_size: float) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Divise les donn√©es en train/val/test avec stratification\"\"\"\n",
        "        try:\n",
        "            train_val, test = train_test_split(\n",
        "                df, test_size=test_size, random_state=42, stratify=df[\"label\"]\n",
        "            )\n",
        "\n",
        "            train, val = train_test_split(\n",
        "                train_val, test_size=val_size, random_state=42, stratify=train_val[\"label\"]\n",
        "            )\n",
        "\n",
        "            return {\"train\": train, \"validation\": val, \"test\": test}\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de la division des donn√©es: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "class ModelManager:\n",
        "    \"\"\"Gestionnaire du mod√®le et de l'entra√Ænement optimis√©\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_logs = {\"train_loss\": [], \"val_loss\": [], \"steps\": [], \"epoch\": []}\n",
        "\n",
        "    def initialize_model(self, lora_params: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Initialise le mod√®le avec gestion d'erreurs\"\"\"\n",
        "        try:\n",
        "            with st.spinner(\"ü§ñ Chargement du tokenizer...\"):\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_NAME)\n",
        "                if self.tokenizer.pad_token is None:\n",
        "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            with st.spinner(\"üß† Chargement du mod√®le de base...\"):\n",
        "                base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                    self.config.MODEL_NAME,\n",
        "                    torch_dtype=self.config.TORCH_DTYPE,\n",
        "                    device_map=None\n",
        "                ).to(self.config.DEVICE)\n",
        "\n",
        "            with st.spinner(\"üîß Configuration LoRA...\"):\n",
        "                lora_config = LoraConfig(\n",
        "                    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "                    r=lora_params[\"lora_r\"],\n",
        "                    lora_alpha=lora_params[\"lora_alpha\"],\n",
        "                    target_modules=[\"q\", \"v\"],\n",
        "                    lora_dropout=lora_params[\"lora_dropout\"],\n",
        "                    bias=\"none\"\n",
        "                )\n",
        "\n",
        "                self.model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "            # Affichage des informations du mod√®le\n",
        "            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total_params = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "            st.success(f\"‚úÖ Mod√®le initialis√©!\")\n",
        "            st.info(f\"üìä Param√®tres entra√Ænables: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "\n",
        "            logger.info(\"‚úÖ Mod√®le initialis√© avec succ√®s\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur d'initialisation du mod√®le: {str(e)}\")\n",
        "            logger.error(f\"Erreur initialisation mod√®le: {e}\")\n",
        "            return False\n",
        "\n",
        "    def preprocess_data(self, examples: Dict[str, List], max_input_length: int, max_target_length: int):\n",
        "        \"\"\"Pr√©processe les donn√©es pour l'entra√Ænement\"\"\"\n",
        "        inputs = [f\"classify sentiment: {text}\" for text in examples[\"text\"]]\n",
        "        targets = [self.config.LABEL_NAMES[label] for label in examples[\"label\"]]\n",
        "\n",
        "        model_inputs = self.tokenizer(\n",
        "            inputs,\n",
        "            max_length=max_input_length,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\" if len(inputs) == 1 else None\n",
        "        )\n",
        "\n",
        "        labels = self.tokenizer(\n",
        "            targets,\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\" if len(targets) == 1 else None\n",
        "        )\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    def compute_metrics(self, eval_pred) -> Dict[str, float]:\n",
        "        \"\"\"Calcule les m√©triques d'√©valuation de mani√®re robuste\"\"\"\n",
        "        try:\n",
        "            predictions = eval_pred.predictions[0]\n",
        "            labels = eval_pred.label_ids\n",
        "\n",
        "            # D√©codage des pr√©dictions\n",
        "            decoded_preds = []\n",
        "            for pred in predictions:\n",
        "                if isinstance(pred, np.ndarray):\n",
        "                    pred_ids = np.argmax(pred, axis=-1) if pred.ndim > 1 else pred\n",
        "                else:\n",
        "                    pred_ids = pred\n",
        "\n",
        "                decoded_text = self.tokenizer.decode(pred_ids, skip_special_tokens=True).strip().lower()\n",
        "\n",
        "                # Mapping robuste des pr√©dictions\n",
        "                if \"negative\" in decoded_text:\n",
        "                    decoded_preds.append(0)\n",
        "                elif \"positive\" in decoded_text:\n",
        "                    decoded_preds.append(2)\n",
        "                else:\n",
        "                    decoded_preds.append(1)  # neutral par d√©faut\n",
        "\n",
        "            # D√©codage des labels\n",
        "            decoded_labels = []\n",
        "            for label in labels:\n",
        "                if hasattr(label, '__iter__') and not isinstance(label, str):\n",
        "                    label_text = self.tokenizer.decode(label, skip_special_tokens=True).strip().lower()\n",
        "                    if \"negative\" in label_text:\n",
        "                        decoded_labels.append(0)\n",
        "                    elif \"positive\" in label_text:\n",
        "                        decoded_labels.append(2)\n",
        "                    else:\n",
        "                        decoded_labels.append(1)\n",
        "                else:\n",
        "                    decoded_labels.append(int(label))\n",
        "\n",
        "            return {\n",
        "                \"accuracy\": accuracy_score(decoded_labels, decoded_preds),\n",
        "                \"f1_weighted\": f1_score(decoded_labels, decoded_preds, average=\"weighted\"),\n",
        "                \"f1_macro\": f1_score(decoded_labels, decoded_preds, average=\"macro\")\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erreur calcul m√©triques: {e}\")\n",
        "            return {\"accuracy\": 0.0, \"f1_weighted\": 0.0, \"f1_macro\": 0.0}\n",
        "\n",
        "    def setup_trainer(self, datasets: Dict[str, Dataset], training_params: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Configure le trainer optimis√©\"\"\"\n",
        "        try:\n",
        "            # Callback pour tracker les losses\n",
        "            class LossTrackingCallback(TrainerCallback):\n",
        "                def __init__(self, logs_dict):\n",
        "                    self.logs = logs_dict\n",
        "\n",
        "                def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "                    if logs:\n",
        "                        if \"loss\" in logs:\n",
        "                            self.logs[\"train_loss\"].append(logs[\"loss\"])\n",
        "                            self.logs[\"steps\"].append(state.global_step)\n",
        "                            self.logs[\"epoch\"].append(state.epoch)\n",
        "                        if \"eval_loss\" in logs:\n",
        "                            self.logs[\"val_loss\"].append(logs[\"eval_loss\"])\n",
        "\n",
        "            # Configuration optimis√©e de l'entra√Ænement\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=\"./lora_climate_model\",\n",
        "                per_device_train_batch_size=training_params[\"train_batch_size\"],\n",
        "                per_device_eval_batch_size=training_params[\"eval_batch_size\"],\n",
        "                num_train_epochs=training_params[\"num_epochs\"],\n",
        "                learning_rate=training_params[\"learning_rate\"],\n",
        "                warmup_steps=100,  # Warm-up pour stabiliser l'entra√Ænement\n",
        "                eval_strategy=\"steps\",\n",
        "                eval_steps=training_params[\"eval_steps\"],\n",
        "                logging_steps=training_params[\"logging_steps\"],\n",
        "                save_strategy=\"steps\",\n",
        "                save_steps=training_params[\"eval_steps\"],\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"eval_f1_weighted\",\n",
        "                greater_is_better=True,\n",
        "                report_to=None,\n",
        "                dataloader_pin_memory=False,\n",
        "                remove_unused_columns=True,\n",
        "                push_to_hub=False,\n",
        "                fp16=torch.cuda.is_available(),  # Optimisation m√©moire si GPU\n",
        "            )\n",
        "\n",
        "            self.trainer = Trainer(\n",
        "                model=self.model,\n",
        "                args=training_args,\n",
        "                train_dataset=datasets[\"train\"],\n",
        "                eval_dataset=datasets[\"validation\"],\n",
        "                tokenizer=self.tokenizer,\n",
        "                compute_metrics=self.compute_metrics,\n",
        "                callbacks=[LossTrackingCallback(self.training_logs)]\n",
        "            )\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur configuration trainer: {str(e)}\")\n",
        "            logger.error(f\"Erreur setup trainer: {e}\")\n",
        "            return False\n",
        "\n",
        "def create_streamlit_app():\n",
        "    \"\"\"Interface Streamlit optimis√©e pour l'Option A\"\"\"\n",
        "\n",
        "    st.set_page_config(\n",
        "        page_title=\"üåç Climate Sentiment AI - Option A\",\n",
        "        page_icon=\"üåç\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\"\n",
        "    )\n",
        "\n",
        "    st.title(\"üåç Climate Sentiment AI - Option A : √âchantillonnage Intelligent\")\n",
        "    st.markdown(\"*Optimis√© pour traiter efficacement des datasets de 1,2 Go avec √©chantillonnage stratifi√©*\")\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Initialisation des objets\n",
        "    config = Config()\n",
        "    data_processor = OptimizedDataProcessor(config)\n",
        "    model_manager = ModelManager(config)\n",
        "\n",
        "    # Sidebar optimis√©e pour l'√©chantillonnage\n",
        "    st.sidebar.header(\"‚öôÔ∏è Configuration Option A\")\n",
        "\n",
        "    # S√©lection de la strat√©gie d'√©chantillonnage\n",
        "    st.sidebar.subheader(\"üéØ Strat√©gie d'√©chantillonnage\")\n",
        "\n",
        "    sample_strategies = {\n",
        "        \"üöÄ Test rapide (30K)\": {\n",
        "            \"size\": config.DEFAULT_PARAMS[\"sample_sizes\"][\"test_rapide\"],\n",
        "            \"description\": \"Validation rapide en 5 min\",\n",
        "            \"use_case\": \"Test de faisabilit√©\"\n",
        "        },\n",
        "        \"‚úÖ Validation (75K)\": {\n",
        "            \"size\": config.DEFAULT_PARAMS[\"sample_sizes\"][\"validation\"],\n",
        "            \"description\": \"√âquilibre temps/qualit√© en 15 min\",\n",
        "            \"use_case\": \"D√©veloppement et test\"\n",
        "        },\n",
        "        \"üéØ Production (150K)\": {\n",
        "            \"size\": config.DEFAULT_PARAMS[\"sample_sizes\"][\"production\"],\n",
        "            \"description\": \"Mod√®le final en 30 min\",\n",
        "            \"use_case\": \"Mod√®le de production\"\n",
        "        },\n",
        "        \"üî• Maximum (300K)\": {\n",
        "            \"size\": config.DEFAULT_PARAMS[\"sample_sizes\"][\"maximum\"],\n",
        "            \"description\": \"Performance maximale en 60 min\",\n",
        "            \"use_case\": \"Si qualit√© insuffisante\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    selected_strategy = st.sidebar.selectbox(\n",
        "        \"Choisir la strat√©gie\",\n",
        "        options=list(sample_strategies.keys()),\n",
        "        index=2,  # Production par d√©faut\n",
        "        help=\"Choisissez selon vos contraintes de temps et qualit√©\"\n",
        "    )\n",
        "\n",
        "    strategy_info = sample_strategies[selected_strategy]\n",
        "    target_sample_size = strategy_info[\"size\"]\n",
        "\n",
        "    # Affichage des informations de la strat√©gie\n",
        "    st.sidebar.info(f\"\"\"\n",
        "    **{selected_strategy}**\n",
        "\n",
        "    üìä √âchantillons: {target_sample_size:,}\n",
        "    ‚è±Ô∏è Temps estim√©: {strategy_info['description']}\n",
        "    üéØ Usage: {strategy_info['use_case']}\n",
        "    \"\"\")\n",
        "\n",
        "    # Upload de fichier\n",
        "    st.sidebar.subheader(\"üìÅ Fichier de donn√©es\")\n",
        "    uploaded_file = st.sidebar.file_uploader(\n",
        "        \"Charger fichier CSV\",\n",
        "        type=[\"csv\"],\n",
        "        help=\"Fichier avec colonnes: comment_sentiment, post_title, self_text\"\n",
        "    )\n",
        "\n",
        "    # Option de chemin local pour tr√®s gros fichiers\n",
        "    st.sidebar.markdown(\"**Pour fichiers > 200MB:**\")\n",
        "    local_file_path = st.sidebar.text_input(\n",
        "        \"Chemin fichier local\",\n",
        "        placeholder=\"/path/to/large_file.csv\",\n",
        "        help=\"Contourner la limite Streamlit\"\n",
        "    )\n",
        "    use_local_file = st.sidebar.button(\"üìÇ Utiliser fichier local\")\n",
        "\n",
        "    # Param√®tres d'entra√Ænement\n",
        "    st.sidebar.subheader(\"üèãÔ∏è Param√®tres d'entra√Ænement\")\n",
        "    num_epochs = st.sidebar.slider(\"√âpoques\", 1, 5, config.DEFAULT_PARAMS[\"num_epochs\"])\n",
        "    learning_rate = st.sidebar.select_slider(\n",
        "        \"Learning rate\",\n",
        "        options=[1e-5, 5e-5, 1e-4, 5e-4, 1e-3],\n",
        "        value=config.DEFAULT_PARAMS[\"learning_rate\"],\n",
        "        format_func=lambda x: f\"{x:.0e}\"\n",
        "    )\n",
        "    batch_size = st.sidebar.selectbox(\"Batch size\", [4, 8, 16], index=1)\n",
        "\n",
        "    # Interface principale\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "\n",
        "    with col1:\n",
        "        st.header(\"üìä Tableau de bord\")\n",
        "\n",
        "        # D√©termination de la source de donn√©es\n",
        "        data_source = None\n",
        "        tmp_file_path = None\n",
        "\n",
        "        if uploaded_file is not None:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp_file:\n",
        "                tmp_file.write(uploaded_file.getvalue())\n",
        "                tmp_file_path = tmp_file.name\n",
        "            data_source = tmp_file_path\n",
        "\n",
        "        elif use_local_file and local_file_path.strip():\n",
        "            if os.path.exists(local_file_path.strip()):\n",
        "                data_source = local_file_path.strip()\n",
        "            else:\n",
        "                st.error(f\"‚ùå Fichier non trouv√©: {local_file_path}\")\n",
        "\n",
        "        # Traitement des donn√©es\n",
        "        if data_source:\n",
        "            try:\n",
        "                st.subheader(\"üéØ √âchantillonnage intelligent\")\n",
        "\n",
        "                # Traitement des donn√©es avec √©chantillonnage\n",
        "                df = data_processor.load_and_sample_data(data_source, target_sample_size)\n",
        "\n",
        "                if df is not None:\n",
        "                    # Division des donn√©es\n",
        "                    data_splits = data_processor.split_data(\n",
        "                        df,\n",
        "                        config.DEFAULT_PARAMS[\"test_size\"],\n",
        "                        config.DEFAULT_PARAMS[\"val_size\"]\n",
        "                    )\n",
        "\n",
        "                    if data_splits:\n",
        "                        # Statistiques d√©taill√©es\n",
        "                        st.subheader(\"üìà Statistiques de l'√©chantillon\")\n",
        "\n",
        "                        # M√©triques principales\n",
        "                        metrics_col1, metrics_col2, metrics_col3, metrics_col4 = st.columns(4)\n",
        "\n",
        "                        with metrics_col1:\n",
        "                            st.metric(\"üìä Total\", f\"{len(df):,}\")\n",
        "                        with metrics_col2:\n",
        "                            st.metric(\"üèãÔ∏è Train\", f\"{len(data_splits['train']):,}\")\n",
        "                        with metrics_col3:\n",
        "                            st.metric(\"‚úÖ Validation\", f\"{len(data_splits['validation']):,}\")\n",
        "                        with metrics_col4:\n",
        "                            st.metric(\"üß™ Test\", f\"{len(data_splits['test']):,}\")\n",
        "\n",
        "                        # Visualisations\n",
        "                        viz_col1, viz_col2 = st.columns(2)\n",
        "\n",
        "                        with viz_col1:\n",
        "                            # Distribution des sentiments\n",
        "                            sentiment_counts = df['comment_sentiment'].value_counts()\n",
        "                            fig_pie = px.pie(\n",
        "                                values=sentiment_counts.values,\n",
        "                                names=sentiment_counts.index,\n",
        "                                title=\"Distribution des sentiments\",\n",
        "                                color_discrete_map={\n",
        "                                    'negative': '#ff6b6b',\n",
        "                                    'neutral': '#ffd93d',\n",
        "                                    'positive': '#6bcf7f'\n",
        "                                }\n",
        "                            )\n",
        "                            st.plotly_chart(fig_pie, use_container_width=True)\n",
        "\n",
        "                        with viz_col2:\n",
        "                            # Distribution par split\n",
        "                            split_data = []\n",
        "                            for split_name, split_df in data_splits.items():\n",
        "                                for sentiment in ['negative', 'neutral', 'positive']:\n",
        "                                    count = len(split_df[split_df['comment_sentiment'] == sentiment])\n",
        "                                    split_data.append({\n",
        "                                        'Split': split_name.capitalize(),\n",
        "                                        'Sentiment': sentiment,\n",
        "                                        'Count': count\n",
        "                                    })\n",
        "\n",
        "                            split_df_viz = pd.DataFrame(split_data)\n",
        "                            fig_bar = px.bar(\n",
        "                                split_df_viz,\n",
        "                                x='Split',\n",
        "                                y='Count',\n",
        "                                color='Sentiment',\n",
        "                                title=\"Distribution par split\",\n",
        "                                color_discrete_map={\n",
        "                                    'negative': '#ff6b6b',\n",
        "                                    'neutral': '#ffd93d',\n",
        "                                    'positive': '#6bcf7f'\n",
        "                                }\n",
        "                            )\n",
        "                            st.plotly_chart(fig_bar, use_container_width=True)\n",
        "\n",
        "                        # Aper√ßu des donn√©es\n",
        "                        st.subheader(\"üëÄ Aper√ßu des donn√©es\")\n",
        "                        sample_data = df.sample(n=min(5, len(df)), random_state=42)\n",
        "\n",
        "                        for idx, row in sample_data.iterrows():\n",
        "                            sentiment_color = {\n",
        "                                'negative': 'üî¥',\n",
        "                                'neutral': 'üü°',\n",
        "                                'positive': 'üü¢'\n",
        "                            }\n",
        "\n",
        "                            with st.expander(f\"{sentiment_color[row['comment_sentiment']]} {row['comment_sentiment'].upper()} - √âchantillon {idx}\"):\n",
        "                                st.write(f\"**Texte:** {row['text'][:200]}...\")\n",
        "\n",
        "                        # Stockage dans session state\n",
        "                        st.session_state[\"data_splits\"] = data_splits\n",
        "                        st.session_state[\"data_ready\"] = True\n",
        "                        st.session_state[\"sample_strategy\"] = selected_strategy\n",
        "\n",
        "                        # Informations sur la strat√©gie utilis√©e\n",
        "                        st.success(f\"‚úÖ {selected_strategy} appliqu√©e avec succ√®s!\")\n",
        "\n",
        "            finally:\n",
        "                # Nettoyage du fichier temporaire\n",
        "                if tmp_file_path and os.path.exists(tmp_file_path):\n",
        "                    os.unlink(tmp_file_path)\n",
        "\n",
        "        elif \"data_ready\" not in st.session_state:\n",
        "            st.info(\"üëÜ Veuillez charger un fichier CSV pour commencer l'√©chantillonnage intelligent\")\n",
        "\n",
        "            # Guide d'utilisation\n",
        "            st.markdown(\"\"\"\n",
        "            ### üéØ Guide d'utilisation Option A\n",
        "\n",
        "            **1. Choisissez votre strat√©gie d'√©chantillonnage:**\n",
        "            - üöÄ **Test rapide (30K)**: Pour valider rapidement votre pipeline\n",
        "            - ‚úÖ **Validation (75K)**: Bon √©quilibre pour le d√©veloppement\n",
        "            - üéØ **Production (150K)**: Mod√®le final de qualit√© production\n",
        "            - üî• **Maximum (300K)**: Performance maximale si n√©cessaire\n",
        "\n",
        "            **2. Chargez vos donn√©es:**\n",
        "            - Fichiers < 200MB: Upload direct\n",
        "            - Fichiers > 200MB: Chemin local\n",
        "\n",
        "            **3. L'algorithme va:**\n",
        "            - Analyser votre dataset\n",
        "            - Cr√©er un √©chantillon √©quilibr√© et repr√©sentatif\n",
        "            - Optimiser pour vos contraintes de temps\n",
        "            \"\"\")\n",
        "\n",
        "    with col2:\n",
        "        st.header(\"üöÄ Actions\")\n",
        "\n",
        "        # Informations sur le GPU/CPU\n",
        "        device_info = \"üî• GPU\" if config.DEVICE == \"cuda\" else \"üíª CPU\"\n",
        "        st.info(f\"**Dispositif:** {device_info}\")\n",
        "\n",
        "        if \"data_ready\" in st.session_state:\n",
        "            strategy_used = st.session_state.get(\"sample_strategy\", \"Non d√©finie\")\n",
        "            st.success(f\"**Strat√©gie:** {strategy_used}\")\n",
        "\n",
        "        # Bouton d'initialisation du mod√®le\n",
        "        if st.button(\"ü§ñ Initialiser Mod√®le\", use_container_width=True):\n",
        "            if \"data_ready\" in st.session_state:\n",
        "                lora_params = {\n",
        "                    \"lora_r\": config.DEFAULT_PARAMS[\"lora_r\"],\n",
        "                    \"lora_alpha\": config.DEFAULT_PARAMS[\"lora_alpha\"],\n",
        "                    \"lora_dropout\": config.DEFAULT_PARAMS[\"lora_dropout\"]\n",
        "                }\n",
        "\n",
        "                if model_manager.initialize_model(lora_params):\n",
        "                    st.session_state[\"model_ready\"] = True\n",
        "            else:\n",
        "                st.warning(\"‚ö†Ô∏è Chargez d'abord les donn√©es\")\n",
        "\n",
        "        # Bouton d'entra√Ænement\n",
        "        if st.button(\"üèãÔ∏è Lancer Entra√Ænement\", use_container_width=True):\n",
        "            if \"model_ready\" in st.session_state and \"data_ready\" in st.session_state:\n",
        "\n",
        "                # Estimation du temps d'entra√Ænement\n",
        "                data_splits = st.session_state[\"data_splits\"]\n",
        "                train_size = len(data_splits[\"train\"])\n",
        "                estimated_time = (train_size * num_epochs * batch_size) // 1000  # Estimation approximative\n",
        "\n",
        "                st.info(f\"‚è±Ô∏è Temps estim√©: ~{estimated_time} minutes\")\n",
        "\n",
        "                with st.spinner(\"üèãÔ∏è Entra√Ænement en cours...\"):\n",
        "                    try:\n",
        "                        # Pr√©paration des datasets\n",
        "                        datasets = {}\n",
        "\n",
        "                        for split_name, split_data in data_splits.items():\n",
        "                            dataset = Dataset.from_pandas(split_data)\n",
        "                            dataset = dataset.map(\n",
        "                                lambda x: model_manager.preprocess_data(\n",
        "                                    x,\n",
        "                                    config.DEFAULT_PARAMS[\"max_input_length\"],\n",
        "                                    config.DEFAULT_PARAMS[\"max_target_length\"]\n",
        "                                ),\n",
        "                                batched=True,\n",
        "                                remove_columns=split_data.columns.tolist()\n",
        "                            )\n",
        "                            dataset.set_format(\"torch\")\n",
        "                            datasets[split_name] = dataset\n",
        "\n",
        "                        # Configuration du trainer\n",
        "                        training_params = {\n",
        "                            \"train_batch_size\": batch_size,\n",
        "                            \"eval_batch_size\": batch_size,\n",
        "                            \"num_epochs\": num_epochs,\n",
        "                            \"learning_rate\": learning_rate,\n",
        "                            \"eval_steps\": config.DEFAULT_PARAMS[\"eval_steps\"],\n",
        "                            \"logging_steps\": config.DEFAULT_PARAMS[\"logging_steps\"]\n",
        "                        }\n",
        "\n",
        "                        if model_manager.setup_trainer(datasets, training_params):\n",
        "                            # Lancement de l'entra√Ænement\n",
        "                            start_time = time.time()\n",
        "                            model_manager.trainer.train()\n",
        "                            training_time = time.time() - start_time\n",
        "\n",
        "                            st.session_state[\"training_complete\"] = True\n",
        "                            st.session_state[\"training_time\"] = training_time\n",
        "\n",
        "                            st.success(f\"‚úÖ Entra√Ænement termin√© en {training_time/60:.1f} minutes!\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"‚ùå Erreur pendant l'entra√Ænement: {str(e)}\")\n",
        "                        logger.error(f\"Erreur entra√Ænement: {e}\")\n",
        "            else:\n",
        "                st.warning(\"‚ö†Ô∏è Initialisez d'abord le mod√®le\")\n",
        "\n",
        "        # Bouton d'√©valuation\n",
        "        if st.button(\"üìä √âvaluer sur Test\", use_container_width=True):\n",
        "            if \"training_complete\" in st.session_state:\n",
        "                with st.spinner(\"üìä √âvaluation en cours...\"):\n",
        "                    try:\n",
        "                        data_splits = st.session_state[\"data_splits\"]\n",
        "                        test_dataset = Dataset.from_pandas(data_splits[\"test\"])\n",
        "                        test_dataset = test_dataset.map(\n",
        "                            lambda x: model_manager.preprocess_data(\n",
        "                                x,\n",
        "                                config.DEFAULT_PARAMS[\"max_input_length\"],\n",
        "                                config.DEFAULT_PARAMS[\"max_target_length\"]\n",
        "                            ),\n",
        "                            batched=True,\n",
        "                            remove_columns=data_splits[\"test\"].columns.tolist()\n",
        "                        )\n",
        "                        test_dataset.set_format(\"torch\")\n",
        "\n",
        "                        results = model_manager.trainer.evaluate(test_dataset)\n",
        "                        st.session_state[\"test_results\"] = results\n",
        "\n",
        "                        # Affichage des r√©sultats avec contexte\n",
        "                        st.subheader(\"üéØ R√©sultats finaux\")\n",
        "\n",
        "                        col1, col2, col3 = st.columns(3)\n",
        "                        with col1:\n",
        "                            acc = results.get('eval_accuracy', 0)\n",
        "                            st.metric(\"üéØ Accuracy\", f\"{acc:.3f}\", delta=f\"{(acc-0.33)*100:+.1f}%\" if acc > 0.33 else None)\n",
        "                        with col2:\n",
        "                            f1w = results.get('eval_f1_weighted', 0)\n",
        "                            st.metric(\"üìä F1 Weighted\", f\"{f1w:.3f}\")\n",
        "                        with col3:\n",
        "                            f1m = results.get('eval_f1_macro', 0)\n",
        "                            st.metric(\"üìà F1 Macro\", f\"{f1m:.3f}\")\n",
        "\n",
        "                        # Interpr√©tation des r√©sultats\n",
        "                        if acc > 0.75:\n",
        "                            st.success(\"üéâ Excellents r√©sultats! Mod√®le pr√™t pour la production.\")\n",
        "                        elif acc > 0.65:\n",
        "                            st.info(\"‚úÖ Bons r√©sultats. Consid√©rez la strat√©gie 'Maximum' pour am√©liorer.\")\n",
        "                        else:\n",
        "                            st.warning(\"‚ö†Ô∏è R√©sultats moyens. Essayez avec plus de donn√©es ou ajustez les param√®tres.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"‚ùå Erreur pendant l'√©valuation: {str(e)}\")\n",
        "                        logger.error(f\"Erreur √©valuation: {e}\")\n",
        "            else:\n",
        "                st.warning(\"‚ö†Ô∏è Terminez d'abord l'entra√Ænement\")\n",
        "\n",
        "        # Informations de performance\n",
        "        if \"training_complete\" in st.session_state:\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"‚ö° Performance\")\n",
        "\n",
        "            training_time = st.session_state.get(\"training_time\", 0)\n",
        "            strategy_used = st.session_state.get(\"sample_strategy\", \"Non d√©finie\")\n",
        "\n",
        "            st.metric(\"‚è±Ô∏è Temps d'entra√Ænement\", f\"{training_time/60:.1f} min\")\n",
        "            st.info(f\"**Strat√©gie utilis√©e:** {strategy_used}\")\n",
        "\n",
        "    # Onglets pour les visualisations avanc√©es\n",
        "    if \"training_complete\" in st.session_state:\n",
        "        st.markdown(\"---\")\n",
        "        tab1, tab2, tab3 = st.tabs([\"üìà Courbes d'apprentissage\", \"üîç Test interactif\", \"üìã Rapport d√©taill√©\"])\n",
        "\n",
        "        with tab1:\n",
        "            if model_manager.training_logs[\"train_loss\"]:\n",
        "                # Graphique des losses\n",
        "                fig = go.Figure()\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=model_manager.training_logs[\"steps\"],\n",
        "                    y=model_manager.training_logs[\"train_loss\"],\n",
        "                    mode='lines+markers',\n",
        "                    name='Train Loss',\n",
        "                    line=dict(color='blue', width=2)\n",
        "                ))\n",
        "\n",
        "                if model_manager.training_logs[\"val_loss\"]:\n",
        "                    val_steps = model_manager.training_logs[\"steps\"][:len(model_manager.training_logs[\"val_loss\"])]\n",
        "                    fig.add_trace(go.Scatter(\n",
        "                        x=val_steps,\n",
        "                        y=model_manager.training_logs[\"val_loss\"],\n",
        "                        mode='lines+markers',\n",
        "                        name='Validation Loss',\n",
        "                        line=dict(color='red', width=2)\n",
        "                    ))\n",
        "\n",
        "                fig.update_layout(\n",
        "                    title=\"√âvolution des losses pendant l'entra√Ænement\",\n",
        "                    xaxis_title=\"Steps\",\n",
        "                    yaxis_title=\"Loss\",\n",
        "                    hovermode='x unified',\n",
        "                    template=\"plotly_white\"\n",
        "                )\n",
        "\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "                # Analyse de la convergence\n",
        "                if len(model_manager.training_logs[\"train_loss\"]) > 5:\n",
        "                    last_losses = model_manager.training_logs[\"train_loss\"][-5:]\n",
        "                    loss_trend = (last_losses[-1] - last_losses[0]) / last_losses[0] * 100\n",
        "\n",
        "                    if loss_trend < -1:\n",
        "                        st.success(f\"üìà Mod√®le en cours d'am√©lioration (-{abs(loss_trend):.1f}% sur les derniers steps)\")\n",
        "                    elif loss_trend > 1:\n",
        "                        st.warning(f\"üìâ Loss en augmentation (+{loss_trend:.1f}% - possible surentra√Ænement)\")\n",
        "                    else:\n",
        "                        st.info(\"üìä Loss stabilis√©e - Convergence atteinte\")\n",
        "            else:\n",
        "                st.info(\"Aucune donn√©e d'entra√Ænement disponible\")\n",
        "\n",
        "        with tab2:\n",
        "            st.subheader(\"üîç Test de pr√©diction interactif\")\n",
        "\n",
        "            # Exemples pr√©d√©finis\n",
        "            example_texts = {\n",
        "                \"N√©gatif\": \"Climate change is destroying our planet and governments are doing nothing about it!\",\n",
        "                \"Neutre\": \"Scientists published a new study about climate change impacts on weather patterns.\",\n",
        "                \"Positif\": \"Great progress on renewable energy! Solar panels are becoming more efficient and affordable.\"\n",
        "            }\n",
        "\n",
        "            col1, col2 = st.columns([2, 1])\n",
        "\n",
        "            with col1:\n",
        "                test_text = st.text_area(\n",
        "                    \"Entrez un texte √† classifier:\",\n",
        "                    value=example_texts[\"Neutre\"],\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            with col2:\n",
        "                st.write(\"**Exemples:**\")\n",
        "                for label, text in example_texts.items():\n",
        "                    if st.button(f\"üìù {label}\", key=f\"example_{label}\"):\n",
        "                        st.session_state[\"test_text\"] = text\n",
        "                        st.experimental_rerun()\n",
        "\n",
        "            if st.session_state.get(\"test_text\"):\n",
        "                test_text = st.session_state[\"test_text\"]\n",
        "\n",
        "            if st.button(\"üéØ Pr√©dire\", use_container_width=True) and test_text.strip():\n",
        "                try:\n",
        "                    # Pr√©paration de l'input\n",
        "                    input_text = f\"classify sentiment: {test_text}\"\n",
        "                    inputs = model_manager.tokenizer(\n",
        "                        input_text,\n",
        "                        return_tensors=\"pt\",\n",
        "                        max_length=config.DEFAULT_PARAMS[\"max_input_length\"],\n",
        "                        truncation=True,\n",
        "                        padding=True\n",
        "                    ).to(config.DEVICE)\n",
        "\n",
        "                    # Pr√©diction avec probabilit√©s\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model_manager.model.generate(\n",
        "                            **inputs,\n",
        "                            max_length=config.DEFAULT_PARAMS[\"max_target_length\"],\n",
        "                            num_beams=3,\n",
        "                            early_stopping=True,\n",
        "                            return_dict_in_generate=True,\n",
        "                            output_scores=True\n",
        "                        )\n",
        "\n",
        "                    predicted_text = model_manager.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "                    # Affichage du r√©sultat avec style\n",
        "                    sentiment_styles = {\n",
        "                        \"negative\": {\"color\": \"#ff6b6b\", \"icon\": \"üî¥\", \"bg\": \"#ffe6e6\"},\n",
        "                        \"neutral\": {\"color\": \"#ffd93d\", \"icon\": \"üü°\", \"bg\": \"#fffacd\"},\n",
        "                        \"positive\": {\"color\": \"#6bcf7f\", \"icon\": \"üü¢\", \"bg\": \"#e6ffe6\"}\n",
        "                    }\n",
        "\n",
        "                    predicted_sentiment = predicted_text.lower()\n",
        "                    if predicted_sentiment in sentiment_styles:\n",
        "                        style = sentiment_styles[predicted_sentiment]\n",
        "\n",
        "                        st.markdown(f\"\"\"\n",
        "                        <div style=\"padding: 20px; border-radius: 10px; background-color: {style['bg']}; border-left: 5px solid {style['color']};\">\n",
        "                            <h3 style=\"color: {style['color']}; margin: 0;\">\n",
        "                                {style['icon']} Pr√©diction: {predicted_sentiment.upper()}\n",
        "                            </h3>\n",
        "                            <p style=\"margin: 10px 0 0 0; font-style: italic;\">\"{test_text}\"</p>\n",
        "                        </div>\n",
        "                        \"\"\", unsafe_allow_html=True)\n",
        "                    else:\n",
        "                        st.success(f\"**Pr√©diction:** {predicted_text.upper()}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"‚ùå Erreur de pr√©diction: {str(e)}\")\n",
        "\n",
        "        with tab3:\n",
        "            st.subheader(\"üìã Rapport d√©taill√© de l'entra√Ænement\")\n",
        "\n",
        "            # R√©sum√© de la configuration\n",
        "            config_summary = {\n",
        "                \"Strat√©gie d'√©chantillonnage\": st.session_state.get(\"sample_strategy\", \"Non d√©finie\"),\n",
        "                \"Taille de l'√©chantillon\": f\"{len(st.session_state.get('data_splits', {}).get('train', [])):,} (train)\",\n",
        "                \"√âpoques\": num_epochs,\n",
        "                \"Learning rate\": f\"{learning_rate:.0e}\",\n",
        "                \"Batch size\": batch_size,\n",
        "                \"Dispositif\": config.DEVICE.upper(),\n",
        "                \"Temps d'entra√Ænement\": f\"{st.session_state.get('training_time', 0)/60:.1f} min\"\n",
        "            }\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "\n",
        "            with col1:\n",
        "                st.markdown(\"**Configuration:**\")\n",
        "                for key, value in config_summary.items():\n",
        "                    st.write(f\"‚Ä¢ **{key}:** {value}\")\n",
        "\n",
        "            with col2:\n",
        "                if \"test_results\" in st.session_state:\n",
        "                    results = st.session_state[\"test_results\"]\n",
        "                    st.markdown(\"**M√©triques finales:**\")\n",
        "                    st.write(f\"‚Ä¢ **Accuracy:** {results.get('eval_accuracy', 0):.3f}\")\n",
        "                    st.write(f\"‚Ä¢ **F1 Weighted:** {results.get('eval_f1_weighted', 0):.3f}\")\n",
        "                    st.write(f\"‚Ä¢ **F1 Macro:** {results.get('eval_f1_macro', 0):.3f}\")\n",
        "                    st.write(f\"‚Ä¢ **Loss finale:** {results.get('eval_loss', 0):.3f}\")\n",
        "\n",
        "            # Recommandations\n",
        "            st.markdown(\"---\")\n",
        "            st.markdown(\"**üéØ Recommandations pour am√©liorer les performances:**\")\n",
        "\n",
        "            if \"test_results\" in st.session_state:\n",
        "                acc = st.session_state[\"test_results\"].get('eval_accuracy', 0)\n",
        "\n",
        "                if acc < 0.65:\n",
        "                    st.markdown(\"\"\"\n",
        "                    - üìà **Augmenter la taille de l'√©chantillon** (strat√©gie Maximum)\n",
        "                    - üîß **Ajuster le learning rate** (essayer 1e-4 ou 1e-3)\n",
        "                    - üìö **Augmenter le nombre d'√©poques** (5-7 √©poques)\n",
        "                    - üéØ **V√©rifier la qualit√© des donn√©es** (textes trop courts, labels incorrects)\n",
        "                    \"\"\")\n",
        "                elif acc < 0.75:\n",
        "                    st.markdown(\"\"\"\n",
        "                    - ‚úÖ **Bon mod√®le!** Consid√©rez la strat√©gie Maximum pour gagner quelques points\n",
        "                    - üîß **Fine-tuning des hyperparam√®tres** (LoRA rank, alpha)\n",
        "                    - üìä **Analyse des erreurs** sur les pr√©dictions incorrectes\n",
        "                    \"\"\")\n",
        "                else:\n",
        "                    st.markdown(\"\"\"\n",
        "                    - üéâ **Excellent mod√®le!** Pr√™t pour la production\n",
        "                    - üíæ **Sauvegarder le mod√®le** pour utilisation future\n",
        "                    - üöÄ **D√©ploiement recommand√©**\n",
        "                    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_streamlit_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCzbe3RqvYEL",
        "outputId": "974023b7-e50f-4cd5-d78c-66ddfda4b3cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting climate_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('streamlit run climate_app.py --server.port=8501 --server.address=0.0.0.0 &')"
      ],
      "metadata": {
        "id": "00xnLGe2wq0y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmXGNSjvBxiD",
        "outputId": "f1418997-7f56-4d77-82a0-4b136b385d1f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, subprocess, socket\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1Ô∏è‚É£ Nettoie tout\n",
        "subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], stderr=subprocess.DEVNULL)\n",
        "subprocess.run([\"pkill\", \"-9\", \"-f\", \"ngrok\"],   stderr=subprocess.DEVNULL)\n",
        "ngrok.kill()\n",
        "time.sleep(2)\n",
        "\n",
        "# 2Ô∏è‚É£ D√©marre Streamlit en arri√®re-plan\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"climate_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Attend que le port 8501 soit vraiment √©cout√©\n",
        "def wait_for_port(port=8501, timeout=10):\n",
        "    for _ in range(timeout):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            if s.connect_ex((\"localhost\", port)) == 0:\n",
        "                return True\n",
        "        time.sleep(1)\n",
        "    return False\n",
        "\n",
        "if wait_for_port():\n",
        "    # 4Ô∏è‚É£ Reconnecte ngrok\n",
        "    ngrok.set_auth_token(\"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\")\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"üîó Acc√®s public :\", public_url)\n",
        "else:\n",
        "    print(\"‚ùå Streamlit n‚Äôa pas d√©marr√© sur le port 8501\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD_R7lqVwrtc",
        "outputId": "a78107ea-8a61-4a10-bf71-3cf3a4f97d8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Acc√®s public : NgrokTunnel: \"https://8c4f2b343f54.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}