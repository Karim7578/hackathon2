{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SPHSAyjovWTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa11806-15ab-402c-e6a8-4726ddcb84b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install streamlit torch transformers datasets peft scikit-learn plotly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile climate_app.py\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import logging\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration centralisée de l'application\"\"\"\n",
        "    MODEL_NAME = \"t5-small\"\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    # Paramètres optimisés pour l'échantillonnage\n",
        "    DEFAULT_PARAMS = {\n",
        "        \"sample_sizes\": {\n",
        "            \"test_rapide\": 30000,    # 10K par classe - test en 5 min\n",
        "            \"validation\": 75000,     # 25K par classe - validation en 15 min\n",
        "            \"production\": 150000,    # 50K par classe - modèle final en 30 min\n",
        "            \"maximum\": 300000        # 100K par classe - si nécessaire\n",
        "        },\n",
        "        \"chunk_size\": 20000,\n",
        "        \"test_size\": 0.2,\n",
        "        \"val_size\": 0.125,\n",
        "        \"max_input_length\": 256,\n",
        "        \"max_target_length\": 16,\n",
        "        \"train_batch_size\": 8,\n",
        "        \"eval_batch_size\": 8,\n",
        "        \"num_epochs\": 3,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"lora_r\": 16,\n",
        "        \"lora_alpha\": 32,\n",
        "        \"lora_dropout\": 0.05,\n",
        "        \"eval_steps\": 50,\n",
        "        \"logging_steps\": 25\n",
        "    }\n",
        "\n",
        "    LABEL_MAPPING = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "    LABEL_NAMES = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "class SmartSampler:\n",
        "    \"\"\"Échantillonneur intelligent pour gros datasets\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def estimate_dataset_size(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Estime la taille et les caractéristiques du dataset\"\"\"\n",
        "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "\n",
        "        # Estimation du nombre de lignes basée sur la taille du fichier\n",
        "        # Règle empirique : ~1KB par ligne pour du texte Reddit\n",
        "        estimated_lines = int(file_size_mb * 1000)\n",
        "\n",
        "        return {\n",
        "            \"file_size_mb\": file_size_mb,\n",
        "            \"estimated_lines\": estimated_lines,\n",
        "            \"processing_time_estimate\": self._estimate_processing_time(estimated_lines)\n",
        "        }\n",
        "\n",
        "    def _estimate_processing_time(self, lines: int) -> Dict[str, str]:\n",
        "        \"\"\"Estime les temps de traitement selon la taille\"\"\"\n",
        "        times = {}\n",
        "        for size_name, sample_size in self.config.DEFAULT_PARAMS[\"sample_sizes\"].items():\n",
        "            if sample_size >= lines:\n",
        "                times[size_name] = f\"{int(lines / 5000)} min (dataset complet)\"\n",
        "            else:\n",
        "                times[size_name] = f\"{int(sample_size / 5000)} min\"\n",
        "        return times\n",
        "\n",
        "    def create_stratified_sample(self, df: pd.DataFrame, target_size: int,\n",
        "                                progress_callback=None) -> pd.DataFrame:\n",
        "        \"\"\"Crée un échantillon stratifié intelligent\"\"\"\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(\"🔍 Analyse de la distribution des classes...\")\n",
        "\n",
        "        # Analyse de la distribution\n",
        "        class_counts = df['label'].value_counts().sort_index()\n",
        "        total_samples = len(df)\n",
        "\n",
        "        st.info(f\"📊 Distribution originale: {dict(class_counts)}\")\n",
        "\n",
        "        # Calcul des tailles par classe pour l'équilibrage\n",
        "        samples_per_class = target_size // 3  # 3 classes\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(f\"🎯 Objectif: {samples_per_class:,} échantillons par classe\")\n",
        "\n",
        "        balanced_samples = []\n",
        "\n",
        "        for class_label in [0, 1, 2]:  # negative, neutral, positive\n",
        "            class_data = df[df['label'] == class_label]\n",
        "            available_samples = len(class_data)\n",
        "\n",
        "            if available_samples == 0:\n",
        "                st.warning(f\"⚠️ Aucun échantillon trouvé pour la classe {self.config.LABEL_NAMES[class_label]}\")\n",
        "                continue\n",
        "\n",
        "            # Prendre le minimum entre ce qui est disponible et ce qui est demandé\n",
        "            n_samples = min(samples_per_class, available_samples)\n",
        "\n",
        "            if progress_callback:\n",
        "                progress_callback(f\"📝 Échantillonnage classe {self.config.LABEL_NAMES[class_label]}: {n_samples:,} échantillons\")\n",
        "\n",
        "            # Échantillonnage aléatoire stratifié\n",
        "            if n_samples < available_samples:\n",
        "                sampled_class = class_data.sample(n=n_samples, random_state=42)\n",
        "            else:\n",
        "                sampled_class = class_data\n",
        "\n",
        "            balanced_samples.append(sampled_class)\n",
        "\n",
        "        # Combinaison et mélange final\n",
        "        if progress_callback:\n",
        "            progress_callback(\"🔄 Combinaison des échantillons...\")\n",
        "\n",
        "        final_sample = pd.concat(balanced_samples, ignore_index=True)\n",
        "        final_sample = final_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        # Statistiques finales\n",
        "        final_class_counts = final_sample['label'].value_counts().sort_index()\n",
        "        st.success(f\"✅ Échantillon créé: {dict(final_class_counts)} (Total: {len(final_sample):,})\")\n",
        "\n",
        "        return final_sample\n",
        "\n",
        "class OptimizedDataProcessor:\n",
        "    \"\"\"Processeur de données optimisé pour l'échantillonnage\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.sampler = SmartSampler(config)\n",
        "\n",
        "    def load_and_sample_data(self, file_path: str, target_sample_size: int) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Charge et échantillonne les données de manière optimisée\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Estimation initiale\n",
        "            file_info = self.sampler.estimate_dataset_size(file_path)\n",
        "            st.info(f\"📁 Fichier: {file_info['file_size_mb']:.1f} MB (~{file_info['estimated_lines']:,} lignes estimées)\")\n",
        "\n",
        "            # Interface de progression\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "\n",
        "            def update_progress(message):\n",
        "                status_text.text(message)\n",
        "\n",
        "            # Stratégie de chargement basée sur la taille\n",
        "            if file_info[\"file_size_mb\"] > 500:  # > 500MB\n",
        "                return self._load_large_file_with_sampling(\n",
        "                    file_path, target_sample_size, progress_bar, update_progress\n",
        "                )\n",
        "            else:\n",
        "                return self._load_and_sample_standard(\n",
        "                    file_path, target_sample_size, progress_bar, update_progress\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors du traitement: {str(e)}\")\n",
        "            logger.error(f\"Erreur traitement données: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _load_and_sample_standard(self, file_path: str, target_size: int,\n",
        "                                 progress_bar, update_progress) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Charge un fichier standard et l'échantillonne\"\"\"\n",
        "\n",
        "        update_progress(\"📖 Lecture du fichier...\")\n",
        "        progress_bar.progress(0.2)\n",
        "\n",
        "        # Tentative de chargement avec différents encodages\n",
        "        df = None\n",
        "        for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
        "            try:\n",
        "                df = pd.read_csv(\n",
        "                    file_path,\n",
        "                    encoding=encoding,\n",
        "                    on_bad_lines='skip',\n",
        "                    engine='python',\n",
        "                    quoting=csv.QUOTE_MINIMAL\n",
        "                )\n",
        "                logger.info(f\"✅ Fichier chargé avec encoding {encoding}\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "\n",
        "        if df is None:\n",
        "            st.error(\"❌ Impossible de décoder le fichier CSV\")\n",
        "            return None\n",
        "\n",
        "        progress_bar.progress(0.4)\n",
        "        update_progress(\"🧹 Validation et nettoyage...\")\n",
        "\n",
        "        # Validation et nettoyage\n",
        "        cleaned_df = self._validate_and_clean_data(df)\n",
        "        if cleaned_df.empty:\n",
        "            return None\n",
        "\n",
        "        progress_bar.progress(0.6)\n",
        "\n",
        "        # Échantillonnage intelligent\n",
        "        sampled_df = self.sampler.create_stratified_sample(\n",
        "            cleaned_df, target_size, update_progress\n",
        "        )\n",
        "\n",
        "        progress_bar.progress(1.0)\n",
        "        update_progress(f\"✅ Traitement terminé: {len(sampled_df):,} échantillons\")\n",
        "\n",
        "        return sampled_df\n",
        "\n",
        "    def _load_large_file_with_sampling(self, file_path: str, target_size: int,\n",
        "                                      progress_bar, update_progress) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Charge un gros fichier avec échantillonnage par chunks\"\"\"\n",
        "\n",
        "        update_progress(\"🔍 Analyse du gros fichier...\")\n",
        "\n",
        "        # Première passe : estimation et échantillonnage des chunks\n",
        "        chunk_size = self.config.DEFAULT_PARAMS[\"chunk_size\"]\n",
        "        sampled_chunks = []\n",
        "        total_processed = 0\n",
        "\n",
        "        # Calcul du ratio d'échantillonnage approximatif\n",
        "        file_info = self.sampler.estimate_dataset_size(file_path)\n",
        "        if file_info[\"estimated_lines\"] > target_size:\n",
        "            chunk_sample_ratio = target_size / file_info[\"estimated_lines\"] * 2  # x2 pour avoir de la marge\n",
        "        else:\n",
        "            chunk_sample_ratio = 1.0\n",
        "\n",
        "        try:\n",
        "            # Lecture par chunks avec échantillonnage\n",
        "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
        "                try:\n",
        "                    chunk_reader = pd.read_csv(\n",
        "                        file_path,\n",
        "                        encoding=encoding,\n",
        "                        chunksize=chunk_size,\n",
        "                        on_bad_lines='skip',\n",
        "                        engine='python',\n",
        "                        quoting=csv.QUOTE_MINIMAL\n",
        "                    )\n",
        "\n",
        "                    for i, chunk in enumerate(chunk_reader):\n",
        "                        # Validation du premier chunk\n",
        "                        if i == 0:\n",
        "                            if not self._validate_columns(chunk):\n",
        "                                return None\n",
        "\n",
        "                        # Nettoyage du chunk\n",
        "                        cleaned_chunk = self._clean_chunk(chunk)\n",
        "                        if len(cleaned_chunk) == 0:\n",
        "                            continue\n",
        "\n",
        "                        # Échantillonnage du chunk si nécessaire\n",
        "                        if chunk_sample_ratio < 1.0:\n",
        "                            n_samples = max(1, int(len(cleaned_chunk) * chunk_sample_ratio))\n",
        "                            cleaned_chunk = cleaned_chunk.sample(n=n_samples, random_state=42)\n",
        "\n",
        "                        sampled_chunks.append(cleaned_chunk)\n",
        "                        total_processed += len(chunk)\n",
        "\n",
        "                        # Mise à jour de la progression\n",
        "                        progress = min(0.4 + (i * 0.4 / 100), 0.8)  # 40-80% pour le chargement\n",
        "                        progress_bar.progress(progress)\n",
        "                        update_progress(f\"📊 Chunks traités: {i+1} - Lignes: {total_processed:,}\")\n",
        "\n",
        "                        # Arrêt si on a assez de données\n",
        "                        total_samples = sum(len(chunk) for chunk in sampled_chunks)\n",
        "                        if total_samples >= target_size * 3:  # x3 pour avoir de la marge avant l'équilibrage\n",
        "                            break\n",
        "\n",
        "                    break  # Succès avec cet encoding\n",
        "\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if not sampled_chunks:\n",
        "                st.error(\"❌ Aucune donnée valide trouvée\")\n",
        "                return None\n",
        "\n",
        "            # Combinaison des chunks\n",
        "            progress_bar.progress(0.85)\n",
        "            update_progress(\"🔄 Assemblage des données...\")\n",
        "\n",
        "            combined_df = pd.concat(sampled_chunks, ignore_index=True)\n",
        "\n",
        "            # Échantillonnage final stratifié\n",
        "            progress_bar.progress(0.9)\n",
        "            final_sample = self.sampler.create_stratified_sample(\n",
        "                combined_df, target_size, update_progress\n",
        "            )\n",
        "\n",
        "            progress_bar.progress(1.0)\n",
        "            update_progress(f\"✅ Gros fichier traité: {len(final_sample):,} échantillons finaux\")\n",
        "\n",
        "            return final_sample\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors du traitement du gros fichier: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _validate_columns(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Valide la présence des colonnes requises\"\"\"\n",
        "        required_cols = [\"comment_sentiment\", \"post_title\", \"self_text\"]\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            st.error(f\"❌ Colonnes manquantes: {missing_cols}\")\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _validate_and_clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Valide et nettoie un DataFrame complet\"\"\"\n",
        "        if not self._validate_columns(df):\n",
        "            return pd.DataFrame()\n",
        "        return self._clean_data(df)\n",
        "\n",
        "    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Nettoie les données\"\"\"\n",
        "        initial_count = len(df)\n",
        "\n",
        "        # Filtrage des labels valides\n",
        "        valid_labels = set(self.config.LABEL_MAPPING.keys())\n",
        "        df = df[df[\"comment_sentiment\"].isin(valid_labels)]\n",
        "\n",
        "        # Suppression des valeurs manquantes\n",
        "        df = df.dropna(subset=[\"comment_sentiment\", \"post_title\", \"self_text\"])\n",
        "\n",
        "        # Création des labels numériques\n",
        "        df[\"label\"] = df[\"comment_sentiment\"].map(self.config.LABEL_MAPPING).astype(int)\n",
        "\n",
        "        # Création du texte combiné\n",
        "        df[\"text\"] = (\n",
        "            df[\"post_title\"].fillna(\"\") + \" \" + df[\"self_text\"].fillna(\"\")\n",
        "        ).str.strip()\n",
        "\n",
        "        # Filtrage des textes vides\n",
        "        df = df[df[\"text\"].str.len() > 0]\n",
        "\n",
        "        final_count = len(df)\n",
        "        if initial_count > 0:\n",
        "            logger.info(f\"Données nettoyées: {initial_count} → {final_count} échantillons ({final_count/initial_count*100:.1f}% conservés)\")\n",
        "\n",
        "        return df[[\"text\", \"label\", \"comment_sentiment\"]]\n",
        "\n",
        "    def _clean_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Nettoie un chunk de données\"\"\"\n",
        "        return self._clean_data(chunk)\n",
        "\n",
        "    def split_data(self, df: pd.DataFrame, test_size: float, val_size: float) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Divise les données en train/val/test avec stratification\"\"\"\n",
        "        try:\n",
        "            train_val, test = train_test_split(\n",
        "                df, test_size=test_size, random_state=42, stratify=df[\"label\"]\n",
        "            )\n",
        "\n",
        "            train, val = train_test_split(\n",
        "                train_val, test_size=val_size, random_state=42, stratify=train_val[\"label\"]\n",
        "            )\n",
        "\n",
        "            return {\"train\": train, \"validation\": val, \"test\": test}\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de la division des données: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "class ModelManager:\n",
        "    \"\"\"Gestionnaire du modèle et de l'entraînement optimisé\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_logs = {\"train_loss\": [], \"val_loss\": [], \"steps\": [], \"epoch\": []}\n",
        "\n",
        "    def initialize_model(self, lora_params: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Initialise le modèle avec gestion d'erreurs\"\"\"\n",
        "        try:\n",
        "            with st.spinner(\"🤖 Chargement du tokenizer...\"):\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_NAME)\n",
        "                if self.tokenizer.pad_token is None:\n",
        "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            with st.spinner(\"🧠 Chargement du modèle de base...\"):\n",
        "                base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                    self.config.MODEL_NAME,\n",
        "                    torch_dtype=self.config.TORCH_DTYPE,\n",
        "                    device_map=None\n",
        "                ).to(self.config.DEVICE)\n",
        "\n",
        "            with st.spinner(\"🔧 Configuration LoRA...\"):\n",
        "                lora_config = LoraConfig(\n",
        "                    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "                    r=lora_params[\"lora_r\"],\n",
        "                    lora_alpha=lora_params[\"lora_alpha\"],\n",
        "                    target_modules=[\"q\", \"v\"],\n",
        "                    lora_dropout=lora_params[\"lora_dropout\"],\n",
        "                    bias=\"none\"\n",
        "                )\n",
        "\n",
        "                self.model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "            # Affichage des informations du modèle\n",
        "            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total_params = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "            st.success(f\"✅ Modèle initialisé!\")\n",
        "            st.info(f\"📊 Paramètres entraînables: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "\n",
        "            logger.info(\"✅ Modèle initialisé avec succès\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur d'initialisation du modèle: {str(e)}\")\n",
        "            logger.error(f\"Erreur initialisation modèle: {e}\")\n",
        "            return False\n",
        "\n",
        "    def preprocess_data(self, examples: Dict[str, List], max_input_length: int, max_target_length: int):\n",
        "        \"\"\"Préprocesse les données pour l'entraînement\"\"\"\n",
        "        inputs = [f\"classify sentiment: {text}\" for text in examples[\"text\"]]\n",
        "        targets = [self.config.LABEL_NAMES[label] for label in examples[\"label\"]]\n",
        "\n",
        "        model_inputs = self.tokenizer(\n",
        "            inputs,\n",
        "            max_length=max_input_length,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\" if len(inputs) == 1 else None\n",
        "        )\n",
        "\n",
        "        labels = self.tokenizer(\n",
        "            targets,\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\" if len(targets) == 1 else None\n",
        "        )\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    def compute_metrics(self, eval_pred) -> Dict[str, float]:\n",
        "        \"\"\"Calcule les métriques d'évaluation de manière robuste\"\"\"\n",
        "        try:\n",
        "            predictions = eval_pred.predictions[0]\n",
        "            labels = eval_pred.label_ids\n",
        "\n",
        "            # Décodage des prédictions\n",
        "            decoded_preds = []\n",
        "            for pred in predictions:\n",
        "                if isinstance(pred, np.ndarray):\n",
        "                    pred_ids = np.argmax(pred, axis=-1) if pred.ndim > 1 else pred\n",
        "                else:\n",
        "                    pred_ids = pred\n",
        "\n",
        "                decoded_text = self.tokenizer.decode(pred_ids, skip_special_tokens=True).strip().lower()\n",
        "\n",
        "                # Mapping robuste des prédictions\n",
        "                if \"negative\" in decoded_text:\n",
        "                    decoded_preds.append(0)\n",
        "                elif \"positive\" in decoded_text:\n",
        "                    decoded_preds.append(2)\n",
        "                else:\n",
        "                    decoded_preds.append(1)  # neutral par défaut\n",
        "\n",
        "            # Décodage des labels\n",
        "            decoded_labels = []\n",
        "            for label in labels:\n",
        "                if hasattr(label, '__iter__') and not isinstance(label, str):\n",
        "                    label_text = self.tokenizer.decode(label, skip_special_tokens=True).strip().lower()\n",
        "                    if \"negative\" in label_text:\n",
        "                        decoded_labels.append(0)\n",
        "                    elif \"positive\" in label_text:\n",
        "                        decoded_labels.append(2)\n",
        "                    else:\n",
        "                        decoded_labels.append(1)\n",
        "                else:\n",
        "                    decoded_labels.append(int(label))\n",
        "\n",
        "            return {\n",
        "                \"accuracy\": accuracy_score(decoded_labels, decoded_preds),\n",
        "                \"f1_weighted\": f1_score(decoded_labels, decoded_preds, average=\"weighted\"),\n",
        "                \"f1_macro\": f1_score(decoded_labels, decoded_preds, average=\"macro\")\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erreur calcul métriques: {e}\")\n",
        "            return {\"accuracy\": 0.0, \"f1_weighted\": 0.0, \"f1_macro\": 0.0}\n",
        "\n",
        "    def setup_trainer(self, datasets: Dict[str, Dataset], training_params: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Configure le trainer optimisé\"\"\"\n",
        "        try:\n",
        "            # Callback pour tracker les losses\n",
        "            class LossTrackingCallback(TrainerCallback):\n",
        "                def __init__(self, logs_dict):\n",
        "                    self.logs = logs_dict\n",
        "\n",
        "                def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "                    if logs:\n",
        "                        if \"loss\" in logs:\n",
        "                            self.logs[\"train_loss\"].append(logs[\"loss\"])\n",
        "                            self.logs[\"steps\"].append(state.global_step)\n",
        "                            self.logs[\"epoch\"].append(state.epoch)\n",
        "                        if \"eval_loss\" in logs:\n",
        "                            self.logs[\"val_loss\"].append(logs[\"eval_loss\"])\n",
        "\n",
        "            # Configuration optimisée de l'entraînement\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=\"./lora_climate_model\",\n",
        "                per_device_train_batch_size=training_params[\"train_batch_size\"],\n",
        "                per_device_eval_batch_size=training_params[\"eval_batch_size\"],\n",
        "                num_train_epochs=training_params[\"num_epochs\"],\n",
        "                learning_rate=training_params[\"learning_rate\"],\n",
        "                warmup_steps=100,  # Warm-up pour stabiliser l'entraînement\n",
        "                eval_strategy=\"steps\",\n",
        "                eval_steps=training_params[\"eval_steps\"],\n",
        "                logging_steps=training_params[\"logging_steps\"],\n",
        "                save_strategy=\"steps\",\n",
        "                save_steps=training_params[\"eval_steps\"],\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"eval_f1_weighted\",\n",
        "                greater_is_better=True,\n",
        "                report_to=None,\n",
        "                dataloader_pin_memory=False,\n",
        "                remove_unused_columns=True,\n",
        "                push_to_hub=False,\n",
        "                fp16=torch.cuda.is_available(),  # Optimisation mémoire si GPU\n",
        "            )\n",
        "\n",
        "            self.trainer = Trainer(\n",
        "                model=self.model,\n",
        "                args=training_args,\n",
        "                train_dataset=datasets[\"train\"],\n",
        "                eval_dataset=datasets[\"validation\"],\n",
        "                tokenizer=self.tokenizer,\n",
        "                compute_metrics=self.compute_metrics,\n",
        "                callbacks=[LossTrackingCallback(self.training_logs)]\n",
        "            )\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur configuration trainer: {str(e)}\")\n",
        "            logger.error(f\"Erreur setup trainer: {e}\")\n",
        "            return False\n",
        "\n",
        "def create_streamlit_app():\n",
        "    \"\"\"Interface Streamlit optimisée pour l'Option A\"\"\"\n",
        "\n",
        "    st.set_page_config(\n",
        "        page_title=\"🌍 Climate Sentiment AI - Option A\",\n",
        "        page_icon=\"🌍\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\"\n",
        "    )\n",
        "\n",
        "    st.title(\"🌍 Climate Sentiment AI - Option A : Échantillonnage Intelligent\")\n",
        "    st.markdown(\"*Optimisé pour traiter efficacement des datasets de 1,2 Go avec échantillonnage stratifié*\")\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Initialisation des objets\n",
        "    config = Config()\n",
        "    data_processor = OptimizedDataProcessor(config)\n",
        "    model_manager = ModelManager(config)\n",
        "\n",
        "    # Sidebar optimisée pour l'échantillonnage\n",
        "    st.sidebar.header(\"⚙️ Configuration Option A\")\n",
        "\n",
        "    # Sélection de la stratégie d'échantillonnage\n",
        "    st.sidebar.subheader(\"🎯 Stratégie d'échantillonnage\")\n",
        "\n",
        "    sample_strategies = {\n",
        "        \"🚀 Test rapide (30K)\": {\n",
        "            \"size\": config.DEFAULT_PARAMS[\"sample_sizes\"][\"test_rapide\"],\n",
        "            \"description\": \"Validation rapide en 5 min\",\n",
        "            \"use_case\": \"Test de faisabilité\"\n",
        "        },\n",
        "        \"✅ Validation (75K)\": {\n",
        "            \"size\": config.DEFAULT_PARAMS[\"sample_sizes\"][\"validation\"],\n",
        "            \"description\": \"Équilibre temps/qualité en 15 min\",\n",
        "            \"use_case\": \"Développement et test\"\n",
        "        },\n",
        "        \"🎯 Production (150K)\": {\n",
        "            \"size\": config.DEFAULT_PARAMS[\"sample_sizes\"][\"production\"],\n",
        "            \"description\": \"Modèle final en 30 min\",\n",
        "            \"use_case\": \"Modèle de production\"\n",
        "        },\n",
        "        \"🔥 Maximum (300K)\": {\n",
        "            \"size\": config.DEFAULT_PARAMS[\"sample_sizes\"][\"maximum\"],\n",
        "            \"description\": \"Performance maximale en 60 min\",\n",
        "            \"use_case\": \"Si qualité insuffisante\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    selected_strategy = st.sidebar.selectbox(\n",
        "        \"Choisir la stratégie\",\n",
        "        options=list(sample_strategies.keys()),\n",
        "        index=2,  # Production par défaut\n",
        "        help=\"Choisissez selon vos contraintes de temps et qualité\"\n",
        "    )\n",
        "\n",
        "    strategy_info = sample_strategies[selected_strategy]\n",
        "    target_sample_size = strategy_info[\"size\"]\n",
        "\n",
        "    # Affichage des informations de la stratégie\n",
        "    st.sidebar.info(f\"\"\"\n",
        "    **{selected_strategy}**\n",
        "\n",
        "    📊 Échantillons: {target_sample_size:,}\n",
        "    ⏱️ Temps estimé: {strategy_info['description']}\n",
        "    🎯 Usage: {strategy_info['use_case']}\n",
        "    \"\"\")\n",
        "\n",
        "    # Upload de fichier\n",
        "    st.sidebar.subheader(\"📁 Fichier de données\")\n",
        "    uploaded_file = st.sidebar.file_uploader(\n",
        "        \"Charger fichier CSV\",\n",
        "        type=[\"csv\"],\n",
        "        help=\"Fichier avec colonnes: comment_sentiment, post_title, self_text\"\n",
        "    )\n",
        "\n",
        "    # Option de chemin local pour très gros fichiers\n",
        "    st.sidebar.markdown(\"**Pour fichiers > 200MB:**\")\n",
        "    local_file_path = st.sidebar.text_input(\n",
        "        \"Chemin fichier local\",\n",
        "        placeholder=\"/path/to/large_file.csv\",\n",
        "        help=\"Contourner la limite Streamlit\"\n",
        "    )\n",
        "    use_local_file = st.sidebar.button(\"📂 Utiliser fichier local\")\n",
        "\n",
        "    # Paramètres d'entraînement\n",
        "    st.sidebar.subheader(\"🏋️ Paramètres d'entraînement\")\n",
        "    num_epochs = st.sidebar.slider(\"Époques\", 1, 5, config.DEFAULT_PARAMS[\"num_epochs\"])\n",
        "    learning_rate = st.sidebar.select_slider(\n",
        "        \"Learning rate\",\n",
        "        options=[1e-5, 5e-5, 1e-4, 5e-4, 1e-3],\n",
        "        value=config.DEFAULT_PARAMS[\"learning_rate\"],\n",
        "        format_func=lambda x: f\"{x:.0e}\"\n",
        "    )\n",
        "    batch_size = st.sidebar.selectbox(\"Batch size\", [4, 8, 16], index=1)\n",
        "\n",
        "    # Interface principale\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "\n",
        "    with col1:\n",
        "        st.header(\"📊 Tableau de bord\")\n",
        "\n",
        "        # Détermination de la source de données\n",
        "        data_source = None\n",
        "        tmp_file_path = None\n",
        "\n",
        "        if uploaded_file is not None:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp_file:\n",
        "                tmp_file.write(uploaded_file.getvalue())\n",
        "                tmp_file_path = tmp_file.name\n",
        "            data_source = tmp_file_path\n",
        "\n",
        "        elif use_local_file and local_file_path.strip():\n",
        "            if os.path.exists(local_file_path.strip()):\n",
        "                data_source = local_file_path.strip()\n",
        "            else:\n",
        "                st.error(f\"❌ Fichier non trouvé: {local_file_path}\")\n",
        "\n",
        "        # Traitement des données\n",
        "        if data_source:\n",
        "            try:\n",
        "                st.subheader(\"🎯 Échantillonnage intelligent\")\n",
        "\n",
        "                # Traitement des données avec échantillonnage\n",
        "                df = data_processor.load_and_sample_data(data_source, target_sample_size)\n",
        "\n",
        "                if df is not None:\n",
        "                    # Division des données\n",
        "                    data_splits = data_processor.split_data(\n",
        "                        df,\n",
        "                        config.DEFAULT_PARAMS[\"test_size\"],\n",
        "                        config.DEFAULT_PARAMS[\"val_size\"]\n",
        "                    )\n",
        "\n",
        "                    if data_splits:\n",
        "                        # Statistiques détaillées\n",
        "                        st.subheader(\"📈 Statistiques de l'échantillon\")\n",
        "\n",
        "                        # Métriques principales\n",
        "                        metrics_col1, metrics_col2, metrics_col3, metrics_col4 = st.columns(4)\n",
        "\n",
        "                        with metrics_col1:\n",
        "                            st.metric(\"📊 Total\", f\"{len(df):,}\")\n",
        "                        with metrics_col2:\n",
        "                            st.metric(\"🏋️ Train\", f\"{len(data_splits['train']):,}\")\n",
        "                        with metrics_col3:\n",
        "                            st.metric(\"✅ Validation\", f\"{len(data_splits['validation']):,}\")\n",
        "                        with metrics_col4:\n",
        "                            st.metric(\"🧪 Test\", f\"{len(data_splits['test']):,}\")\n",
        "\n",
        "                        # Visualisations\n",
        "                        viz_col1, viz_col2 = st.columns(2)\n",
        "\n",
        "                        with viz_col1:\n",
        "                            # Distribution des sentiments\n",
        "                            sentiment_counts = df['comment_sentiment'].value_counts()\n",
        "                            fig_pie = px.pie(\n",
        "                                values=sentiment_counts.values,\n",
        "                                names=sentiment_counts.index,\n",
        "                                title=\"Distribution des sentiments\",\n",
        "                                color_discrete_map={\n",
        "                                    'negative': '#ff6b6b',\n",
        "                                    'neutral': '#ffd93d',\n",
        "                                    'positive': '#6bcf7f'\n",
        "                                }\n",
        "                            )\n",
        "                            st.plotly_chart(fig_pie, use_container_width=True)\n",
        "\n",
        "                        with viz_col2:\n",
        "                            # Distribution par split\n",
        "                            split_data = []\n",
        "                            for split_name, split_df in data_splits.items():\n",
        "                                for sentiment in ['negative', 'neutral', 'positive']:\n",
        "                                    count = len(split_df[split_df['comment_sentiment'] == sentiment])\n",
        "                                    split_data.append({\n",
        "                                        'Split': split_name.capitalize(),\n",
        "                                        'Sentiment': sentiment,\n",
        "                                        'Count': count\n",
        "                                    })\n",
        "\n",
        "                            split_df_viz = pd.DataFrame(split_data)\n",
        "                            fig_bar = px.bar(\n",
        "                                split_df_viz,\n",
        "                                x='Split',\n",
        "                                y='Count',\n",
        "                                color='Sentiment',\n",
        "                                title=\"Distribution par split\",\n",
        "                                color_discrete_map={\n",
        "                                    'negative': '#ff6b6b',\n",
        "                                    'neutral': '#ffd93d',\n",
        "                                    'positive': '#6bcf7f'\n",
        "                                }\n",
        "                            )\n",
        "                            st.plotly_chart(fig_bar, use_container_width=True)\n",
        "\n",
        "                        # Aperçu des données\n",
        "                        st.subheader(\"👀 Aperçu des données\")\n",
        "                        sample_data = df.sample(n=min(5, len(df)), random_state=42)\n",
        "\n",
        "                        for idx, row in sample_data.iterrows():\n",
        "                            sentiment_color = {\n",
        "                                'negative': '🔴',\n",
        "                                'neutral': '🟡',\n",
        "                                'positive': '🟢'\n",
        "                            }\n",
        "\n",
        "                            with st.expander(f\"{sentiment_color[row['comment_sentiment']]} {row['comment_sentiment'].upper()} - Échantillon {idx}\"):\n",
        "                                st.write(f\"**Texte:** {row['text'][:200]}...\")\n",
        "\n",
        "                        # Stockage dans session state\n",
        "                        st.session_state[\"data_splits\"] = data_splits\n",
        "                        st.session_state[\"data_ready\"] = True\n",
        "                        st.session_state[\"sample_strategy\"] = selected_strategy\n",
        "\n",
        "                        # Informations sur la stratégie utilisée\n",
        "                        st.success(f\"✅ {selected_strategy} appliquée avec succès!\")\n",
        "\n",
        "            finally:\n",
        "                # Nettoyage du fichier temporaire\n",
        "                if tmp_file_path and os.path.exists(tmp_file_path):\n",
        "                    os.unlink(tmp_file_path)\n",
        "\n",
        "        elif \"data_ready\" not in st.session_state:\n",
        "            st.info(\"👆 Veuillez charger un fichier CSV pour commencer l'échantillonnage intelligent\")\n",
        "\n",
        "            # Guide d'utilisation\n",
        "            st.markdown(\"\"\"\n",
        "            ### 🎯 Guide d'utilisation Option A\n",
        "\n",
        "            **1. Choisissez votre stratégie d'échantillonnage:**\n",
        "            - 🚀 **Test rapide (30K)**: Pour valider rapidement votre pipeline\n",
        "            - ✅ **Validation (75K)**: Bon équilibre pour le développement\n",
        "            - 🎯 **Production (150K)**: Modèle final de qualité production\n",
        "            - 🔥 **Maximum (300K)**: Performance maximale si nécessaire\n",
        "\n",
        "            **2. Chargez vos données:**\n",
        "            - Fichiers < 200MB: Upload direct\n",
        "            - Fichiers > 200MB: Chemin local\n",
        "\n",
        "            **3. L'algorithme va:**\n",
        "            - Analyser votre dataset\n",
        "            - Créer un échantillon équilibré et représentatif\n",
        "            - Optimiser pour vos contraintes de temps\n",
        "            \"\"\")\n",
        "\n",
        "    with col2:\n",
        "        st.header(\"🚀 Actions\")\n",
        "\n",
        "        # Informations sur le GPU/CPU\n",
        "        device_info = \"🔥 GPU\" if config.DEVICE == \"cuda\" else \"💻 CPU\"\n",
        "        st.info(f\"**Dispositif:** {device_info}\")\n",
        "\n",
        "        if \"data_ready\" in st.session_state:\n",
        "            strategy_used = st.session_state.get(\"sample_strategy\", \"Non définie\")\n",
        "            st.success(f\"**Stratégie:** {strategy_used}\")\n",
        "\n",
        "        # Bouton d'initialisation du modèle\n",
        "        if st.button(\"🤖 Initialiser Modèle\", use_container_width=True):\n",
        "            if \"data_ready\" in st.session_state:\n",
        "                lora_params = {\n",
        "                    \"lora_r\": config.DEFAULT_PARAMS[\"lora_r\"],\n",
        "                    \"lora_alpha\": config.DEFAULT_PARAMS[\"lora_alpha\"],\n",
        "                    \"lora_dropout\": config.DEFAULT_PARAMS[\"lora_dropout\"]\n",
        "                }\n",
        "\n",
        "                if model_manager.initialize_model(lora_params):\n",
        "                    st.session_state[\"model_ready\"] = True\n",
        "            else:\n",
        "                st.warning(\"⚠️ Chargez d'abord les données\")\n",
        "\n",
        "        # Bouton d'entraînement\n",
        "        if st.button(\"🏋️ Lancer Entraînement\", use_container_width=True):\n",
        "            if \"model_ready\" in st.session_state and \"data_ready\" in st.session_state:\n",
        "\n",
        "                # Estimation du temps d'entraînement\n",
        "                data_splits = st.session_state[\"data_splits\"]\n",
        "                train_size = len(data_splits[\"train\"])\n",
        "                estimated_time = (train_size * num_epochs * batch_size) // 1000  # Estimation approximative\n",
        "\n",
        "                st.info(f\"⏱️ Temps estimé: ~{estimated_time} minutes\")\n",
        "\n",
        "                with st.spinner(\"🏋️ Entraînement en cours...\"):\n",
        "                    try:\n",
        "                        # Préparation des datasets\n",
        "                        datasets = {}\n",
        "\n",
        "                        for split_name, split_data in data_splits.items():\n",
        "                            dataset = Dataset.from_pandas(split_data)\n",
        "                            dataset = dataset.map(\n",
        "                                lambda x: model_manager.preprocess_data(\n",
        "                                    x,\n",
        "                                    config.DEFAULT_PARAMS[\"max_input_length\"],\n",
        "                                    config.DEFAULT_PARAMS[\"max_target_length\"]\n",
        "                                ),\n",
        "                                batched=True,\n",
        "                                remove_columns=split_data.columns.tolist()\n",
        "                            )\n",
        "                            dataset.set_format(\"torch\")\n",
        "                            datasets[split_name] = dataset\n",
        "\n",
        "                        # Configuration du trainer\n",
        "                        training_params = {\n",
        "                            \"train_batch_size\": batch_size,\n",
        "                            \"eval_batch_size\": batch_size,\n",
        "                            \"num_epochs\": num_epochs,\n",
        "                            \"learning_rate\": learning_rate,\n",
        "                            \"eval_steps\": config.DEFAULT_PARAMS[\"eval_steps\"],\n",
        "                            \"logging_steps\": config.DEFAULT_PARAMS[\"logging_steps\"]\n",
        "                        }\n",
        "\n",
        "                        if model_manager.setup_trainer(datasets, training_params):\n",
        "                            # Lancement de l'entraînement\n",
        "                            start_time = time.time()\n",
        "                            model_manager.trainer.train()\n",
        "                            training_time = time.time() - start_time\n",
        "\n",
        "                            st.session_state[\"training_complete\"] = True\n",
        "                            st.session_state[\"training_time\"] = training_time\n",
        "\n",
        "                            st.success(f\"✅ Entraînement terminé en {training_time/60:.1f} minutes!\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"❌ Erreur pendant l'entraînement: {str(e)}\")\n",
        "                        logger.error(f\"Erreur entraînement: {e}\")\n",
        "            else:\n",
        "                st.warning(\"⚠️ Initialisez d'abord le modèle\")\n",
        "\n",
        "        # Bouton d'évaluation\n",
        "        if st.button(\"📊 Évaluer sur Test\", use_container_width=True):\n",
        "            if \"training_complete\" in st.session_state:\n",
        "                with st.spinner(\"📊 Évaluation en cours...\"):\n",
        "                    try:\n",
        "                        data_splits = st.session_state[\"data_splits\"]\n",
        "                        test_dataset = Dataset.from_pandas(data_splits[\"test\"])\n",
        "                        test_dataset = test_dataset.map(\n",
        "                            lambda x: model_manager.preprocess_data(\n",
        "                                x,\n",
        "                                config.DEFAULT_PARAMS[\"max_input_length\"],\n",
        "                                config.DEFAULT_PARAMS[\"max_target_length\"]\n",
        "                            ),\n",
        "                            batched=True,\n",
        "                            remove_columns=data_splits[\"test\"].columns.tolist()\n",
        "                        )\n",
        "                        test_dataset.set_format(\"torch\")\n",
        "\n",
        "                        results = model_manager.trainer.evaluate(test_dataset)\n",
        "                        st.session_state[\"test_results\"] = results\n",
        "\n",
        "                        # Affichage des résultats avec contexte\n",
        "                        st.subheader(\"🎯 Résultats finaux\")\n",
        "\n",
        "                        col1, col2, col3 = st.columns(3)\n",
        "                        with col1:\n",
        "                            acc = results.get('eval_accuracy', 0)\n",
        "                            st.metric(\"🎯 Accuracy\", f\"{acc:.3f}\", delta=f\"{(acc-0.33)*100:+.1f}%\" if acc > 0.33 else None)\n",
        "                        with col2:\n",
        "                            f1w = results.get('eval_f1_weighted', 0)\n",
        "                            st.metric(\"📊 F1 Weighted\", f\"{f1w:.3f}\")\n",
        "                        with col3:\n",
        "                            f1m = results.get('eval_f1_macro', 0)\n",
        "                            st.metric(\"📈 F1 Macro\", f\"{f1m:.3f}\")\n",
        "\n",
        "                        # Interprétation des résultats\n",
        "                        if acc > 0.75:\n",
        "                            st.success(\"🎉 Excellents résultats! Modèle prêt pour la production.\")\n",
        "                        elif acc > 0.65:\n",
        "                            st.info(\"✅ Bons résultats. Considérez la stratégie 'Maximum' pour améliorer.\")\n",
        "                        else:\n",
        "                            st.warning(\"⚠️ Résultats moyens. Essayez avec plus de données ou ajustez les paramètres.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"❌ Erreur pendant l'évaluation: {str(e)}\")\n",
        "                        logger.error(f\"Erreur évaluation: {e}\")\n",
        "            else:\n",
        "                st.warning(\"⚠️ Terminez d'abord l'entraînement\")\n",
        "\n",
        "        # Informations de performance\n",
        "        if \"training_complete\" in st.session_state:\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"⚡ Performance\")\n",
        "\n",
        "            training_time = st.session_state.get(\"training_time\", 0)\n",
        "            strategy_used = st.session_state.get(\"sample_strategy\", \"Non définie\")\n",
        "\n",
        "            st.metric(\"⏱️ Temps d'entraînement\", f\"{training_time/60:.1f} min\")\n",
        "            st.info(f\"**Stratégie utilisée:** {strategy_used}\")\n",
        "\n",
        "    # Onglets pour les visualisations avancées\n",
        "    if \"training_complete\" in st.session_state:\n",
        "        st.markdown(\"---\")\n",
        "        tab1, tab2, tab3 = st.tabs([\"📈 Courbes d'apprentissage\", \"🔍 Test interactif\", \"📋 Rapport détaillé\"])\n",
        "\n",
        "        with tab1:\n",
        "            if model_manager.training_logs[\"train_loss\"]:\n",
        "                # Graphique des losses\n",
        "                fig = go.Figure()\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=model_manager.training_logs[\"steps\"],\n",
        "                    y=model_manager.training_logs[\"train_loss\"],\n",
        "                    mode='lines+markers',\n",
        "                    name='Train Loss',\n",
        "                    line=dict(color='blue', width=2)\n",
        "                ))\n",
        "\n",
        "                if model_manager.training_logs[\"val_loss\"]:\n",
        "                    val_steps = model_manager.training_logs[\"steps\"][:len(model_manager.training_logs[\"val_loss\"])]\n",
        "                    fig.add_trace(go.Scatter(\n",
        "                        x=val_steps,\n",
        "                        y=model_manager.training_logs[\"val_loss\"],\n",
        "                        mode='lines+markers',\n",
        "                        name='Validation Loss',\n",
        "                        line=dict(color='red', width=2)\n",
        "                    ))\n",
        "\n",
        "                fig.update_layout(\n",
        "                    title=\"Évolution des losses pendant l'entraînement\",\n",
        "                    xaxis_title=\"Steps\",\n",
        "                    yaxis_title=\"Loss\",\n",
        "                    hovermode='x unified',\n",
        "                    template=\"plotly_white\"\n",
        "                )\n",
        "\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "                # Analyse de la convergence\n",
        "                if len(model_manager.training_logs[\"train_loss\"]) > 5:\n",
        "                    last_losses = model_manager.training_logs[\"train_loss\"][-5:]\n",
        "                    loss_trend = (last_losses[-1] - last_losses[0]) / last_losses[0] * 100\n",
        "\n",
        "                    if loss_trend < -1:\n",
        "                        st.success(f\"📈 Modèle en cours d'amélioration (-{abs(loss_trend):.1f}% sur les derniers steps)\")\n",
        "                    elif loss_trend > 1:\n",
        "                        st.warning(f\"📉 Loss en augmentation (+{loss_trend:.1f}% - possible surentraînement)\")\n",
        "                    else:\n",
        "                        st.info(\"📊 Loss stabilisée - Convergence atteinte\")\n",
        "            else:\n",
        "                st.info(\"Aucune donnée d'entraînement disponible\")\n",
        "\n",
        "        with tab2:\n",
        "            st.subheader(\"🔍 Test de prédiction interactif\")\n",
        "\n",
        "            # Exemples prédéfinis\n",
        "            example_texts = {\n",
        "                \"Négatif\": \"Climate change is destroying our planet and governments are doing nothing about it!\",\n",
        "                \"Neutre\": \"Scientists published a new study about climate change impacts on weather patterns.\",\n",
        "                \"Positif\": \"Great progress on renewable energy! Solar panels are becoming more efficient and affordable.\"\n",
        "            }\n",
        "\n",
        "            col1, col2 = st.columns([2, 1])\n",
        "\n",
        "            with col1:\n",
        "                test_text = st.text_area(\n",
        "                    \"Entrez un texte à classifier:\",\n",
        "                    value=example_texts[\"Neutre\"],\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            with col2:\n",
        "                st.write(\"**Exemples:**\")\n",
        "                for label, text in example_texts.items():\n",
        "                    if st.button(f\"📝 {label}\", key=f\"example_{label}\"):\n",
        "                        st.session_state[\"test_text\"] = text\n",
        "                        st.experimental_rerun()\n",
        "\n",
        "            if st.session_state.get(\"test_text\"):\n",
        "                test_text = st.session_state[\"test_text\"]\n",
        "\n",
        "            if st.button(\"🎯 Prédire\", use_container_width=True) and test_text.strip():\n",
        "                try:\n",
        "                    # Préparation de l'input\n",
        "                    input_text = f\"classify sentiment: {test_text}\"\n",
        "                    inputs = model_manager.tokenizer(\n",
        "                        input_text,\n",
        "                        return_tensors=\"pt\",\n",
        "                        max_length=config.DEFAULT_PARAMS[\"max_input_length\"],\n",
        "                        truncation=True,\n",
        "                        padding=True\n",
        "                    ).to(config.DEVICE)\n",
        "\n",
        "                    # Prédiction avec probabilités\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model_manager.model.generate(\n",
        "                            **inputs,\n",
        "                            max_length=config.DEFAULT_PARAMS[\"max_target_length\"],\n",
        "                            num_beams=3,\n",
        "                            early_stopping=True,\n",
        "                            return_dict_in_generate=True,\n",
        "                            output_scores=True\n",
        "                        )\n",
        "\n",
        "                    predicted_text = model_manager.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "                    # Affichage du résultat avec style\n",
        "                    sentiment_styles = {\n",
        "                        \"negative\": {\"color\": \"#ff6b6b\", \"icon\": \"🔴\", \"bg\": \"#ffe6e6\"},\n",
        "                        \"neutral\": {\"color\": \"#ffd93d\", \"icon\": \"🟡\", \"bg\": \"#fffacd\"},\n",
        "                        \"positive\": {\"color\": \"#6bcf7f\", \"icon\": \"🟢\", \"bg\": \"#e6ffe6\"}\n",
        "                    }\n",
        "\n",
        "                    predicted_sentiment = predicted_text.lower()\n",
        "                    if predicted_sentiment in sentiment_styles:\n",
        "                        style = sentiment_styles[predicted_sentiment]\n",
        "\n",
        "                        st.markdown(f\"\"\"\n",
        "                        <div style=\"padding: 20px; border-radius: 10px; background-color: {style['bg']}; border-left: 5px solid {style['color']};\">\n",
        "                            <h3 style=\"color: {style['color']}; margin: 0;\">\n",
        "                                {style['icon']} Prédiction: {predicted_sentiment.upper()}\n",
        "                            </h3>\n",
        "                            <p style=\"margin: 10px 0 0 0; font-style: italic;\">\"{test_text}\"</p>\n",
        "                        </div>\n",
        "                        \"\"\", unsafe_allow_html=True)\n",
        "                    else:\n",
        "                        st.success(f\"**Prédiction:** {predicted_text.upper()}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"❌ Erreur de prédiction: {str(e)}\")\n",
        "\n",
        "        with tab3:\n",
        "            st.subheader(\"📋 Rapport détaillé de l'entraînement\")\n",
        "\n",
        "            # Résumé de la configuration\n",
        "            config_summary = {\n",
        "                \"Stratégie d'échantillonnage\": st.session_state.get(\"sample_strategy\", \"Non définie\"),\n",
        "                \"Taille de l'échantillon\": f\"{len(st.session_state.get('data_splits', {}).get('train', [])):,} (train)\",\n",
        "                \"Époques\": num_epochs,\n",
        "                \"Learning rate\": f\"{learning_rate:.0e}\",\n",
        "                \"Batch size\": batch_size,\n",
        "                \"Dispositif\": config.DEVICE.upper(),\n",
        "                \"Temps d'entraînement\": f\"{st.session_state.get('training_time', 0)/60:.1f} min\"\n",
        "            }\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "\n",
        "            with col1:\n",
        "                st.markdown(\"**Configuration:**\")\n",
        "                for key, value in config_summary.items():\n",
        "                    st.write(f\"• **{key}:** {value}\")\n",
        "\n",
        "            with col2:\n",
        "                if \"test_results\" in st.session_state:\n",
        "                    results = st.session_state[\"test_results\"]\n",
        "                    st.markdown(\"**Métriques finales:**\")\n",
        "                    st.write(f\"• **Accuracy:** {results.get('eval_accuracy', 0):.3f}\")\n",
        "                    st.write(f\"• **F1 Weighted:** {results.get('eval_f1_weighted', 0):.3f}\")\n",
        "                    st.write(f\"• **F1 Macro:** {results.get('eval_f1_macro', 0):.3f}\")\n",
        "                    st.write(f\"• **Loss finale:** {results.get('eval_loss', 0):.3f}\")\n",
        "\n",
        "            # Recommandations\n",
        "            st.markdown(\"---\")\n",
        "            st.markdown(\"**🎯 Recommandations pour améliorer les performances:**\")\n",
        "\n",
        "            if \"test_results\" in st.session_state:\n",
        "                acc = st.session_state[\"test_results\"].get('eval_accuracy', 0)\n",
        "\n",
        "                if acc < 0.65:\n",
        "                    st.markdown(\"\"\"\n",
        "                    - 📈 **Augmenter la taille de l'échantillon** (stratégie Maximum)\n",
        "                    - 🔧 **Ajuster le learning rate** (essayer 1e-4 ou 1e-3)\n",
        "                    - 📚 **Augmenter le nombre d'époques** (5-7 époques)\n",
        "                    - 🎯 **Vérifier la qualité des données** (textes trop courts, labels incorrects)\n",
        "                    \"\"\")\n",
        "                elif acc < 0.75:\n",
        "                    st.markdown(\"\"\"\n",
        "                    - ✅ **Bon modèle!** Considérez la stratégie Maximum pour gagner quelques points\n",
        "                    - 🔧 **Fine-tuning des hyperparamètres** (LoRA rank, alpha)\n",
        "                    - 📊 **Analyse des erreurs** sur les prédictions incorrectes\n",
        "                    \"\"\")\n",
        "                else:\n",
        "                    st.markdown(\"\"\"\n",
        "                    - 🎉 **Excellent modèle!** Prêt pour la production\n",
        "                    - 💾 **Sauvegarder le modèle** pour utilisation future\n",
        "                    - 🚀 **Déploiement recommandé**\n",
        "                    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_streamlit_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCzbe3RqvYEL",
        "outputId": "974023b7-e50f-4cd5-d78c-66ddfda4b3cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting climate_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('streamlit run climate_app.py --server.port=8501 --server.address=0.0.0.0 &')"
      ],
      "metadata": {
        "id": "00xnLGe2wq0y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmXGNSjvBxiD",
        "outputId": "f1418997-7f56-4d77-82a0-4b136b385d1f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, subprocess, socket\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1️⃣ Nettoie tout\n",
        "subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], stderr=subprocess.DEVNULL)\n",
        "subprocess.run([\"pkill\", \"-9\", \"-f\", \"ngrok\"],   stderr=subprocess.DEVNULL)\n",
        "ngrok.kill()\n",
        "time.sleep(2)\n",
        "\n",
        "# 2️⃣ Démarre Streamlit en arrière-plan\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"climate_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3️⃣ Attend que le port 8501 soit vraiment écouté\n",
        "def wait_for_port(port=8501, timeout=10):\n",
        "    for _ in range(timeout):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            if s.connect_ex((\"localhost\", port)) == 0:\n",
        "                return True\n",
        "        time.sleep(1)\n",
        "    return False\n",
        "\n",
        "if wait_for_port():\n",
        "    # 4️⃣ Reconnecte ngrok\n",
        "    ngrok.set_auth_token(\"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\")\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"🔗 Accès public :\", public_url)\n",
        "else:\n",
        "    print(\"❌ Streamlit n’a pas démarré sur le port 8501\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD_R7lqVwrtc",
        "outputId": "a78107ea-8a61-4a10-bf71-3cf3a4f97d8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Accès public : NgrokTunnel: \"https://8c4f2b343f54.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}