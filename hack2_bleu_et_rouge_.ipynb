{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Module Core - core_modules.py"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional\n",
        "import torch\n",
        "import logging   # ← remplacement simple\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    text: str\n",
        "    predicted_label: str\n",
        "    confidence: float\n",
        "    all_scores: Dict[str, float]\n",
        "    context: Optional[List[str]] = None\n",
        "    processing_time: float = 0.0\n",
        "\n",
        "class ClimateConfig:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"distilbert-base-uncased\"\n",
        "        self.max_length = 256\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 2e-4\n",
        "        self.epochs = 3\n",
        "        self.lora_r = 16\n",
        "        self.lora_alpha = 32\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.output_dir = \"outputs/final_model\"\n",
        "        self.qa_model_name = 'all-MiniLM-L6-v2'\n",
        "        self.qa_similarity_threshold = 0.1"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05289ae0-ed19-4e79-9676-32783a630eff"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Module Data Processing - data_modules.py"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Tuple\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "        self.reverse_label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        text_keywords = ['text', 'content', 'message', 'comment', 'body', 'description', 'self_text']\n",
        "        label_keywords = ['label', 'sentiment', 'category', 'class', 'target', 'comment_sentiment']\n",
        "        text_col = next((c for c in df.columns if any(k in str(c).lower() for k in text_keywords)), None)\n",
        "        label_col = next((c for c in df.columns if any(k in str(c).lower() for k in label_keywords)), None)\n",
        "        if not text_col:\n",
        "            text_col = df.select_dtypes(include=['object']).columns[0]\n",
        "        if not label_col:\n",
        "            label_col = df.columns[-1]\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text) or str(text).strip().lower() in ['nan', 'none', '', 'null']:\n",
        "            return None\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r'&gt;|&lt;|&amp;', lambda m: {'&gt;': '>', '&lt;': '<', '&amp;': '&'}[m.group()], text)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip() if text.strip() else None\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "        df_clean = df[[self.text_col, self.label_col]].copy()\n",
        "        df_clean.columns = ['text', 'label']\n",
        "        df_clean['text'] = df_clean['text'].apply(self.clean_text)\n",
        "        df_clean['label'] = df_clean['label'].astype(str)\n",
        "        df_clean = df_clean.dropna().reset_index(drop=True)\n",
        "        df_clean = df_clean[df_clean['text'].str.len() >= 10]\n",
        "\n",
        "        if len(df_clean) > sample_size:\n",
        "            df_clean = df_clean.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        unique_labels = sorted(df_clean['label'].unique())\n",
        "        self.label_mapping = {str(l): i for i, l in enumerate(unique_labels)}\n",
        "        df_clean['label_id'] = df_clean['label'].map(self.label_mapping)\n",
        "\n",
        "        # Nettoyage final NaN\n",
        "        df_clean = df_clean.dropna(subset=['label_id'])\n",
        "        df_clean['label_id'] = df_clean['label_id'].astype(int)\n",
        "\n",
        "        if df_clean.empty:\n",
        "            raise ValueError(\"❌ Aucune donnée valide après nettoyage.\")\n",
        "\n",
        "        train_df, temp = train_test_split(df_clean, test_size=0.4, stratify=df_clean['label_id'], random_state=42)\n",
        "        val_df, test_df = train_test_split(temp, test_size=0.5, stratify=temp['label_id'], random_state=42)\n",
        "\n",
        "        return (\n",
        "            Dataset.from_pandas(train_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(val_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(test_df[['text', 'label_id']])\n",
        "        )"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2813998-21c3-4ac6-92a2-b89659fbc9f1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3️⃣ Module Modèle - model_modules.py"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        return self.tokenizer\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float32,\n",
        "            problem_type=\"single_label_classification\"\n",
        "        )\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=self.config.lora_r,\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_lin\", \"v_lin\"],\n",
        "            bias=\"none\",\n",
        "        )\n",
        "        self.peft_model = get_peft_model(base_model, lora_config)\n",
        "        return self.peft_model\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.config.max_length,\n",
        "        )\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        preds = np.argmax(predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "        }\n",
        "\n",
        "    def setup_training_args(self):\n",
        "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.config.output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            save_steps=500,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "            fp16=False,\n",
        "            bf16=False,\n",
        "            fp16_full_eval=False,\n",
        "            bf16_full_eval=False,\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        return Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=self.setup_training_args(),\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "        )"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05786a7a-9815-4e4e-d77e-55623cff34cc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. visualization_modules.py"
      ],
      "metadata": {
        "id": "qYJhFun3Mws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# --- NLTK / BLEU / ROUGE ---\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Téléchargement silencieux des ressources NLTK\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "plt.style.use('default')\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestionnaire de visualisations pour Climate Analyzer.\"\"\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Outils internes BLEU / ROUGE\n",
        "    # --------------------------------------------------\n",
        "    _smoothie = SmoothingFunction().method4\n",
        "    _rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _bleu(ref: str, hyp: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        ref_tok = nltk.word_tokenize(ref.lower())\n",
        "        hyp_tok = nltk.word_tokenize(hyp.lower())\n",
        "        return sentence_bleu([ref_tok], hyp_tok, smoothing_function=VisualizationManager._smoothie)\n",
        "\n",
        "    @staticmethod\n",
        "    def _rouge_score(ref: str, hyp: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        scores = VisualizationManager._rouge_scorer.score(ref.lower(), hyp.lower())\n",
        "        return {'rouge-1': scores['rouge1'].fmeasure,\n",
        "                'rouge-l': scores['rougeL'].fmeasure}\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1) Courbes d’entraînement\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str = \"outputs/final_model\"):\n",
        "        try:\n",
        "            log_file = os.path.join(log_dir, \"trainer_state.json\")\n",
        "            if not os.path.exists(log_file):\n",
        "                st.warning(\"📄 Aucun log d'entraînement trouvé.\")\n",
        "                return\n",
        "\n",
        "            with open(log_file, 'r', encoding='utf-8') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"📉 Aucune donnée d'historique trouvée.\")\n",
        "                return\n",
        "\n",
        "            epochs, train_loss, eval_loss, eval_acc, eval_f1 = [], [], [], [], []\n",
        "\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_acc.append(entry.get('eval_accuracy', 0))\n",
        "                    eval_f1.append(entry.get('eval_f1_weighted', 0))\n",
        "                elif 'train_loss' in entry:\n",
        "                    train_loss.append(entry.get('train_loss', 0))\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(\"📈 Évolution de l'entraînement\", fontsize=16)\n",
        "\n",
        "            if train_loss and eval_loss:\n",
        "                train_steps = np.linspace(0, max(epochs) if epochs else 1, len(train_loss))\n",
        "                axes[0, 0].plot(train_steps, train_loss, 'b-', label='Train Loss', alpha=0.7)\n",
        "                axes[0, 0].plot(epochs[:len(eval_loss)], eval_loss, 'r-o', label='Eval Loss', markersize=4)\n",
        "                axes[0, 0].set_title('Loss Evolution')\n",
        "                axes[0, 0].legend()\n",
        "                axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc:\n",
        "                axes[0, 1].plot(epochs[:len(eval_acc)], eval_acc, 'g-o', label='Accuracy', markersize=4)\n",
        "                axes[0, 1].set_title('Accuracy Evolution')\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_f1:\n",
        "                axes[1, 0].plot(epochs[:len(eval_f1)], eval_f1, 'm-o', label='F1-Weighted', markersize=4)\n",
        "                axes[1, 0].set_title('F1-Score Evolution')\n",
        "                axes[1, 0].legend()\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc and eval_f1:\n",
        "                final_metrics = ['Accuracy', 'F1-Score']\n",
        "                final_values = [eval_acc[-1], eval_f1[-1]]\n",
        "                bars = axes[1, 1].bar(final_metrics, final_values, color=['green', 'purple'], alpha=0.7)\n",
        "                axes[1, 1].set_title('Final Metrics')\n",
        "                axes[1, 1].set_ylim(0, 1)\n",
        "                for bar, value in zip(bars, final_values):\n",
        "                    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                                   f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage des courbes : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2) Matrice de confusion\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names: List[str]):\n",
        "        try:\n",
        "            predictions_output = trainer.predict(test_dataset)\n",
        "            predictions = predictions_output.predictions.argmax(axis=1)\n",
        "            true_labels = predictions_output.label_ids\n",
        "\n",
        "            cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax1)\n",
        "            ax1.set_title(\"Matrice de confusion\")\n",
        "            ax1.set_xlabel(\"Prédictions\")\n",
        "            ax1.set_ylabel(\"Vraies valeurs\")\n",
        "\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax2)\n",
        "            ax2.set_title(\"Matrice de confusion (normalisée)\")\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            report = classification_report(true_labels, predictions,\n",
        "                                         target_names=label_names,\n",
        "                                         output_dict=True, zero_division=0)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.subheader(\"📊 Rapport de classification\")\n",
        "            st.dataframe(report_df.round(3))\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage de la matrice de confusion : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3) Distribution des classes\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(labels, label_names: List[str] = None, title: str = \"Distribution des classes\"):\n",
        "        try:\n",
        "            if hasattr(labels, 'tolist'):\n",
        "                labels = labels.tolist()\n",
        "            labels = [int(x) for x in labels]\n",
        "            label_counts = Counter(labels)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            if label_names:\n",
        "                x_labels = [label_names[i] if i < len(label_names) else f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "            else:\n",
        "                x_labels = [f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "\n",
        "            bars = ax.bar(range(len(x_labels)), counts, color=plt.cm.Set3(np.linspace(0, 1, len(x_labels))))\n",
        "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "            ax.set_xlabel(\"Classes\")\n",
        "            ax.set_ylabel(\"Nombre d'échantillons\")\n",
        "            ax.set_xticks(range(len(x_labels)))\n",
        "            ax.set_xticklabels(x_labels, rotation=45 if max(map(len, x_labels)) > 10 else 0)\n",
        "\n",
        "            for bar, count in zip(bars, counts):\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
        "                       str(count), ha='center', va='bottom')\n",
        "            total = sum(counts)\n",
        "            ax.text(0.02, 0.98, f\"Total: {total}\\nClasses: {len(x_labels)}\",\n",
        "                   transform=ax.transAxes, va='top', ha='left',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.7))\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage de la distribution : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 4) Analyse des résultats Q&A\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_qa_results_analysis(qa_results: List[Dict], question: str):\n",
        "        if not qa_results:\n",
        "            st.info(\"Aucun résultat à analyser\")\n",
        "            return\n",
        "        try:\n",
        "            scores = [r['score'] for r in qa_results]\n",
        "            ranks = [r['rank'] for r in qa_results]\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(f\"Analyse des résultats pour: '{question[:50]}...'\", fontsize=14)\n",
        "\n",
        "            axes[0, 0].hist(scores, bins=min(10, len(scores)), alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            axes[0, 0].set_title(\"Distribution des scores de similarité\")\n",
        "            axes[0, 0].axvline(np.mean(scores), color='red', linestyle='--', label=f'Moyenne: {np.mean(scores):.3f}')\n",
        "            axes[0, 0].legend()\n",
        "\n",
        "            axes[0, 1].bar(ranks, scores, color='lightcoral', alpha=0.7)\n",
        "            axes[0, 1].set_title(\"Scores par rang\")\n",
        "            axes[0, 1].set_xlabel(\"Rang\")\n",
        "\n",
        "            text_lengths = [len(r['text']) for r in qa_results]\n",
        "            axes[1, 0].scatter(text_lengths, scores, alpha=0.6, color='green')\n",
        "            axes[1, 0].set_title(\"Score vs Longueur du texte\")\n",
        "            axes[1, 0].set_xlabel(\"Longueur du texte\")\n",
        "\n",
        "            top_scores = scores[:min(5, len(scores))]\n",
        "            top_ranks = ranks[:min(5, len(ranks))]\n",
        "            axes[1, 1].barh(range(len(top_scores)), top_scores, color='purple', alpha=0.7)\n",
        "            axes[1, 1].set_title(\"Top 5 des scores\")\n",
        "            axes[1, 1].set_yticks(range(len(top_scores)))\n",
        "            axes[1, 1].set_yticklabels([f\"Rang {r}\" for r in top_ranks])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            st.subheader(\"📈 Statistiques détaillées\")\n",
        "            stats_df = pd.DataFrame({\n",
        "                \"Métrique\": [\"Score moyen\", \"Score médian\", \"Score max\", \"Score min\", \"Écart-type\"],\n",
        "                \"Valeur\": [np.mean(scores), np.median(scores), np.max(scores), np.min(scores), np.std(scores)]\n",
        "            })\n",
        "            st.dataframe(stats_df.round(4))\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'analyse des résultats Q&A : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 5) Méthodes BLEU / ROUGE manquantes\n",
        "    # --------------------------------------------------\n",
        "    def calculate_bleu_score(self, reference: str, candidate: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        return self._bleu(reference, candidate)\n",
        "\n",
        "    def calculate_rouge_score(self, reference: str, candidate: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        return self._rouge_score(reference, candidate)\n",
        "\n",
        "    def visualize_bleu_rouge_scores(self, qa_results, references):\n",
        "        \"\"\"Visualisation BLEU & ROUGE pour chaque paire (ref, résultat).\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for ref, res in zip(references, qa_results):\n",
        "            bleus.append(self._bleu(ref, res['text']))\n",
        "            r1s.append(self._rouge_score(ref, res['text'])['rouge-1'])\n",
        "            rls.append(self._rouge_score(ref, res['text'])['rouge-l'])\n",
        "\n",
        "        x = list(range(1, len(bleus)+1))\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar([i-0.2 for i in x], bleus, 0.4, label='BLEU')\n",
        "        plt.bar([i+0.2 for i in x], r1s, 0.4, label='ROUGE-1')\n",
        "        plt.xlabel('Rang')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('BLEU & ROUGE vs références')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(plt.gcf())\n",
        "\n",
        "    def evaluate_qa_performance(self, qa_module, questions, references):\n",
        "        \"\"\"Évaluation complète Q-A avec scores BLEU/ROUGE.\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for q, ref in zip(questions, references):\n",
        "            res = qa_module.query_with_fallback(q, top_k=1)\n",
        "            if res:\n",
        "                cand = res[0]['text']\n",
        "                bleus.append(self._bleu(ref, cand))\n",
        "                r1s.append(self._rouge_score(ref, cand)['rouge-1'])\n",
        "                rls.append(self._rouge_score(ref, cand)['rouge-l'])\n",
        "\n",
        "        st.write(\"### 📊 Global Q-A metrics\")\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        col1.metric(\"Avg BLEU\", f\"{np.mean(bleus):.4f}\")\n",
        "        col2.metric(\"Avg ROUGE-1\", f\"{np.mean(r1s):.4f}\")\n",
        "        col3.metric(\"Avg ROUGE-L\", f\"{np.mean(rls):.4f}\")"
      ],
      "metadata": {
        "id": "oMZt-WwIMwez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f5d82a-e5f5-4580-b645-3ef54db0b5d5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting visualization_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. qa_modules.py"
      ],
      "metadata": {
        "id": "1HSXVYNkNc-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_modules.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class QAModule:\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        self.model_name = model_name\n",
        "        self.encoder = None\n",
        "        self.corpus_texts = []\n",
        "        self.labels = []\n",
        "        self.corpus_embeddings = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "        try:\n",
        "            print(f\"Chargement du modèle de similarité: {model_name}\")\n",
        "            self.encoder = SentenceTransformer(model_name)\n",
        "            print(\"✅ Modèle de similarité chargé avec succès\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur lors du chargement du modèle de similarité: {e}\")\n",
        "            self.encoder = None\n",
        "\n",
        "    def fit(self, dataset: List[Dict[str, Any]]):\n",
        "        \"\"\"Entraînement du module Q&A avec le dataset\"\"\"\n",
        "        try:\n",
        "            if self.encoder is None:\n",
        "                print(\"❌ Encodeur non disponible pour le module Q&A\")\n",
        "                return False\n",
        "\n",
        "            print(\"Initialisation du module Q&A...\")\n",
        "\n",
        "            # Extraction des textes et labels\n",
        "            self.corpus_texts = []\n",
        "            self.labels = []\n",
        "\n",
        "            for item in dataset:\n",
        "                if 'text' in item and item['text']:\n",
        "                    text = str(item['text']).strip()\n",
        "                    if len(text) > 10:  # Filtrer les textes trop courts\n",
        "                        self.corpus_texts.append(text)\n",
        "                        self.labels.append(item.get('label_id', 0))\n",
        "\n",
        "            if not self.corpus_texts:\n",
        "                print(\"❌ Aucun texte valide trouvé dans le dataset\")\n",
        "                return False\n",
        "\n",
        "            print(f\"Génération des embeddings pour {len(self.corpus_texts)} textes...\")\n",
        "\n",
        "            # Génération des embeddings\n",
        "            self.corpus_embeddings = self.encoder.encode(\n",
        "                self.corpus_texts,\n",
        "                convert_to_tensor=False,\n",
        "                show_progress_bar=True,\n",
        "                batch_size=32\n",
        "            )\n",
        "\n",
        "            self.is_fitted = True\n",
        "            print(f\"✅ Module Q&A initialisé avec {len(self.corpus_texts)} textes\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur lors de l'initialisation du module Q&A: {e}\")\n",
        "            return False\n",
        "\n",
        "    def query(self, question: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche sémantique\"\"\"\n",
        "        try:\n",
        "            if not self.is_fitted or self.encoder is None:\n",
        "                print(\"Module Q&A non initialisé\")\n",
        "                return []\n",
        "\n",
        "            if not question.strip():\n",
        "                return []\n",
        "\n",
        "            # Génération de l'embedding de la question\n",
        "            question_embedding = self.encoder.encode([question.strip()], convert_to_tensor=False)\n",
        "\n",
        "            # Calcul des similarités\n",
        "            similarities = cosine_similarity(question_embedding, self.corpus_embeddings)[0]\n",
        "\n",
        "            # Tri des résultats\n",
        "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "            results = []\n",
        "            for rank, idx in enumerate(top_indices):\n",
        "                if similarities[idx] > 0:  # Seuil minimal de similarité\n",
        "                    results.append({\n",
        "                        \"text\": self.corpus_texts[idx],\n",
        "                        \"label_id\": int(self.labels[idx]),\n",
        "                        \"score\": float(similarities[idx]),\n",
        "                        \"rank\": rank + 1\n",
        "                    })\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de la recherche sémantique: {e}\")\n",
        "            return []\n",
        "\n",
        "    def keyword_search(self, question: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par mots-clés (fallback)\"\"\"\n",
        "        try:\n",
        "            if not self.corpus_texts:\n",
        "                return []\n",
        "\n",
        "            if not question.strip():\n",
        "                return []\n",
        "\n",
        "            # Extraction des mots-clés de la question\n",
        "            question_words = set(re.findall(r'\\b\\w+\\b', question.lower()))\n",
        "\n",
        "            if not question_words:\n",
        "                return []\n",
        "\n",
        "            scored_results = []\n",
        "\n",
        "            for i, text in enumerate(self.corpus_texts):\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "\n",
        "                # Score basé sur l'intersection des mots\n",
        "                common_words = question_words & text_words\n",
        "                if common_words:\n",
        "                    score = len(common_words) / max(len(question_words | text_words), 1)\n",
        "\n",
        "                    scored_results.append({\n",
        "                        \"text\": text,\n",
        "                        \"label_id\": int(self.labels[i]),\n",
        "                        \"score\": score,\n",
        "                        \"rank\": 0,\n",
        "                        \"common_words\": list(common_words)\n",
        "                    })\n",
        "\n",
        "            # Tri par score décroissant\n",
        "            scored_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "            # Attribution des rangs et limitation des résultats\n",
        "            for rank, result in enumerate(scored_results[:top_k]):\n",
        "                result['rank'] = rank + 1\n",
        "\n",
        "            return scored_results[:top_k]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de la recherche par mots-clés: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_with_fallback(self, question: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche avec fallback automatique\"\"\"\n",
        "        # Tentative de recherche sémantique\n",
        "        results = self.query(question, top_k)\n",
        "\n",
        "        # Si pas de résultats satisfaisants, utiliser la recherche par mots-clés\n",
        "        if not results or not any(r['score'] > 0.1 for r in results):\n",
        "            print(\"Fallback vers la recherche par mots-clés\")\n",
        "            results = self.keyword_search(question, top_k)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_debug_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Informations de debug\"\"\"\n",
        "        return {\n",
        "            \"is_fitted\": self.is_fitted,\n",
        "            \"encoder_available\": self.encoder is not None,\n",
        "            \"corpus_size\": len(self.corpus_texts),\n",
        "            \"embeddings_generated\": self.corpus_embeddings is not None,\n",
        "            \"model_name\": self.model_name\n",
        "        }"
      ],
      "metadata": {
        "id": "CdRC_3s7Ncy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008d25f1-50dd-4744-e692-825ae8d577aa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting qa_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4️⃣ Module Knowledge Base - knowledge_modules.py"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le réchauffement climatique est principalement causé par les émissions de gaz à effet de serre d'origine humaine.\",\n",
        "            \"Les énergies renouvelables comme le solaire et l'éolien sont essentielles pour décarboner notre économie.\",\n",
        "            \"La déforestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports représente environ 24% des émissions mondiales de gaz à effet de serre.\",\n",
        "            \"L'amélioration de l'efficacité énergétique des bâtiments peut réduire jusqu'à 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et régénératrice peut séquestrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les océans absorbent 25% du CO2 atmosphérique mais s'acidifient, menaçant les écosystèmes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises à réduire leurs émissions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'atténuation des émissions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralité carbone.\"\n",
        "        ]\n",
        "        print(\"✅ Base de connaissances initialisée avec recherche par mots-clés\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarité textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarité basé sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score décroissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"✅ Nouvelle connaissance ajoutée: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fc48bb-d16e-404f-aad9-a25f887961a4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5️⃣ Module Streamlit - streamlit_app.py"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "# streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import sys\n",
        "from datasets import Dataset\n",
        "from contextlib import contextmanager\n",
        "\n",
        "st.set_page_config(page_title=\"🌍 Climate Analyzer\", page_icon=\"🌍\", layout=\"wide\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Barre de progression persistante\n",
        "# ---------------------------------------------------------\n",
        "@contextmanager\n",
        "def st_progress(title=\"Progress\", max_value=100):\n",
        "    bar = st.progress(0, text=title)\n",
        "    try:\n",
        "        yield bar\n",
        "    finally:\n",
        "        bar.empty()\n",
        "\n",
        "# Persistance session_state - Initialisation correcte\n",
        "if \"trainer\" not in st.session_state:\n",
        "    st.session_state.trainer = None\n",
        "if \"label_names\" not in st.session_state:\n",
        "    st.session_state.label_names = None\n",
        "if \"test_ds\" not in st.session_state:\n",
        "    st.session_state.test_ds = None\n",
        "if \"training\" not in st.session_state:\n",
        "    st.session_state.training = False\n",
        "if \"raw_train_data\" not in st.session_state:\n",
        "    st.session_state.raw_train_data = None\n",
        "\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from qa_modules import QAModule\n",
        "from visualization_modules import VisualizationManager\n",
        "\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "        self.qa_module = QAModule()\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.load_saved_model()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Chargement automatique du modèle si déjà présent\n",
        "    # ------------------------------------------------------------------\n",
        "    def load_saved_model(self):\n",
        "        if st.session_state.trainer is None and os.path.exists(\"outputs/final_model/config.json\"):\n",
        "            try:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                num_labels = len(self.data_processor.label_mapping) or 2\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                trainer = self.model_manager.setup_trainer(None, None)\n",
        "                trainer.model = trainer.model.from_pretrained(\"outputs/final_model\")\n",
        "                st.session_state.trainer = trainer\n",
        "                st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "                st.success(\"✅ Modèle chargé depuis le disque.\")\n",
        "            except Exception as e:\n",
        "                st.warning(f\"⚠️ Chargement impossible : {e}\")\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # MENU PRINCIPAL\n",
        "    # ------------------------------------------------------------------\n",
        "    def run(self):\n",
        "        st.title(\"🌍 Climate Sentiment Analyzer\")\n",
        "        mode = st.sidebar.selectbox(\n",
        "            \"Mode\",\n",
        "            [\"🚀 Pipeline Complet\", \"❓ Q&A\", \"📈 Visualisations\"]\n",
        "        )\n",
        "\n",
        "        if mode == \"🚀 Pipeline Complet\":\n",
        "            self.run_complete_pipeline()\n",
        "        elif mode == \"❓ Q&A\":\n",
        "            self.run_qa_interface()\n",
        "        elif mode == \"📈 Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 1) PIPELINE COMPLET\n",
        "    # ------------------------------------------------------------------\n",
        "    def run_complete_pipeline(self):\n",
        "        uploaded_file = st.file_uploader(\"Téléchargez votre CSV\", type=[\"csv\"])\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.dataframe(df.head())\n",
        "\n",
        "            # SLIDERS\n",
        "            sample_size = st.slider(\"Taille échantillon\", 1000, 10000, value=4000)\n",
        "            self.config.epochs = st.slider(\"Epochs\", 1, 5, value=3)\n",
        "\n",
        "            # Vérification de l'état d'entraînement\n",
        "            is_training = st.session_state.get(\"training\", False)\n",
        "            if is_training is None:\n",
        "                is_training = False\n",
        "                st.session_state.training = False\n",
        "\n",
        "            if st.button(\n",
        "                \"🚀 Lancer l'entraînement\",\n",
        "                type=\"primary\",\n",
        "                disabled=bool(is_training)\n",
        "            ):\n",
        "                st.session_state.training = True\n",
        "                try:\n",
        "                    self.train_pipeline(df, sample_size)\n",
        "                finally:\n",
        "                    st.session_state.training = False\n",
        "\n",
        "    def train_pipeline(self, df: pd.DataFrame, sample_size: int):\n",
        "        try:\n",
        "            # 1/4 — Analyse des données\n",
        "            with st_progress(\"1/4  Analyse des données …\") as bar:\n",
        "                train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "                bar.progress(25)\n",
        "\n",
        "            # CORRECTION: Sauvegarder les données brutes avant tokenisation\n",
        "            raw_train_data = []\n",
        "            for item in train_ds:\n",
        "                raw_train_data.append({\n",
        "                    \"text\": item[\"text\"],\n",
        "                    \"label_id\": item[\"label_id\"]\n",
        "                })\n",
        "            st.session_state.raw_train_data = raw_train_data\n",
        "\n",
        "            # 2/4 — Tokenizer\n",
        "            with st_progress(\"2/4  Chargement du tokenizer …\") as bar:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                bar.progress(50)\n",
        "\n",
        "            # 3/4 — Modèle\n",
        "            with st_progress(\"3/4  Initialisation du modèle …\") as bar:\n",
        "                num_labels = len(self.data_processor.label_mapping)\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                bar.progress(75)\n",
        "\n",
        "            # 4/4 — Tokenisation + format torch\n",
        "            def prep(ds):\n",
        "                with st_progress(\"4/4  Tokenisation …\") as bar:\n",
        "                    ds = ds.map(\n",
        "                        self.model_manager.tokenize_function,\n",
        "                        batched=True,\n",
        "                        desc=\"Tokenisation\"\n",
        "                    )\n",
        "                    ds = ds.rename_column(\"label_id\", \"labels\")\n",
        "                    # Garder seulement les colonnes nécessaires\n",
        "                    keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
        "                    for col in list(ds.column_names):\n",
        "                        if col not in keep:\n",
        "                            ds = ds.remove_columns(col)\n",
        "                    ds.set_format(type=\"torch\", columns=list(keep))\n",
        "                    bar.progress(100)\n",
        "                    return ds\n",
        "\n",
        "            train_ds_processed, val_ds_processed, test_ds_processed = map(prep, (train_ds, val_ds, test_ds))\n",
        "\n",
        "            trainer = self.model_manager.setup_trainer(train_ds_processed, val_ds_processed)\n",
        "\n",
        "            with st.spinner(\"Entraînement en cours … (cela peut prendre quelques minutes)\"):\n",
        "                trainer.train()\n",
        "\n",
        "            trainer.save_model(\"outputs/final_model\")\n",
        "            trainer.state.save_to_json(\"outputs/final_model/trainer_state.json\")\n",
        "\n",
        "            # CORRECTION: Utiliser les données brutes sauvegardées pour Q&A\n",
        "            if st.session_state.raw_train_data:\n",
        "                self.qa_module.fit(st.session_state.raw_train_data)\n",
        "\n",
        "            st.session_state.trainer = trainer\n",
        "            st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "            st.session_state.test_ds = test_ds_processed\n",
        "            st.success(\"🎉 Entraînement terminé !\")\n",
        "            st.balloons()\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur : {e}\")\n",
        "            import traceback\n",
        "            st.error(f\"Détail: {traceback.format_exc()}\")\n",
        "            st.session_state.training = False\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 2) INTERFACE Q&A\n",
        "    # ------------------------------------------------------------------\n",
        "    def run_qa_interface(self):\n",
        "        st.header(\"❓ Interface Q&A\")\n",
        "        if st.session_state.trainer is None:\n",
        "            st.warning(\"⚠️ Aucun modèle entraîné.\")\n",
        "            return\n",
        "\n",
        "        question = st.text_input(\"Posez votre question :\")\n",
        "        mode = st.selectbox(\"Mode de recherche\", [\"Auto\", \"Sémantique\", \"Mots-clés\"])\n",
        "        top_k = st.slider(\"Résultats\", 1, 10, value=5)\n",
        "\n",
        "        if question:\n",
        "            try:\n",
        "                if mode == \"Sémantique\":\n",
        "                    res = self.qa_module.query(question, top_k)\n",
        "                elif mode == \"Mots-clés\":\n",
        "                    res = self.qa_module.keyword_search(question, top_k)\n",
        "                else:\n",
        "                    res = self.qa_module.query_with_fallback(question, top_k)\n",
        "\n",
        "                if res:\n",
        "                    for r in res:\n",
        "                        with st.expander(f\"Score: {r['score']:.3f}\"):\n",
        "                            st.write(r[\"text\"])\n",
        "                else:\n",
        "                    st.info(\"Aucun résultat trouvé.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"❌ Erreur lors de la recherche : {e}\")\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 3) VISUALISATIONS\n",
        "    # ------------------------------------------------------------------\n",
        "    def run_visualizations(self):\n",
        "        st.header(\"📈 Visualisations\")\n",
        "        if st.session_state.trainer is None:\n",
        "            st.warning(\"⚠️ Aucun modèle entraîné.\")\n",
        "            return\n",
        "\n",
        "        viz = st.selectbox(\n",
        "            \"Choisir\",\n",
        "            [\"Distribution des classes\", \"Matrice de confusion\", \"Courbes d'entraînement\",\n",
        "             \"📊 Métriques BLEU/ROUGE\", \"🔍 Évaluation Q&A\"]\n",
        "        )\n",
        "        test_ds = st.session_state.get(\"test_ds\")\n",
        "        label_names = st.session_state.get(\"label_names\")\n",
        "\n",
        "        try:\n",
        "            if viz == \"Distribution des classes\" and test_ds:\n",
        "                self.visualizer.plot_class_distribution(test_ds[\"labels\"], label_names)\n",
        "            elif viz == \"Matrice de confusion\" and test_ds:\n",
        "                self.visualizer.show_confusion_matrix(st.session_state.trainer, test_ds, label_names)\n",
        "            elif viz == \"Courbes d'entraînement\":\n",
        "                self.visualizer.plot_training_curves(\"outputs/final_model\")\n",
        "            elif viz == \"📊 Métriques BLEU/ROUGE\":\n",
        "                self.run_bleu_rouge_analysis()\n",
        "            elif viz == \"🔍 Évaluation Q&A\":\n",
        "                self.run_qa_evaluation()\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de la visualisation : {e}\")\n",
        "            import traceback\n",
        "            st.error(f\"Détail: {traceback.format_exc()}\")\n",
        "\n",
        "    def run_bleu_rouge_analysis(self):\n",
        "        \"\"\"Interface pour l'analyse BLEU/ROUGE\"\"\"\n",
        "        st.subheader(\"📊 Analyse des métriques BLEU et ROUGE\")\n",
        "\n",
        "        # Section 1: Test avec textes personnalisés\n",
        "        st.write(\"### 🧪 Test avec vos propres textes\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            reference_text = st.text_area(\n",
        "                \"Texte de référence:\",\n",
        "                \"Le réchauffement climatique est un phénomène global causé par les activités humaines.\",\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "        with col2:\n",
        "            candidate_text = st.text_area(\n",
        "                \"Texte candidat:\",\n",
        "                \"Le changement climatique est un problème mondial dû aux actions humaines.\",\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "        if st.button(\"Calculer les scores\"):\n",
        "            bleu_score = self.visualizer.calculate_bleu_score(reference_text, candidate_text)\n",
        "            rouge_scores = self.visualizer.calculate_rouge_score(reference_text, candidate_text)\n",
        "\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            with col1:\n",
        "                st.metric(\"BLEU Score\", f\"{bleu_score:.4f}\")\n",
        "            with col2:\n",
        "                st.metric(\"ROUGE-1\", f\"{rouge_scores['rouge-1']:.4f}\")\n",
        "            with col3:\n",
        "                st.metric(\"ROUGE-L\", f\"{rouge_scores['rouge-l']:.4f}\")\n",
        "\n",
        "        # Section 2: Analyse des résultats Q&A\n",
        "        st.write(\"### 🔍 Analyse des résultats de recherche\")\n",
        "\n",
        "        if st.session_state.raw_train_data:\n",
        "            test_question = st.text_input(\n",
        "                \"Question de test:\",\n",
        "                \"Quelles sont les causes du réchauffement climatique ?\"\n",
        "            )\n",
        "\n",
        "            if test_question and st.button(\"Analyser la question\"):\n",
        "                # Obtenir les résultats de la recherche\n",
        "                results = self.qa_module.query_with_fallback(test_question, top_k=5)\n",
        "\n",
        "                if results:\n",
        "                    # Prendre les premiers textes comme références\n",
        "                    sample_references = [item['text'] for item in st.session_state.raw_train_data[:len(results)]]\n",
        "\n",
        "                    # Visualiser les métriques\n",
        "                    self.visualizer.visualize_bleu_rouge_scores(results, sample_references)\n",
        "\n",
        "                    # Afficher les résultats détaillés\n",
        "                    st.subheader(\"📋 Résultats détaillés\")\n",
        "                    for i, result in enumerate(results):\n",
        "                        with st.expander(f\"Résultat {i+1} - Score: {result['score']:.3f}\"):\n",
        "                            st.write(\"**Texte trouvé:**\")\n",
        "                            st.write(result['text'])\n",
        "\n",
        "                            if i < len(sample_references):\n",
        "                                bleu = self.visualizer.calculate_bleu_score(sample_references[i], result['text'])\n",
        "                                rouge = self.visualizer.calculate_rouge_score(sample_references[i], result['text'])\n",
        "\n",
        "                                col1, col2, col3 = st.columns(3)\n",
        "                                with col1:\n",
        "                                    st.metric(\"BLEU\", f\"{bleu:.4f}\")\n",
        "                                with col2:\n",
        "                                    st.metric(\"ROUGE-1\", f\"{rouge['rouge-1']:.4f}\")\n",
        "                                with col3:\n",
        "                                    st.metric(\"ROUGE-L\", f\"{rouge['rouge-l']:.4f}\")\n",
        "                else:\n",
        "                    st.info(\"Aucun résultat trouvé pour cette question.\")\n",
        "        else:\n",
        "            st.info(\"Entraînez d'abord un modèle pour utiliser cette fonctionnalité.\")\n",
        "\n",
        "    def run_qa_evaluation(self):\n",
        "        \"\"\"Interface pour l'évaluation complète du système Q&A\"\"\"\n",
        "        st.subheader(\"🔍 Évaluation complète du système Q&A\")\n",
        "\n",
        "        if not st.session_state.raw_train_data:\n",
        "            st.info(\"Entraînez d'abord un modèle pour utiliser cette fonctionnalité.\")\n",
        "            return\n",
        "\n",
        "        # Questions de test prédéfinies pour le climat\n",
        "        default_questions = [\n",
        "            \"Quelles sont les principales causes du réchauffement climatique ?\",\n",
        "            \"Comment les énergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la déforestation sur le climat ?\",\n",
        "            \"Comment réduire les émissions de gaz à effet de serre ?\",\n",
        "            \"Que peut-on faire pour s'adapter au changement climatique ?\"\n",
        "        ]\n",
        "\n",
        "        # Interface pour personnaliser les questions\n",
        "        st.write(\"### 📝 Questions de test\")\n",
        "\n",
        "        use_default = st.checkbox(\"Utiliser les questions prédéfinies\", value=True)\n",
        "\n",
        "        if use_default:\n",
        "            test_questions = default_questions\n",
        "            st.write(\"Questions utilisées:\")\n",
        "            for i, q in enumerate(test_questions, 1):\n",
        "                st.write(f\"{i}. {q}\")\n",
        "        else:\n",
        "            test_questions = []\n",
        "            num_questions = st.number_input(\"Nombre de questions\", min_value=1, max_value=10, value=3)\n",
        "\n",
        "            for i in range(num_questions):\n",
        "                question = st.text_input(f\"Question {i+1}:\", key=f\"q_{i}\")\n",
        "                if question:\n",
        "                    test_questions.append(question)\n",
        "\n",
        "        # Évaluation\n",
        "        if test_questions and st.button(\"🚀 Lancer l'évaluation\"):\n",
        "\n",
        "            # Générer des références basées sur les données d'entraînement\n",
        "            reference_answers = []\n",
        "            for question in test_questions:\n",
        "                # Trouver les textes les plus pertinents comme références\n",
        "                results = self.qa_module.query_with_fallback(question, top_k=1)\n",
        "                if results:\n",
        "                    reference_answers.append(results[0]['text'])\n",
        "                else:\n",
        "                    # Utiliser un texte aléatoire des données d'entraînement\n",
        "                    import random\n",
        "                    reference_answers.append(random.choice(st.session_state.raw_train_data)['text'])\n",
        "\n",
        "            # Lancer l'évaluation\n",
        "            self.visualizer.evaluate_qa_performance(\n",
        "                self.qa_module,\n",
        "                test_questions,\n",
        "                reference_answers\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = ClimateAnalyzerApp()\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ec707c-64b4-442f-ef29-1fe6e5575c0b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6️⃣ Script d'Installation - setup_pipeline.py"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\",\n",
        "        \"rouge-score\",\n",
        "        \"nltk\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "            print(f\"✅ {package} installé\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠️ Erreur avec {package}: {e}\")\n",
        "\n",
        "    # Téléchargement des ressources NLTK\n",
        "    import nltk\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"✅ Ressources NLTK prêtes\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73091239-0099-4f64-cb8b-9de9b6592015"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f5b555-b8b9-44fe-b8cf-70cd5e995672"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "✅ transformers>=4.36.0 installé\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "✅ datasets>=2.16.0 installé\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "✅ torch>=2.1.0 installé\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "✅ peft>=0.7.0 installé\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "✅ sentence-transformers>=2.2.0 installé\n",
            "Requirement already satisfied: faiss-cpu>=1.7.0 in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "✅ faiss-cpu>=1.7.0 installé\n",
            "Requirement already satisfied: streamlit>=1.29.0 in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "✅ streamlit>=1.29.0 installé\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "✅ plotly>=5.17.0 installé\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "✅ scikit-learn>=1.3.0 installé\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "✅ matplotlib>=3.7.0 installé\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "✅ seaborn>=0.12.0 installé\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "✅ pandas>=1.5.0 installé\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "✅ numpy>=1.24.0 installé\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "✅ rouge-score installé\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "✅ nltk installé\n",
            "✅ Ressources NLTK prêtes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "-MEnnZWsOuL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d3a853-9d93-42d3-e14b-2b7df6cab7f3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "Y3wskx_fZLEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cba06c6-be39-4174-9dc0-1f686dcd01f9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Lancement Streamlit + ngrok (version corrigée)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1️⃣ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2️⃣ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3️⃣ Attendre et créer le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🚀 Interface Streamlit disponible à :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "IFyKJzjISjWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3f74e89-13da-4381-b477-2ed66f8e71ee"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "🚀 Interface Streamlit disponible à :\n",
            "NgrokTunnel: \"https://ca05fe247825.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}