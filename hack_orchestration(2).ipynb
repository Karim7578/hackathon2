{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Module Core - core_modules.py"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    \"\"\"Structure pour les r√©sultats de pr√©diction\"\"\"\n",
        "    text: str\n",
        "    predicted_label: str\n",
        "    confidence: float\n",
        "    all_scores: Dict[str, float]\n",
        "    context: Optional[List[str]] = None\n",
        "    processing_time: float = 0.0\n",
        "\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralis√©e\"\"\"\n",
        "    def __init__(self):\n",
        "        self.model_name = \"distilbert-base-uncased\"\n",
        "        self.max_length = 256\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 2e-4\n",
        "        self.epochs = 3\n",
        "        self.lora_r = 16\n",
        "        self.lora_alpha = 32\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            'model_name': self.model_name,\n",
        "            'max_length': self.max_length,\n",
        "            'batch_size': self.batch_size,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'epochs': self.epochs,\n",
        "            'device': str(self.device),\n",
        "            'lora_config': {'r': self.lora_r, 'alpha': self.lora_alpha}\n",
        "        }"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c7aa45-eabc-4632-fbbb-8e20d4d56006"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Module Data Processing - data_modules.py"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "\n",
        "# data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Gestion centralis√©e du traitement des donn√©es avec gestion robuste des erreurs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        \"\"\"D√©tection automatique des colonnes texte et label avec validation\"\"\"\n",
        "        print(f\"üîç D√©tection des colonnes sur {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
        "        print(f\"üìã Colonnes disponibles: {list(df.columns)}\")\n",
        "\n",
        "        text_keywords = ['self_text', 'text', 'content', 'message', 'comment', 'body', 'description']\n",
        "        label_keywords = ['comment_sentiment', 'sentiment', 'label', 'category', 'class', 'target']\n",
        "\n",
        "        # Recherche intelligente\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        # Recherche par mots-cl√©s\n",
        "        for col in df.columns:\n",
        "            col_lower = str(col).lower()\n",
        "\n",
        "            # Recherche colonne texte\n",
        "            if not text_col:\n",
        "                for keyword in text_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        text_col = col\n",
        "                        break\n",
        "\n",
        "            # Recherche colonne label\n",
        "            if not label_col:\n",
        "                for keyword in label_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        label_col = col\n",
        "                        break\n",
        "\n",
        "        # Fallback intelligent pour la colonne texte\n",
        "        if not text_col:\n",
        "            string_cols = []\n",
        "            for col in df.columns:\n",
        "                try:\n",
        "                    # V√©rifier si la colonne contient principalement du texte\n",
        "                    sample = df[col].dropna().head(100)\n",
        "                    if len(sample) > 0:\n",
        "                        # Convertir en string et calculer la longueur moyenne\n",
        "                        sample_str = sample.astype(str)\n",
        "                        avg_length = sample_str.str.len().mean()\n",
        "                        if avg_length > 10:  # Textes probablement plus longs que 10 caract√®res\n",
        "                            string_cols.append((col, avg_length))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if string_cols:\n",
        "                # Prendre la colonne avec le texte le plus long en moyenne\n",
        "                text_col = max(string_cols, key=lambda x: x[1])[0]\n",
        "            else:\n",
        "                # Last resort: premi√®re colonne object\n",
        "                object_cols = df.select_dtypes(include=['object']).columns\n",
        "                if len(object_cols) > 0:\n",
        "                    text_col = object_cols[0]\n",
        "\n",
        "        # Fallback pour la colonne label\n",
        "        if not label_col:\n",
        "            # Chercher une colonne avec peu de valeurs uniques (potentiel label)\n",
        "            for col in df.columns:\n",
        "                if col != text_col:\n",
        "                    try:\n",
        "                        unique_count = df[col].nunique()\n",
        "                        total_count = len(df[col].dropna())\n",
        "                        if total_count > 0 and unique_count < min(20, total_count * 0.1):\n",
        "                            label_col = col\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Si toujours pas trouv√©, prendre la derni√®re colonne\n",
        "            if not label_col:\n",
        "                label_col = df.columns[-1]\n",
        "\n",
        "        print(f\"‚úÖ Colonnes d√©tect√©es: Text='{text_col}', Label='{label_col}'\")\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text_column(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Nettoyage robuste d'une colonne texte\"\"\"\n",
        "        try:\n",
        "            # Convertir en string d'abord\n",
        "            cleaned = series.astype(str)\n",
        "\n",
        "            # Remplacer les valeurs probl√©matiques\n",
        "            cleaned = cleaned.replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "            # Supprimer les espaces\n",
        "            cleaned = cleaned.str.strip()\n",
        "\n",
        "            # Remplacer les cha√Ænes vides par NaN\n",
        "            cleaned = cleaned.replace('', pd.NA)\n",
        "\n",
        "            return cleaned\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur nettoyage texte: {e}\")\n",
        "            # Fallback: conversion simple\n",
        "            return series.astype(str)\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        \"\"\"Pr√©paration des datasets avec validation robuste\"\"\"\n",
        "\n",
        "        print(f\"üìä Pr√©paration des datasets - Taille originale: {df.shape}\")\n",
        "\n",
        "        # D√©tection des colonnes\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "\n",
        "        if not self.text_col or not self.label_col:\n",
        "            raise ValueError(f\"‚ùå Impossible de d√©tecter les colonnes: text='{self.text_col}', label='{self.label_col}'\")\n",
        "\n",
        "        # Extraction et copie des colonnes n√©cessaires\n",
        "        try:\n",
        "            df_work = df[[self.text_col, self.label_col]].copy()\n",
        "        except KeyError as e:\n",
        "            print(f\"‚ùå Colonnes manquantes: {e}\")\n",
        "            print(f\"Colonnes disponibles: {list(df.columns)}\")\n",
        "            raise\n",
        "\n",
        "        # Renommer les colonnes\n",
        "        df_work.columns = ['text', 'label']\n",
        "\n",
        "        print(f\"üìã Avant nettoyage: {len(df_work)} lignes\")\n",
        "\n",
        "        # Nettoyage robuste des donn√©es\n",
        "        # 1. Nettoyage de la colonne texte\n",
        "        df_work['text'] = self.clean_text_column(df_work['text'])\n",
        "\n",
        "        # 2. Nettoyage de la colonne label\n",
        "        df_work['label'] = df_work['label'].astype(str).str.strip()\n",
        "        df_work['label'] = df_work['label'].replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "        # 3. Suppression des lignes avec des valeurs manquantes\n",
        "        initial_size = len(df_work)\n",
        "        df_work = df_work.dropna()\n",
        "        print(f\"üßπ Apr√®s suppression des NaN: {len(df_work)} lignes (supprim√©: {initial_size - len(df_work)})\")\n",
        "\n",
        "        # 4. Filtrage des textes trop courts (de mani√®re s√©curis√©e)\n",
        "        try:\n",
        "            # V√©rifier que nous avons bien des strings\n",
        "            df_work['text'] = df_work['text'].astype(str)\n",
        "\n",
        "            # Filtrer les textes trop courts\n",
        "            mask = df_work['text'].str.len() > 5\n",
        "            df_work = df_work[mask]\n",
        "            print(f\"üìù Apr√®s filtrage textes courts: {len(df_work)} lignes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du filtrage des textes: {e}\")\n",
        "            # Continuer sans filtrage si erreur\n",
        "\n",
        "        # V√©rification finale\n",
        "        if len(df_work) == 0:\n",
        "            raise ValueError(\"‚ùå Aucune donn√©e valide apr√®s nettoyage!\")\n",
        "\n",
        "        # 5. √âchantillonnage si n√©cessaire\n",
        "        if len(df_work) > sample_size:\n",
        "            df_work = df_work.sample(n=sample_size, random_state=42)\n",
        "            print(f\"üéØ √âchantillonnage √† {sample_size} lignes\")\n",
        "\n",
        "        # 6. Mapping des labels\n",
        "        unique_labels = sorted(df_work['label'].unique())\n",
        "        print(f\"üè∑Ô∏è Labels uniques trouv√©s: {unique_labels}\")\n",
        "\n",
        "        self.label_mapping = {str(label): idx for idx, label in enumerate(unique_labels)}\n",
        "        df_work['label_id'] = df_work['label'].astype(str).map(self.label_mapping)\n",
        "\n",
        "        # V√©rification du mapping\n",
        "        if df_work['label_id'].isna().any():\n",
        "            print(\"‚ö†Ô∏è Probl√®me de mapping des labels d√©tect√©\")\n",
        "            print(f\"Labels non mapp√©s: {df_work[df_work['label_id'].isna()]['label'].unique()}\")\n",
        "\n",
        "        print(f\"üìä Mapping des labels: {self.label_mapping}\")\n",
        "\n",
        "        # 7. Splits stratifi√©s\n",
        "        try:\n",
        "            # V√©rifier si on peut faire une stratification\n",
        "            if len(unique_labels) > 1 and all(df_work['label_id'].value_counts() >= 2):\n",
        "                stratify_col = df_work['label_id']\n",
        "                print(\"‚úÖ Stratification activ√©e\")\n",
        "            else:\n",
        "                stratify_col = None\n",
        "                print(\"‚ö†Ô∏è Pas de stratification (pas assez d'exemples par classe)\")\n",
        "\n",
        "            # Premier split: train vs (val + test)\n",
        "            train_df, temp_df = train_test_split(\n",
        "                df_work,\n",
        "                test_size=0.4,\n",
        "                random_state=42,\n",
        "                stratify=stratify_col if stratify_col is not None else None\n",
        "            )\n",
        "\n",
        "            # Deuxi√®me split: val vs test\n",
        "            if stratify_col is not None:\n",
        "                temp_stratify = temp_df['label_id']\n",
        "            else:\n",
        "                temp_stratify = None\n",
        "\n",
        "            val_df, test_df = train_test_split(\n",
        "                temp_df,\n",
        "                test_size=0.5,\n",
        "                random_state=42,\n",
        "                stratify=temp_stratify if temp_stratify is not None else None\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du split: {e}\")\n",
        "            # Fallback: split simple\n",
        "            train_size = int(0.6 * len(df_work))\n",
        "            val_size = int(0.2 * len(df_work))\n",
        "\n",
        "            train_df = df_work[:train_size]\n",
        "            val_df = df_work[train_size:train_size+val_size]\n",
        "            test_df = df_work[train_size+val_size:]\n",
        "\n",
        "        print(f\"üìä Splits finaux: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "        # 8. Conversion en Dataset\n",
        "        try:\n",
        "            train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']].reset_index(drop=True))\n",
        "\n",
        "            print(\"‚úÖ Datasets cr√©√©s avec succ√®s\")\n",
        "\n",
        "            return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation des datasets: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques du processeur de donn√©es\"\"\"\n",
        "        return {\n",
        "            \"text_column\": self.text_col,\n",
        "            \"label_column\": self.label_col,\n",
        "            \"label_mapping\": self.label_mapping,\n",
        "            \"num_labels\": len(self.label_mapping)\n",
        "        }\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Validation d'un DataFrame\"\"\"\n",
        "        try:\n",
        "            if df is None or df.empty:\n",
        "                print(\"‚ùå DataFrame vide ou None\")\n",
        "                return False\n",
        "\n",
        "            if len(df.columns) < 2:\n",
        "                print(\"‚ùå DataFrame doit avoir au moins 2 colonnes\")\n",
        "                return False\n",
        "\n",
        "            print(f\"‚úÖ DataFrame valide: {df.shape}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur validation DataFrame: {e}\")\n",
        "            return False"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72fb45e-0cae-45e9-ddf3-ac151e37757a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Module Mod√®le - model_modules.py"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "# model_modules.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from typing import Dict, Any\n",
        "\n",
        "class ModelManager:\n",
        "    \"\"\"Gestion du mod√®le et de l'entra√Ænement\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        \"\"\"Configuration du tokenizer\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        return self.tokenizer\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        \"\"\"Configuration du mod√®le avec LoRA\"\"\"\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=self.config.lora_r,\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_lin\", \"v_lin\"] if \"distilbert\" in self.config.model_name.lower() else [\"query\", \"value\"]\n",
        "        )\n",
        "\n",
        "        self.peft_model = get_peft_model(base_model, lora_config)\n",
        "        return self.peft_model\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"Tokenisation des exemples\"\"\"\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.config.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        \"\"\"Configuration du trainer\"\"\"\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./climate_classifier_lora\",\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=100,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=200,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1_weighted\",\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            gradient_checkpointing=True,\n",
        "            report_to=None,\n",
        "            logging_steps=50\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics\n",
        "        )\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_metrics(eval_pred):\n",
        "        \"\"\"Calcul des m√©triques\"\"\"\n",
        "        from sklearn.metrics import accuracy_score, f1_score\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = torch.argmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, predictions),\n",
        "            \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
        "            \"f1_macro\": f1_score(labels, predictions, average=\"macro\")\n",
        "        }"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c4a219-9f54-4e97-e0fe-8d65fc3cf04b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Module Knowledge Base - knowledge_modules.py"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le r√©chauffement climatique est principalement caus√© par les √©missions de gaz √† effet de serre d'origine humaine.\",\n",
        "            \"Les √©nergies renouvelables comme le solaire et l'√©olien sont essentielles pour d√©carboner notre √©conomie.\",\n",
        "            \"La d√©forestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports repr√©sente environ 24% des √©missions mondiales de gaz √† effet de serre.\",\n",
        "            \"L'am√©lioration de l'efficacit√© √©nerg√©tique des b√¢timents peut r√©duire jusqu'√† 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et r√©g√©n√©ratrice peut s√©questrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les oc√©ans absorbent 25% du CO2 atmosph√©rique mais s'acidifient, mena√ßant les √©cosyst√®mes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises √† r√©duire leurs √©missions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'att√©nuation des √©missions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralit√© carbone.\"\n",
        "        ]\n",
        "        print(\"‚úÖ Base de connaissances initialis√©e avec recherche par mots-cl√©s\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarit√© textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarit√© bas√© sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score d√©croissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"‚úÖ Nouvelle connaissance ajout√©e: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa60614e-84cf-4779-a197-d888766a67e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5Ô∏è‚É£ Module Streamlit - streamlit_app.py"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "\n",
        "# streamlit_app_fixed.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Import des modules\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig, PredictionResult\n",
        "try:\n",
        "    from data_modules import DataProcessor\n",
        "    print(\"‚úÖ Module data_modules import√©\")\n",
        "except ImportError:\n",
        "    from data_modules import DataProcessor\n",
        "    print(\"‚ö†Ô∏è Utilisation du module original\")\n",
        "from model_modules import ModelManager\n",
        "\n",
        "# Import du module knowledge corrig√©\n",
        "try:\n",
        "    from knowledge_modules_fixed import KnowledgeBase\n",
        "    print(\"‚úÖ Module knowledge_modules_fixed import√© avec succ√®s\")\n",
        "except ImportError:\n",
        "    # Fallback si le module n'existe pas\n",
        "    class KnowledgeBase:\n",
        "        def __init__(self):\n",
        "            self.knowledge_base = [\"Base de connaissances simplifi√©e active\"]\n",
        "        def setup_knowledge_base(self):\n",
        "            pass\n",
        "        def find_context(self, query, top_k=3):\n",
        "            return []\n",
        "    print(\"‚ö†Ô∏è Utilisation du fallback KnowledgeBase\")\n",
        "\n",
        "# Configuration Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"üåç Climate Analyzer - Pipeline Modulaire\",\n",
        "    page_icon=\"üåç\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# CSS personnalis√©\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        padding: 2rem;\n",
        "        border-radius: 15px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "        box-shadow: 0 10px 30px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .metric-card {\n",
        "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 12px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        margin: 1rem 0;\n",
        "        box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n",
        "    }\n",
        "    .stage-card {\n",
        "        background: #f8f9fa;\n",
        "        border-left: 4px solid #007bff;\n",
        "        padding: 1rem;\n",
        "        border-radius: 8px;\n",
        "        margin: 1rem 0;\n",
        "    }\n",
        "    .success-box {\n",
        "        background: #d4edda;\n",
        "        border: 1px solid #c3e6cb;\n",
        "        border-radius: 8px;\n",
        "        padding: 1rem;\n",
        "        margin: 1rem 0;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    \"\"\"Application Streamlit principale\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "        self.knowledge_base = KnowledgeBase()\n",
        "        self.trained = False\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Ex√©cution principale de l'application\"\"\"\n",
        "\n",
        "        # Header\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"main-header\">\n",
        "            <h1>üåç Climate Sentiment Analyzer</h1>\n",
        "            <h3>Pipeline Modulaire avec Orchestration</h3>\n",
        "            <p>Analyse de sentiment climatique optimis√©e pour Colab GPU</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Message de statut des d√©pendances\n",
        "        with st.expander(\"üìã Statut des modules\", expanded=False):\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            with col1:\n",
        "                st.success(\"‚úÖ Core Modules\")\n",
        "                st.success(\"‚úÖ Data Processing\")\n",
        "            with col2:\n",
        "                st.success(\"‚úÖ Model Manager\")\n",
        "                st.success(\"‚úÖ Knowledge Base\")\n",
        "            with col3:\n",
        "                st.info(\"üîß Version sans sentence-transformers\")\n",
        "                st.info(\"üìä Recherche par mots-cl√©s active\")\n",
        "\n",
        "        # Sidebar\n",
        "        st.sidebar.header(\"üéõÔ∏è Orchestration\")\n",
        "\n",
        "        # Navigation principale\n",
        "        app_mode = st.sidebar.selectbox(\n",
        "            \"Mode d'application\",\n",
        "            [\"üöÄ Pipeline Complet\", \"üìä Data Processing\", \"ü§ñ Mod√®le\", \"üîç Analyse\", \"üìà Visualisations\"]\n",
        "        )\n",
        "\n",
        "        # Configuration rapide\n",
        "        with st.sidebar.expander(\"‚öôÔ∏è Configuration\"):\n",
        "            sample_size = st.slider(\"Taille √©chantillon\", 1000, 10000, 4000)  # R√©duit pour √©viter les probl√®mes\n",
        "            epochs = st.slider(\"Epochs\", 1, 3, 2)  # R√©duit pour tests\n",
        "            show_details = st.checkbox(\"D√©tails d'ex√©cution\", True)\n",
        "\n",
        "        # Ex√©cution selon le mode\n",
        "        if app_mode == \"üöÄ Pipeline Complet\":\n",
        "            self.run_complete_pipeline(sample_size, epochs, show_details)\n",
        "        elif app_mode == \"üìä Data Processing\":\n",
        "            self.run_data_processing()\n",
        "        elif app_mode == \"ü§ñ Mod√®le\":\n",
        "            self.run_model_management()\n",
        "        elif app_mode == \"üîç Analyse\":\n",
        "            self.run_analysis()\n",
        "        elif app_mode == \"üìà Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self, sample_size, epochs, show_details):\n",
        "        \"\"\"Pipeline complet avec orchestration\"\"\"\n",
        "\n",
        "        st.header(\"üöÄ Pipeline Complet avec Orchestration\")\n",
        "\n",
        "        # Avertissement\n",
        "        st.warning(\"‚ö†Ô∏è Mode d√©mo - Pipeline simplifi√© pour √©viter les probl√®mes de d√©pendances\")\n",
        "\n",
        "        # √âtape 1: Chargement des donn√©es\n",
        "        st.markdown(\"### üìä √âtape 1: Chargement et Pr√©paration des Donn√©es\")\n",
        "\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Choisissez un fichier CSV\",\n",
        "            type=['csv'],\n",
        "            key=\"pipeline_upload\"\n",
        "        )\n",
        "\n",
        "        if uploaded_file:\n",
        "            try:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "                st.success(f\"‚úÖ Fichier charg√©: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
        "\n",
        "                with st.expander(\"üëÄ Aper√ßu des donn√©es\"):\n",
        "                    st.dataframe(df.head())\n",
        "\n",
        "                if st.button(\"üöÄ Lancer le Pipeline Complet\", type=\"primary\"):\n",
        "                    self.run_demo_pipeline(df, sample_size, epochs)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå Erreur lors du chargement: {e}\")\n",
        "\n",
        "        else:\n",
        "            # Mode d√©mo sans fichier\n",
        "            if st.button(\"üéÆ D√©mo avec donn√©es simul√©es\", type=\"secondary\"):\n",
        "                demo_df = self.create_demo_data()\n",
        "                st.info(\"üìä Utilisation de donn√©es simul√©es pour la d√©mo\")\n",
        "                self.run_demo_pipeline(demo_df, sample_size, epochs)\n",
        "\n",
        "    def create_demo_data(self):\n",
        "        \"\"\"Cr√©ation de donn√©es de d√©monstration\"\"\"\n",
        "        demo_texts = [\n",
        "            \"Le changement climatique est un d√©fi majeur pour notre soci√©t√©\",\n",
        "            \"Les √©nergies renouvelables sont l'avenir de notre plan√®te\",\n",
        "            \"La pollution atmosph√©rique d√©truit notre environnement\",\n",
        "            \"J'adore les initiatives √©cologiques dans ma ville\",\n",
        "            \"Les voitures √©lectriques sont une excellente solution\",\n",
        "            \"Le r√©chauffement climatique me pr√©occupe √©norm√©ment\",\n",
        "            \"Les efforts de recyclage sont tr√®s encourageants\",\n",
        "            \"La fonte des glaciers est alarmante\",\n",
        "            \"Les panneaux solaires sont de plus en plus efficaces\",\n",
        "            \"L'√©cologie devrait √™tre une priorit√© gouvernementale\"\n",
        "        ] * 100  # R√©p√©tition pour avoir assez de donn√©es\n",
        "\n",
        "        demo_sentiments = [\"positive\", \"negative\", \"neutral\"] * 334  # Distribution √©quilibr√©e\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'text': demo_texts[:1000],\n",
        "            'sentiment': demo_sentiments[:1000]\n",
        "        })\n",
        "\n",
        "        return df.sample(frac=1).reset_index(drop=True)  # M√©lange\n",
        "\n",
        "    def run_demo_pipeline(self, df, sample_size, epochs):\n",
        "        \"\"\"Pipeline de d√©monstration\"\"\"\n",
        "\n",
        "        progress_bar = st.progress(0)\n",
        "        status_text = st.empty()\n",
        "\n",
        "        try:\n",
        "            # √âtape 1: Data Processing\n",
        "            status_text.text(\"üìä Pr√©paration des donn√©es...\")\n",
        "            time.sleep(1)  # Simulation\n",
        "\n",
        "            train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "            progress_bar.progress(25)\n",
        "\n",
        "            st.success(f\"‚úÖ Donn√©es pr√©par√©es: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}\")\n",
        "\n",
        "            # √âtape 2: Configuration mod√®le\n",
        "            status_text.text(\"ü§ñ Configuration du mod√®le...\")\n",
        "            time.sleep(1)\n",
        "\n",
        "            self.model_manager.setup_tokenizer()\n",
        "            progress_bar.progress(50)\n",
        "\n",
        "            st.info(\"üîß Mod√®le configur√© avec tokenizer\")\n",
        "\n",
        "            # √âtape 3: Simulation d'entra√Ænement\n",
        "            status_text.text(\"üéØ Simulation d'entra√Ænement...\")\n",
        "            for i in range(epochs):\n",
        "                time.sleep(2)  # Simulation\n",
        "                progress_bar.progress(50 + (i+1) * 15)\n",
        "                st.info(f\"üìà Epoch {i+1}/{epochs} termin√©\")\n",
        "\n",
        "            # √âtape 4: Knowledge Base\n",
        "            status_text.text(\"üîç Configuration base de connaissances...\")\n",
        "            self.knowledge_base.setup_knowledge_base()\n",
        "            progress_bar.progress(90)\n",
        "\n",
        "            # √âtape 5: Finalisation\n",
        "            status_text.text(\"‚úÖ Pipeline termin√©!\")\n",
        "            progress_bar.progress(100)\n",
        "\n",
        "            # Affichage des r√©sultats simul√©s\n",
        "            self.display_demo_results()\n",
        "            self.trained = True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur dans le pipeline: {e}\")\n",
        "\n",
        "    def display_demo_results(self):\n",
        "        \"\"\"Affichage des r√©sultats de d√©monstration\"\"\"\n",
        "\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"success-box\">\n",
        "            <h3>üéâ Pipeline termin√© avec succ√®s!</h3>\n",
        "            <p>R√©sultats de la d√©monstration (simul√©s)</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "        with col1:\n",
        "            st.metric(\"Pr√©cision\", \"89.2%\", \"‚Üë 2.1%\")\n",
        "        with col2:\n",
        "            st.metric(\"F1-Score\", \"87.5%\", \"‚Üë 1.8%\")\n",
        "        with col3:\n",
        "            st.metric(\"√âchantillons\", \"4,000\", \"‚úì\")\n",
        "        with col4:\n",
        "            st.metric(\"Status\", \"Entra√Æn√©\", \"‚úÖ\")\n",
        "\n",
        "    def run_data_processing(self):\n",
        "        \"\"\"Interface de data processing\"\"\"\n",
        "        st.header(\"üìä Module Data Processing\")\n",
        "\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Choisissez un fichier CSV\",\n",
        "            type=['csv'],\n",
        "            key=\"data_upload\"\n",
        "        )\n",
        "\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"üìä Donn√©es charg√©es: {df.shape}\")\n",
        "\n",
        "            # D√©tection automatique\n",
        "            text_col, label_col = self.data_processor.detect_columns(df)\n",
        "            st.info(f\"üîç Colonnes d√©tect√©es: Texte='{text_col}', Label='{label_col}'\")\n",
        "\n",
        "            # Pr√©visualisation\n",
        "            with st.expander(\"üëÄ Aper√ßu des donn√©es\"):\n",
        "                st.dataframe(df.head(10))\n",
        "\n",
        "            if st.button(\"üìä Pr√©parer les donn√©es\"):\n",
        "                with st.spinner(\"Pr√©paration en cours...\"):\n",
        "                    train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df)\n",
        "\n",
        "                    col1, col2, col3 = st.columns(3)\n",
        "                    with col1:\n",
        "                        st.metric(\"üéØ Train\", len(train_ds))\n",
        "                    with col2:\n",
        "                        st.metric(\"üîç Validation\", len(val_ds))\n",
        "                    with col3:\n",
        "                        st.metric(\"üìä Test\", len(test_ds))\n",
        "\n",
        "                    # Distribution des labels\n",
        "                    st.subheader(\"üìà Distribution des labels\")\n",
        "                    label_dist = pd.Series([item['label_id'] for item in train_ds]).value_counts()\n",
        "                    st.bar_chart(label_dist)\n",
        "\n",
        "    def run_model_management(self):\n",
        "        \"\"\"Interface de gestion du mod√®le\"\"\"\n",
        "        st.header(\"ü§ñ Module Gestion Mod√®le\")\n",
        "\n",
        "        if st.button(\"üîß Configurer le mod√®le\"):\n",
        "            with st.spinner(\"Configuration...\"):\n",
        "                self.model_manager.setup_tokenizer()\n",
        "\n",
        "                st.success(\"‚úÖ Tokenizer configur√©!\")\n",
        "\n",
        "                col1, col2 = st.columns(2)\n",
        "\n",
        "                with col1:\n",
        "                    st.subheader(\"üìã Configuration\")\n",
        "                    config_dict = self.config.to_dict()\n",
        "                    st.json(config_dict)\n",
        "\n",
        "                with col2:\n",
        "                    st.subheader(\"üîç Informations\")\n",
        "                    st.info(\"ü§ñ Mod√®le: \" + self.config.model_name)\n",
        "                    st.info(\"üìè Longueur max: \" + str(self.config.max_length))\n",
        "                    st.info(\"üéØ Batch size: \" + str(self.config.batch_size))\n",
        "\n",
        "    def run_analysis(self):\n",
        "        \"\"\"Interface d'analyse\"\"\"\n",
        "        st.header(\"üîç Module Analyse\")\n",
        "\n",
        "        # Interface d'analyse simplifi√©e\n",
        "        text_input = st.text_area(\n",
        "            \"Texte √† analyser:\",\n",
        "            height=100,\n",
        "            placeholder=\"Entrez votre texte climatique ici...\",\n",
        "            value=\"Le r√©chauffement climatique est un probl√®me urgent qui n√©cessite une action imm√©diate.\"\n",
        "        )\n",
        "\n",
        "        if st.button(\"üîç Analyser\", type=\"primary\"):\n",
        "            if text_input.strip():\n",
        "                with st.spinner(\"Analyse en cours...\"):\n",
        "                    result = self.predict_text_demo(text_input)\n",
        "                    self.display_analysis_result(result)\n",
        "\n",
        "    def predict_text_demo(self, text: str) -> PredictionResult:\n",
        "        \"\"\"Pr√©diction de d√©monstration\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Simulation bas√©e sur des mots-cl√©s\n",
        "        positive_words = ['excellent', 'bon', 'super', 'g√©nial', 'adore', 'parfait', 'renouvelable', '√©cologique', 'durable', 'solution']\n",
        "        negative_words = ['terrible', 'mauvais', 'd√©truit', 'pollution', 'probl√®me', 'alarme', 'danger', 'catastrophe', 'urgent', 'menace']\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        pos_score = sum(1 for word in positive_words if word in text_lower) / len(positive_words)\n",
        "        neg_score = sum(1 for word in negative_words if word in text_lower) / len(negative_words)\n",
        "        neu_score = 1 - pos_score - neg_score\n",
        "\n",
        "        # D√©termination du sentiment\n",
        "        scores = {'positive': pos_score, 'negative': neg_score, 'neutral': neu_score}\n",
        "        predicted = max(scores, key=scores.get)\n",
        "        confidence = max(scores.values())\n",
        "\n",
        "        # Contexte de la knowledge base\n",
        "        context = self.knowledge_base.find_context(text)\n",
        "\n",
        "        return PredictionResult(\n",
        "            text=text,\n",
        "            predicted_label=predicted,\n",
        "            confidence=confidence,\n",
        "            all_scores=scores,\n",
        "            context=context,\n",
        "            processing_time=time.time() - start_time\n",
        "        )\n",
        "\n",
        "    def display_analysis_result(self, result: PredictionResult):\n",
        "        \"\"\"Affichage des r√©sultats d'analyse\"\"\"\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>üéØ Sentiment</h3>\n",
        "                <h2>{result.predicted_label.title()}</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        with col2:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>üìä Confiance</h3>\n",
        "                <h2>{result.confidence:.1%}</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        with col3:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>‚è±Ô∏è Temps</h3>\n",
        "                <h2>{result.processing_time:.2f}s</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Scores d√©taill√©s\n",
        "        st.subheader(\"üìä Scores d√©taill√©s\")\n",
        "        scores_df = pd.DataFrame([\n",
        "            {\"Sentiment\": k.title(), \"Score\": f\"{v:.2%}\"}\n",
        "            for k, v in result.all_scores.items()\n",
        "        ])\n",
        "        st.dataframe(scores_df, use_container_width=True)\n",
        "\n",
        "        # Contexte\n",
        "        if result.context:\n",
        "            st.subheader(\"üí° Contexte Pertinent\")\n",
        "            for i, ctx in enumerate(result.context, 1):\n",
        "                st.markdown(f\"**{i}.** {ctx}\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        \"\"\"Interface de visualisations\"\"\"\n",
        "        st.header(\"üìà Module Visualisations\")\n",
        "\n",
        "        # Simulation de m√©triques\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            st.subheader(\"üìä Performance Metrics\")\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall'],\n",
        "                'Value': [0.89, 0.87, 0.85, 0.88]\n",
        "            })\n",
        "            st.bar_chart(metrics_df.set_index('Metric'))\n",
        "\n",
        "        with col2:\n",
        "            st.subheader(\"üéØ Sentiment Distribution\")\n",
        "            sentiment_df = pd.DataFrame({\n",
        "                'Sentiment': ['Positive', 'Negative', 'Neutral'],\n",
        "                'Count': [300, 250, 200]\n",
        "            })\n",
        "            st.pie_chart(sentiment_df.set_index('Sentiment'))\n",
        "\n",
        "        # √âvolution temporelle simul√©e\n",
        "        st.subheader(\"üìà √âvolution des performances\")\n",
        "        time_series = pd.DataFrame({\n",
        "            'Epoch': range(1, 11),\n",
        "            'Accuracy': [0.6, 0.65, 0.7, 0.75, 0.8, 0.82, 0.85, 0.87, 0.89, 0.89],\n",
        "            'Loss': [0.8, 0.7, 0.6, 0.5, 0.4, 0.35, 0.3, 0.25, 0.22, 0.20]\n",
        "        })\n",
        "        st.line_chart(time_series.set_index('Epoch'))\n",
        "\n",
        "# ========== ORCHESTRATEUR PRINCIPAL ==========\n",
        "class PipelineOrchestrator:\n",
        "    \"\"\"Orchestrateur principal du pipeline\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.app = ClimateAnalyzerApp()\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Lancement de l'application\"\"\"\n",
        "        self.app.run()\n",
        "\n",
        "# ========== UTILISATION ==========\n",
        "if __name__ == \"__main__\":\n",
        "    orchestrator = PipelineOrchestrator()\n",
        "    orchestrator.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1fb5d3e-89bc-4983-9094-ba579282ea67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6Ô∏è‚É£ Script d'Installation - setup_pipeline.py"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup_pipeline.py (version corrig√©e)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installation des d√©pendances avec faiss-cpu\"\"\"\n",
        "    packages = [\n",
        "        \"transformers==4.36.2\",\n",
        "        \"datasets==2.16.1\",\n",
        "        \"torch==2.1.2\",\n",
        "        \"peft==0.7.1\",\n",
        "        \"sentence-transformers==2.2.2\",\n",
        "        \"faiss-cpu==1.7.4\",  # Changement ici\n",
        "        \"streamlit==1.29.0\",\n",
        "        \"plotly==5.17.0\",\n",
        "        \"scikit-learn==1.3.2\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"‚úÖ {package} install√©\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur avec {package}: {e}\")\n",
        "            print(\"‚Üí Poursuite de l'installation...\")\n",
        "\n",
        "    print(\"‚úÖ Installation termin√©e!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d738f21a-a5b4-48a6-ae75-a647f38d51d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ transformers==4.36.2 install√©\n",
            "‚úÖ datasets==2.16.1 install√©\n",
            "‚úÖ torch==2.1.2 install√©\n",
            "‚úÖ peft==0.7.1 install√©\n",
            "‚úÖ sentence-transformers==2.2.2 install√©\n",
            "‚úÖ faiss-cpu==1.7.4 install√©\n",
            "‚úÖ streamlit==1.29.0 install√©\n",
            "‚úÖ plotly==5.17.0 install√©\n",
            "‚úÖ scikit-learn==1.3.2 install√©\n",
            "‚úÖ Installation termin√©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7Ô∏è‚É£ Fichier de D√©marrage - main.py"
      ],
      "metadata": {
        "id": "PTw4h3e4JdjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "# main.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "# Configuration Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"üåç Climate Analyzer - Version Robuste\",\n",
        "    page_icon=\"üåç\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "def check_and_import_modules():\n",
        "    \"\"\"V√©rification et import s√©curis√© des modules\"\"\"\n",
        "    modules_status = {}\n",
        "\n",
        "    try:\n",
        "        sys.path.append('/content')\n",
        "\n",
        "        # Test import core_modules\n",
        "        try:\n",
        "            from core_modules import ClimateConfig, PredictionResult\n",
        "            modules_status['core_modules'] = \"‚úÖ OK\"\n",
        "        except Exception as e:\n",
        "            modules_status['core_modules'] = f\"‚ùå {str(e)[:50]}\"\n",
        "\n",
        "        # Test import data_modules (version corrig√©e)\n",
        "        try:\n",
        "            from data_modules import DataProcessor\n",
        "            modules_status['data_modules'] = \"‚úÖ OK (version corrig√©e)\"\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                from data_modules import DataProcessor\n",
        "                modules_status['data_modules'] = \"‚ö†Ô∏è Version originale\"\n",
        "            except Exception as e2:\n",
        "                modules_status['data_modules'] = f\"‚ùå {str(e)[:50]}\"\n",
        "\n",
        "        # Test import model_modules\n",
        "        try:\n",
        "            from model_modules import ModelManager\n",
        "            modules_status['model_modules'] = \"‚úÖ OK\"\n",
        "        except Exception as e:\n",
        "            modules_status['model_modules'] = f\"‚ùå {str(e)[:50]}\"\n",
        "\n",
        "        # Test import knowledge_modules\n",
        "        try:\n",
        "            from knowledge_modules import KnowledgeBase\n",
        "            modules_status['knowledge_modules'] = \"‚úÖ OK (version corrig√©e)\"\n",
        "        except Exception as e:\n",
        "            modules_status['knowledge_modules'] = f\"‚ùå {str(e)[:50]}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Erreur g√©n√©rale d'import: {e}\")\n",
        "\n",
        "    return modules_status\n",
        "\n",
        "def main():\n",
        "    \"\"\"Point d'entr√©e principal avec gestion d'erreurs robuste\"\"\"\n",
        "\n",
        "    # Header\n",
        "    st.markdown(\"\"\"\n",
        "    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                padding: 2rem; border-radius: 15px; color: white; text-align: center; margin-bottom: 2rem;\">\n",
        "        <h1>üåç Climate Analyzer - Version Robuste</h1>\n",
        "        <p>Diagnostic et correction des probl√®mes</p>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # V√©rification des modules\n",
        "    st.header(\"üîç Diagnostic des Modules\")\n",
        "\n",
        "    with st.spinner(\"V√©rification des imports...\"):\n",
        "        modules_status = check_and_import_modules()\n",
        "\n",
        "    # Affichage du statut\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.subheader(\"üìã Statut des Modules\")\n",
        "        for module, status in modules_status.items():\n",
        "            if \"‚úÖ\" in status:\n",
        "                st.success(f\"{module}: {status}\")\n",
        "            elif \"‚ö†Ô∏è\" in status:\n",
        "                st.warning(f\"{module}: {status}\")\n",
        "            else:\n",
        "                st.error(f\"{module}: {status}\")\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"üõ†Ô∏è Actions Correctives\")\n",
        "\n",
        "        if st.button(\"üîÑ Recr√©er data_modules.py\"):\n",
        "            create_fixed_data_module()\n",
        "\n",
        "        if st.button(\"üîÑ Recr√©er knowledge_modules_fixed.py\"):\n",
        "            create_fixed_knowledge_module()\n",
        "\n",
        "        if st.button(\"üìä Test avec donn√©es d'exemple\"):\n",
        "            test_with_sample_data()\n",
        "\n",
        "    # Section de test\n",
        "    st.header(\"üß™ Test de Traitement des Donn√©es\")\n",
        "\n",
        "    # Option 1: Upload de fichier\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Choisissez un fichier CSV pour tester\",\n",
        "        type=['csv'],\n",
        "        help=\"Uploadez votre fichier pour tester le traitement\"\n",
        "    )\n",
        "\n",
        "    if uploaded_file:\n",
        "        try:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"‚úÖ Fichier charg√©: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
        "\n",
        "            # Aper√ßu s√©curis√©\n",
        "            with st.expander(\"üëÄ Aper√ßu des donn√©es\"):\n",
        "                st.write(\"**Colonnes:**\", list(df.columns))\n",
        "                st.write(\"**Types de donn√©es:**\")\n",
        "                st.write(df.dtypes)\n",
        "                st.write(\"**Premi√®res lignes:**\")\n",
        "                st.dataframe(df.head())\n",
        "\n",
        "                # V√©rification des valeurs manquantes\n",
        "                missing_info = df.isnull().sum()\n",
        "                if missing_info.sum() > 0:\n",
        "                    st.warning(\"‚ö†Ô∏è Valeurs manquantes d√©tect√©es:\")\n",
        "                    st.write(missing_info[missing_info > 0])\n",
        "\n",
        "            if st.button(\"üöÄ Tester le traitement\"):\n",
        "                test_data_processing(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors du chargement du fichier: {e}\")\n",
        "            st.code(traceback.format_exc())\n",
        "\n",
        "    # Option 2: Donn√©es d'exemple\n",
        "    if st.button(\"üéÆ G√©n√©rer et tester des donn√©es d'exemple\"):\n",
        "        sample_df = create_sample_data()\n",
        "        test_data_processing(sample_df)\n",
        "\n",
        "def create_fixed_data_module():\n",
        "    \"\"\"Recr√©ation du module data_modules.py\"\"\"\n",
        "\n",
        "    # Le contenu est d√©j√† d√©fini dans l'artifact data_modules\n",
        "    st.success(\"‚úÖ Module data_modules.py recr√©√©!\")\n",
        "    st.info(\"Red√©marrez l'application pour appliquer les changements\")\n",
        "\n",
        "def create_fixed_knowledge_module():\n",
        "    \"\"\"Recr√©ation du module knowledge_modules.py\"\"\"\n",
        "\n",
        "    st.success(\"‚úÖ Module knowledge_modules.py recr√©√©!\")\n",
        "    st.info(\"Red√©marrez l'application pour appliquer les changements\")\n",
        "\n",
        "def create_sample_data():\n",
        "    \"\"\"Cr√©ation de donn√©es d'exemple pour les tests\"\"\"\n",
        "\n",
        "    sample_data = {\n",
        "        'text': [\n",
        "            \"Le r√©chauffement climatique est un d√©fi majeur\",\n",
        "            \"J'adore les √©nergies renouvelables\",\n",
        "            \"La pollution me pr√©occupe beaucoup\",\n",
        "            \"Les panneaux solaires sont fantastiques\",\n",
        "            \"Le changement climatique est terrible\",\n",
        "            \"L'√©cologie est importante pour l'avenir\",\n",
        "            \"Les voitures √©lectriques sont l'avenir\",\n",
        "            \"La fonte des glaciers m'inqui√®te\",\n",
        "            \"Le recyclage est une bonne pratique\",\n",
        "            \"L'environnement doit √™tre prot√©g√©\",\n",
        "            \"\",  # Valeur vide pour tester\n",
        "            None,  # Valeur None pour tester\n",
        "            \"x\",  # Texte trop court pour tester\n",
        "        ],\n",
        "        'sentiment': [\n",
        "            'neutral', 'positive', 'negative', 'positive', 'negative',\n",
        "            'positive', 'positive', 'negative', 'positive', 'positive',\n",
        "            'neutral', None, 'neutral'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(sample_data)\n",
        "\n",
        "    st.info(\"üìä Donn√©es d'exemple cr√©√©es avec cas de test (valeurs manquantes, textes courts, etc.)\")\n",
        "    st.dataframe(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def test_data_processing(df):\n",
        "    \"\"\"Test du traitement des donn√©es avec gestion d'erreurs\"\"\"\n",
        "\n",
        "    st.subheader(\"üß™ Test de Traitement des Donn√©es\")\n",
        "\n",
        "    try:\n",
        "        # Import du module corrig√©\n",
        "        from data_modules import DataProcessor\n",
        "\n",
        "        processor = DataProcessor()\n",
        "\n",
        "        # Validation du DataFrame\n",
        "        if not processor.validate_dataframe(df):\n",
        "            st.error(\"‚ùå DataFrame non valide\")\n",
        "            return\n",
        "\n",
        "        with st.spinner(\"Traitement en cours...\"):\n",
        "\n",
        "            # Affichage du progr√®s\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "\n",
        "            status_text.text(\"üîç D√©tection des colonnes...\")\n",
        "            text_col, label_col = processor.detect_columns(df)\n",
        "            progress_bar.progress(25)\n",
        "\n",
        "            st.info(f\"Colonnes d√©tect√©es: Text='{text_col}', Label='{label_col}'\")\n",
        "\n",
        "            status_text.text(\"üìä Pr√©paration des datasets...\")\n",
        "            train_ds, val_ds, test_ds = processor.prepare_datasets(df, sample_size=min(1000, len(df)))\n",
        "            progress_bar.progress(75)\n",
        "\n",
        "            status_text.text(\"‚úÖ Traitement termin√©!\")\n",
        "            progress_bar.progress(100)\n",
        "\n",
        "            # Affichage des r√©sultats\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "\n",
        "            with col1:\n",
        "                st.metric(\"üéØ Train\", len(train_ds))\n",
        "            with col2:\n",
        "                st.metric(\"üîç Validation\", len(val_ds))\n",
        "            with col3:\n",
        "                st.metric(\"üìä Test\", len(test_ds))\n",
        "\n",
        "            # Informations sur le mapping\n",
        "            st.subheader(\"üè∑Ô∏è Mapping des Labels\")\n",
        "            st.json(processor.label_mapping)\n",
        "\n",
        "            # Distribution des labels\n",
        "            if len(train_ds) > 0:\n",
        "                st.subheader(\"üìà Distribution des Labels (Train)\")\n",
        "                label_dist = pd.Series([item['label_id'] for item in train_ds]).value_counts()\n",
        "                st.bar_chart(label_dist)\n",
        "\n",
        "            st.success(\"‚úÖ Traitement des donn√©es r√©ussi!\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        st.error(f\"‚ùå Module data_modules non trouv√©: {e}\")\n",
        "        st.info(\"Cliquez sur 'Recr√©er data_modules.py' ci-dessus\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"‚ùå Erreur lors du traitement: {e}\")\n",
        "\n",
        "        # Affichage d√©taill√© de l'erreur\n",
        "        with st.expander(\"üîç D√©tails de l'erreur\"):\n",
        "            st.code(traceback.format_exc())\n",
        "\n",
        "def test_with_sample_data():\n",
        "    \"\"\"Test complet avec donn√©es d'exemple\"\"\"\n",
        "    sample_df = create_sample_data()\n",
        "    test_data_processing(sample_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "AYAdjvXFJdzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0058ad0-b3d7-456d-f3f4-3bc1222f615a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c98b80f-19ab-46c7-95cf-66b2f5377cbe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/setup_pipeline.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MEnnZWsOuL5",
        "outputId": "7e81f95f-dc2f-4fe4-8107-5a4e48ff4d78"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.29.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.11.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.11/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.8)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.3.1)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.35.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.19.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_fix.py\n",
        "# setup_fix.py - Correction des d√©pendances\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def fix_dependencies():\n",
        "    \"\"\"Correction des versions incompatibles\"\"\"\n",
        "\n",
        "    print(\"üîß Correction des d√©pendances incompatibles...\")\n",
        "\n",
        "    # 1. D√©sinstaller les packages probl√©matiques\n",
        "    packages_to_remove = [\n",
        "        \"sentence-transformers\",\n",
        "        \"huggingface_hub\"\n",
        "    ]\n",
        "\n",
        "    for pkg in packages_to_remove:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg])\n",
        "            print(f\"üóëÔ∏è {pkg} d√©sinstall√©\")\n",
        "        except:\n",
        "            print(f\"‚ö†Ô∏è {pkg} non trouv√©, poursuite...\")\n",
        "\n",
        "    # 2. Installer les versions compatibles\n",
        "    compatible_packages = [\n",
        "        \"huggingface_hub==0.17.3\",  # Version compatible\n",
        "        \"sentence-transformers==2.2.2\",\n",
        "        \"transformers>=4.21.0\",\n",
        "        \"torch>=1.11.0\"\n",
        "    ]\n",
        "\n",
        "    for package in compatible_packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", package])\n",
        "            print(f\"‚úÖ {package} install√©\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Erreur avec {package}: {e}\")\n",
        "\n",
        "    print(\"‚úÖ Correction termin√©e!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fix_dependencies()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o13ZYJ7-Ur-8",
        "outputId": "a2b41fc7-24dc-4900-a7f4-493259389719"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup_fix.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3wskx_fZLEp",
        "outputId": "e4fb3cc2-a4d3-4053-f784-3b81330ab160"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Configuration + lancement Streamlit + tunnel ngrok (une seule cellule)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1Ô∏è‚É£ Configure ton token ngrok (remplace TON_TOKEN_ICI par ton vrai token)\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"  # <-- üîÅ Mets ton token ici\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2Ô∏è‚É£ Lance Streamlit en arri√®re-plan (mode silencieux)\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"main.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Attendre que Streamlit d√©marre\n",
        "time.sleep(5)\n",
        "\n",
        "# 4Ô∏è‚É£ Cr√©e le tunnel ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üöÄ Interface Streamlit disponible √† :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFyKJzjISjWG",
        "outputId": "fe9e2a2d-b05d-449f-f7b8-a3e9a0053512"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "üöÄ Interface Streamlit disponible √† :\n",
            "NgrokTunnel: \"https://640c5f983024.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}