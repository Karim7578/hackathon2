{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-Tuned LLM for Sentiment Analysis and Contextual Responses**"
      ],
      "metadata": {
        "id": "kRV1ui22XOMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans un monde saturÃ© de donnÃ©es, oÃ¹ les entreprises cherchent Ã  automatiser leurs interactions tout en maintenant un haut niveau de personnalisation, lâ€™Intelligence Artificielle gÃ©nÃ©rative ouvre des perspectives inÃ©dites. Mais comment crÃ©er un assistant conversationnel qui ne se contente pas de rÃ©pondre, mais qui comprend vraiment ce que ressent lâ€™utilisateur et sâ€™adapte Ã  son contexteâ€¯?\n",
        "\n",
        "Câ€™est tout lâ€™enjeu de notre projet : construire un assistant IA intelligent capable Ã  la fois de dÃ©tecter le ton Ã©motionnel dâ€™un message (positif, nÃ©gatif, neutre) et de gÃ©nÃ©rer des rÃ©ponses enrichies par un moteur de recherche contextuel, le tout avec des modÃ¨les lÃ©gers et optimisÃ©s grÃ¢ce Ã  LoRA, une mÃ©thode de fine-tuning Ã©conome en ressources.\n",
        "\n",
        "Notre mission : rendre lâ€™IA utile, pertinente et accessible, mÃªme avec des moyens limitÃ©s. Ce projet dÃ©montre quâ€™on peut concilier efficacitÃ©, sobriÃ©tÃ© technologique et intelligence conversationnelle, pour rÃ©pondre Ã  des enjeux mÃ©tiers bien rÃ©els : service client, RH, assistance juridique, e-commerceâ€¦ les cas dâ€™usage sont nombreux."
      ],
      "metadata": {
        "id": "lby4w31eZ5u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. **Module Core - core_modules.py**"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce module core_modules.py constitue un composant fondamental de lâ€™architecture du projet. Il centralise Ã  la fois les paramÃ¨tres de configuration du systÃ¨me et la structure de sortie des prÃ©dictions, permettant ainsi une meilleure organisation, une lisibilitÃ© accrue, et une Ã©volutivitÃ© maÃ®trisÃ©e du code."
      ],
      "metadata": {
        "id": "7E-mLbnTbf5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Intention et dÃ©marche**\n",
        "\n",
        "Objectif du module :\n",
        "Structurer proprement les rÃ©sultats via la classe PredictionResult.\n",
        "\n",
        "Externaliser la configuration dans une classe ClimateConfig pour Ã©viter les variables magiques dispersÃ©es dans le code.\n",
        "\n",
        "Pourquoi câ€™est important :\n",
        "Lorsqu'on travaille sur un projet complexe avec fine-tuning de LLMs, classification, gÃ©nÃ©ration, et retrieval, il devient essentiel de standardiser les flux de donnÃ©es et de modulariser les paramÃ¨tres.\n",
        "\n",
        "Cela permet aussi dâ€™assurer une compatibilitÃ© fluide entre les modules dâ€™Ã©valuation, dâ€™interface et de gÃ©nÃ©ration.\n",
        "\n"
      ],
      "metadata": {
        "id": "umwdBD-Tbjnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    \"\"\"Structure pour les rÃ©sultats de prÃ©diction\"\"\"\n",
        "    text: str\n",
        "    predicted_label: str\n",
        "    confidence: float\n",
        "    all_scores: Dict[str, float]\n",
        "    context: Optional[List[str]] = None\n",
        "    processing_time: float = 0.0\n",
        "\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralisÃ©e\"\"\"\n",
        "    def __init__(self):\n",
        "        self.model_name = \"distilbert-base-uncased\"\n",
        "        self.max_length = 256\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 2e-4\n",
        "        self.epochs = 3\n",
        "        self.lora_r = 16\n",
        "        self.lora_alpha = 32\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            'model_name': self.model_name,\n",
        "            'max_length': self.max_length,\n",
        "            'batch_size': self.batch_size,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'epochs': self.epochs,\n",
        "            'device': str(self.device),\n",
        "            'lora_config': {'r': self.lora_r, 'alpha': self.lora_alpha}\n",
        "        }"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e005a9bd-9509-45cc-f254-9756e7fc1523"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ClimateConfig`\n",
        "Une classe de configuration centralisÃ©e :\n",
        "\n",
        "Contient les hyperparamÃ¨tres du modÃ¨le, du fine-tuning LoRA, et la dÃ©tection automatique de device (GPU/CPU).\n",
        "\n",
        "GrÃ¢ce Ã  la mÃ©thode to_dict(), cette configuration peut Ãªtre journalisÃ©e, sauvegardÃ©e, ou utilisÃ©e dynamiquement dans dâ€™autres modules.\n",
        "\n",
        "**Avantage** : on peut tester plusieurs variantes de configuration sans toucher au cÅ“ur du code. Câ€™est essentiel pour lâ€™expÃ©rimentation en machine learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "IaISoKnccayf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enjeu technique et challenge :**\n",
        "Le challenge ici est de maintenir un code propre, modulaire et traÃ§able, dans un projet mÃªlant fine-tuning PEFT (LoRA), embedding, retrieval, infÃ©rence gÃ©nÃ©rative, et interface utilisateur.\n",
        "\n",
        "Une mauvaise gestion des paramÃ¨tres ou un manque de structuration des rÃ©sultats rendraient le projet instable, peu rÃ©utilisable, et difficile Ã  Ã©valuer.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3xmaORacng1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**InterprÃ©tation du rÃ©sultat :**\n",
        "Ce module ne retourne pas un rÃ©sultat au sens fonctionnel immÃ©diat, mais il formalise et encapsule deux Ã©lÃ©ments essentiels :\n",
        "\n",
        "\n",
        "\n",
        "*   La standardisation de la sortie du modÃ¨le (PredictionResult)\n",
        "*   La gouvernance centralisÃ©e des paramÃ¨tres (ClimateConfig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Cela permet de :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Faciliter lâ€™orchestration du pipeline global\n",
        "*   Tracer les expÃ©rimentations\n",
        "*   IntÃ©grer facilement des logs, mÃ©triques, et dashboards\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gP5q7ZdJc4eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :** Ce module pose les bases de la robustesse du projet IA. Il reflÃ¨te une bonne pratique dâ€™industrialisation des projets LLM : abstraction des paramÃ¨tres, traÃ§abilitÃ©, et standardisation des rÃ©sultats. Dans un contexte professionnel, cette structuration permet :\n",
        "\n",
        "\n",
        "*   Une collaboration fluide entre data scientists et dÃ©veloppeurs front-end\n",
        "*   Une rÃ©utilisabilitÃ© du modÃ¨le dans d'autres projets\n",
        "*   Une mise en production simplifiÃ©e\n",
        "\n",
        "\n",
        "En rÃ©sumÃ©, ce fichier est invisible pour lâ€™utilisateur final, mais essentiel Ã  la stabilitÃ©, lâ€™Ã©volution, et la fiabilitÃ© du systÃ¨me."
      ],
      "metadata": {
        "id": "8s4auA5IdRic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. **Module Data Processing - data_modules.py**"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce module data_modules.py est le pÃ´le central de prÃ©traitement des donnÃ©es dans le pipeline du projet de fine-tuning LLM avec LoRA et gÃ©nÃ©ration contextuelle. Il a Ã©tÃ© conÃ§u avec une logique rÃ©siliente, automatisÃ©e et hautement rÃ©utilisable, indispensable pour manipuler des datasets hÃ©tÃ©rogÃ¨nes dans un contexte de hackathon ou dâ€™expÃ©rimentation rapide.\n",
        "\n"
      ],
      "metadata": {
        "id": "ojWFjD4keP89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intention et DÃ©marche**"
      ],
      "metadata": {
        "id": "BlfrNA8_eZrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Objectifs principaux :**\n",
        "\n",
        "*   DÃ©tecter automatiquement les colonnes texte et label dans nâ€™importe quel DataFrame.\n",
        "*   Nettoyer et prÃ©parer les donnÃ©es de maniÃ¨re robuste, quels que soient leur format ou leur qualitÃ© initiale.\n",
        "*   GÃ©nÃ©rer un triplet train/val/test proprement structurÃ©, stratifiÃ© si possible.\n",
        "*   Produire un format compatible avec les modÃ¨les Hugging Face (datasets.Dataset).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fyki8ArTeWD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pourquoi cette approche ?**\n",
        "Parce que dans un hackathon ou une expÃ©rimentation IA rapide :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Les donnÃ©es ne sont pas toujours bien structurÃ©es.\n",
        "*   Le temps est limitÃ© pour ajuster manuellement les colonnes ou le nettoyage.\n",
        "*   Une flexibilitÃ© et automatisation maximale est nÃ©cessaire.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MM6w7f53esmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "\n",
        "# data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Gestion centralisÃ©e du traitement des donnÃ©es avec gestion robuste des erreurs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        \"\"\"DÃ©tection automatique des colonnes texte et label avec validation\"\"\"\n",
        "        print(f\" DÃ©tection des colonnes sur {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
        "        print(f\" Colonnes disponibles: {list(df.columns)}\")\n",
        "\n",
        "        text_keywords = ['self_text', 'text', 'content', 'message', 'comment', 'body', 'description']\n",
        "        label_keywords = ['comment_sentiment', 'sentiment', 'label', 'category', 'class', 'target']\n",
        "\n",
        "        # Recherche intelligente\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        # Recherche par mots-clÃ©s\n",
        "        for col in df.columns:\n",
        "            col_lower = str(col).lower()\n",
        "\n",
        "            # Recherche colonne texte\n",
        "            if not text_col:\n",
        "                for keyword in text_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        text_col = col\n",
        "                        break\n",
        "\n",
        "            # Recherche colonne label\n",
        "            if not label_col:\n",
        "                for keyword in label_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        label_col = col\n",
        "                        break\n",
        "\n",
        "        # Fallback intelligent pour la colonne texte\n",
        "        if not text_col:\n",
        "            string_cols = []\n",
        "            for col in df.columns:\n",
        "                try:\n",
        "                    # VÃ©rifier si la colonne contient principalement du texte\n",
        "                    sample = df[col].dropna().head(100)\n",
        "                    if len(sample) > 0:\n",
        "                        # Convertir en string et calculer la longueur moyenne\n",
        "                        sample_str = sample.astype(str)\n",
        "                        avg_length = sample_str.str.len().mean()\n",
        "                        if avg_length > 10:  # Textes probablement plus longs que 10 caractÃ¨res\n",
        "                            string_cols.append((col, avg_length))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if string_cols:\n",
        "                # Prendre la colonne avec le texte le plus long en moyenne\n",
        "                text_col = max(string_cols, key=lambda x: x[1])[0]\n",
        "            else:\n",
        "                # Last resort: premiÃ¨re colonne object\n",
        "                object_cols = df.select_dtypes(include=['object']).columns\n",
        "                if len(object_cols) > 0:\n",
        "                    text_col = object_cols[0]\n",
        "\n",
        "        # Fallback pour la colonne label\n",
        "        if not label_col:\n",
        "            # Chercher une colonne avec peu de valeurs uniques (potentiel label)\n",
        "            for col in df.columns:\n",
        "                if col != text_col:\n",
        "                    try:\n",
        "                        unique_count = df[col].nunique()\n",
        "                        total_count = len(df[col].dropna())\n",
        "                        if total_count > 0 and unique_count < min(20, total_count * 0.1):\n",
        "                            label_col = col\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Si toujours pas trouvÃ©, prendre la derniÃ¨re colonne\n",
        "            if not label_col:\n",
        "                label_col = df.columns[-1]\n",
        "\n",
        "        print(f\" Colonnes dÃ©tectÃ©es: Text='{text_col}', Label='{label_col}'\")\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text_column(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Nettoyage robuste d'une colonne texte\"\"\"\n",
        "        try:\n",
        "            # Convertir en string d'abord\n",
        "            cleaned = series.astype(str)\n",
        "\n",
        "            # Remplacer les valeurs problÃ©matiques\n",
        "            cleaned = cleaned.replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "            # Supprimer les espaces\n",
        "            cleaned = cleaned.str.strip()\n",
        "\n",
        "            # Remplacer les chaÃ®nes vides par NaN\n",
        "            cleaned = cleaned.replace('', pd.NA)\n",
        "\n",
        "            return cleaned\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur nettoyage texte: {e}\")\n",
        "            # Fallback: conversion simple\n",
        "            return series.astype(str)\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        \"\"\"PrÃ©paration des datasets avec validation robuste\"\"\"\n",
        "\n",
        "        print(f\" PrÃ©paration des datasets - Taille originale: {df.shape}\")\n",
        "\n",
        "        # DÃ©tection des colonnes\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "\n",
        "        if not self.text_col or not self.label_col:\n",
        "            raise ValueError(f\" Impossible de dÃ©tecter les colonnes: text='{self.text_col}', label='{self.label_col}'\")\n",
        "\n",
        "        # Extraction et copie des colonnes nÃ©cessaires\n",
        "        try:\n",
        "            df_work = df[[self.text_col, self.label_col]].copy()\n",
        "        except KeyError as e:\n",
        "            print(f\" Colonnes manquantes: {e}\")\n",
        "            print(f\"Colonnes disponibles: {list(df.columns)}\")\n",
        "            raise\n",
        "\n",
        "        # Renommer les colonnes\n",
        "        df_work.columns = ['text', 'label']\n",
        "\n",
        "        print(f\" Avant nettoyage: {len(df_work)} lignes\")\n",
        "\n",
        "        # Nettoyage robuste des donnÃ©es\n",
        "        # 1. Nettoyage de la colonne texte\n",
        "        df_work['text'] = self.clean_text_column(df_work['text'])\n",
        "\n",
        "        # 2. Nettoyage de la colonne label\n",
        "        df_work['label'] = df_work['label'].astype(str).str.strip()\n",
        "        df_work['label'] = df_work['label'].replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "        # 3. Suppression des lignes avec des valeurs manquantes\n",
        "        initial_size = len(df_work)\n",
        "        df_work = df_work.dropna()\n",
        "        print(f\"ðŸ§¹ AprÃ¨s suppression des NaN: {len(df_work)} lignes (supprimÃ©: {initial_size - len(df_work)})\")\n",
        "\n",
        "        # 4. Filtrage des textes trop courts (de maniÃ¨re sÃ©curisÃ©e)\n",
        "        try:\n",
        "            # VÃ©rifier que nous avons bien des strings\n",
        "            df_work['text'] = df_work['text'].astype(str)\n",
        "\n",
        "            # Filtrer les textes trop courts\n",
        "            mask = df_work['text'].str.len() > 5\n",
        "            df_work = df_work[mask]\n",
        "            print(f\" AprÃ¨s filtrage textes courts: {len(df_work)} lignes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erreur lors du filtrage des textes: {e}\")\n",
        "            # Continuer sans filtrage si erreur\n",
        "\n",
        "        # VÃ©rification finale\n",
        "        if len(df_work) == 0:\n",
        "            raise ValueError(\" Aucune donnÃ©e valide aprÃ¨s nettoyage!\")\n",
        "\n",
        "        # 5. Ã‰chantillonnage si nÃ©cessaire\n",
        "        if len(df_work) > sample_size:\n",
        "            df_work = df_work.sample(n=sample_size, random_state=42)\n",
        "            print(f\" Ã‰chantillonnage Ã  {sample_size} lignes\")\n",
        "\n",
        "        # 6. Mapping des labels\n",
        "        unique_labels = sorted(df_work['label'].unique())\n",
        "        print(f\" Labels uniques trouvÃ©s: {unique_labels}\")\n",
        "\n",
        "        self.label_mapping = {str(label): idx for idx, label in enumerate(unique_labels)}\n",
        "        df_work['label_id'] = df_work['label'].astype(str).map(self.label_mapping)\n",
        "\n",
        "        # VÃ©rification du mapping\n",
        "        if df_work['label_id'].isna().any():\n",
        "            print(\" ProblÃ¨me de mapping des labels dÃ©tectÃ©\")\n",
        "            print(f\"Labels non mappÃ©s: {df_work[df_work['label_id'].isna()]['label'].unique()}\")\n",
        "\n",
        "        print(f\" Mapping des labels: {self.label_mapping}\")\n",
        "\n",
        "        # 7. Splits stratifiÃ©s\n",
        "        try:\n",
        "            # VÃ©rifier si on peut faire une stratification\n",
        "            if len(unique_labels) > 1 and all(df_work['label_id'].value_counts() >= 2):\n",
        "                stratify_col = df_work['label_id']\n",
        "                print(\" Stratification activÃ©e\")\n",
        "            else:\n",
        "                stratify_col = None\n",
        "                print(\" Pas de stratification (pas assez d'exemples par classe)\")\n",
        "\n",
        "            # Premier split: train vs (val + test)\n",
        "            train_df, temp_df = train_test_split(\n",
        "                df_work,\n",
        "                test_size=0.4,\n",
        "                random_state=42,\n",
        "                stratify=stratify_col if stratify_col is not None else None\n",
        "            )\n",
        "\n",
        "            # DeuxiÃ¨me split: val vs test\n",
        "            if stratify_col is not None:\n",
        "                temp_stratify = temp_df['label_id']\n",
        "            else:\n",
        "                temp_stratify = None\n",
        "\n",
        "            val_df, test_df = train_test_split(\n",
        "                temp_df,\n",
        "                test_size=0.5,\n",
        "                random_state=42,\n",
        "                stratify=temp_stratify if temp_stratify is not None else None\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur lors du split: {e}\")\n",
        "            # Fallback: split simple\n",
        "            train_size = int(0.6 * len(df_work))\n",
        "            val_size = int(0.2 * len(df_work))\n",
        "\n",
        "            train_df = df_work[:train_size]\n",
        "            val_df = df_work[train_size:train_size+val_size]\n",
        "            test_df = df_work[train_size+val_size:]\n",
        "\n",
        "        print(f\" Splits finaux: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "        # 8. Conversion en Dataset\n",
        "        try:\n",
        "            train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']].reset_index(drop=True))\n",
        "\n",
        "            print(\" Datasets crÃ©Ã©s avec succÃ¨s\")\n",
        "\n",
        "            return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur lors de la crÃ©ation des datasets: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques du processeur de donnÃ©es\"\"\"\n",
        "        return {\n",
        "            \"text_column\": self.text_col,\n",
        "            \"label_column\": self.label_col,\n",
        "            \"label_mapping\": self.label_mapping,\n",
        "            \"num_labels\": len(self.label_mapping)\n",
        "        }\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Validation d'un DataFrame\"\"\"\n",
        "        try:\n",
        "            if df is None or df.empty:\n",
        "                print(\" DataFrame vide ou None\")\n",
        "                return False\n",
        "\n",
        "            if len(df.columns) < 2:\n",
        "                print(\" DataFrame doit avoir au moins 2 colonnes\")\n",
        "                return False\n",
        "\n",
        "            print(f\" DataFrame valide: {df.shape}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur validation DataFrame: {e}\")\n",
        "            return False"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f71032-24a3-4c1e-c5df-c5742cbdb14f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enjeux :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   FiabilitÃ© sur donnÃ©es inconnues : il faut que le module fonctionne sur n'importe quel jeu de donnÃ©es texte+label.\n",
        "*   Robustesse aux erreurs utilisateurs : fichiers mal formatÃ©s, labels textuels, colonnes manquantes ou bruitÃ©es.\n",
        "*   PrÃ©paration optimisÃ©e pour le fine-tuning LLMs : nettoyage, encodage des labels, et split cohÃ©rent sont critiques pour Ã©viter du surapprentissage ou des Ã©checs d'entraÃ®nement.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Challenges techniques :**\n",
        "\n",
        "\n",
        "\n",
        "*   DÃ©tecter automatiquement les bonnes colonnes sans\n",
        "heuristique trop rigide.\n",
        "*   Nettoyer du texte avec des formats incohÃ©rents, des valeurs manquantes, des types mixtes.\n",
        "*   GÃ©rer des jeux de donnÃ©es dÃ©sÃ©quilibrÃ©s ou avec trop peu d'exemples par classe.\n",
        "*   Assurer une compatibilitÃ© fluide avec la bibliothÃ¨que datasets de Hugging Face.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p_o8Cgbvfjua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce module produit trois objets de type Dataset : train_dataset, val_dataset, test_dataset â€“ directement utilisables avec les tokenizers et les Trainer de Hugging Face.\n",
        "\n",
        "Il gÃ©nÃ¨re aussi :\n",
        "\n",
        "Un mapping label â†’ id compatible avec les modÃ¨les de classification.\n",
        "\n",
        "Des messages de log dÃ©taillÃ©s pour comprendre chaque Ã©tape et corriger si besoin.\n",
        "\n",
        "Un fallback automatique si certaines opÃ©rations Ã©chouent (e.g., split sans stratification).\n",
        "\n",
        "GrÃ¢ce Ã  cette logique :\n",
        "\n",
        "Le modÃ¨le ne se base jamais sur des donnÃ©es corrompues ou mal Ã©tiquetÃ©es.\n",
        "\n",
        "Lâ€™entraÃ®nement est plus stable, reproductible et transparent."
      ],
      "metadata": {
        "id": "1m1vP8V4fngl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "Ce module est un vÃ©ritable pont entre le monde brut des donnÃ©es rÃ©elles et les exigences rigoureuses du machine learning moderne. Il incarne une bonne pratique fondamentale en IA : rendre le traitement des donnÃ©es automatique, sÃ©curisÃ© et traÃ§able.\n",
        "\n",
        "**BÃ©nÃ©fices mÃ©tiers :**\n",
        "Permet Ã  une Ã©quipe data ou produit de tester rapidement plusieurs sources de donnÃ©es sans ajustement manuel.\n",
        "\n",
        "RÃ©duit considÃ©rablement le risque d'erreur humaine lors de lâ€™exploration et la prÃ©paration des donnÃ©es.\n",
        "\n",
        "AccÃ©lÃ¨re la mise en production ou le prototypage dâ€™outils IA centrÃ©s sur le langage naturel.\n",
        "\n",
        "En somme, data_modules.py est un accÃ©lÃ©rateur de projet IA, une brique essentielle pour garantir que \"garbage in â‰  garbage out\"."
      ],
      "metadata": {
        "id": "l1taFg01fvpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. **Module ModÃ¨le - model_modules.py**"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le fichier model_modules.py reprÃ©sente la brique stratÃ©gique du projet IA, dÃ©diÃ©e Ã  la configuration, lâ€™adaptation et lâ€™entraÃ®nement du modÃ¨le de classification via une approche parameter-efficient (fine-tuning avec LoRA). Il encapsule l'ensemble des dÃ©cisions techniques critiques liÃ©es au modÃ¨le, au tokenizer, aux paramÃ¨tres d'entraÃ®nement, et Ã  la logique d'Ã©valuation, dans une classe modulaire, claire et facilement maintenable.\n",
        "\n"
      ],
      "metadata": {
        "id": "E0vCHuPWg1CT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intention et DÃ©marche**"
      ],
      "metadata": {
        "id": "T6ZXCsv1hCRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Objectifs du module ModelManager :**\n",
        "\n",
        "\n",
        "\n",
        "*   Centraliser la configuration du tokenizer et du modÃ¨le Hugging Face.\n",
        "*   Appliquer LoRA (Low-Rank Adaptation) pour une fine-tuning efficace sur GPU ou CPU.\n",
        "*   ParamÃ©trer finement lâ€™entraÃ®nement, en prenant en compte la gestion mÃ©moire, les mÃ©triques pertinentes, et les stratÃ©gies de logging/saving.\n",
        "*   Instancier Trainer de Hugging Face de maniÃ¨re optimisÃ©e pour une expÃ©rience de fine-tuning fluide et reproductible.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Pourquoi cette structuration ?**\n",
        "\n",
        "\n",
        "\n",
        "*   Favorise la rÃ©utilisabilitÃ© (nouveau modÃ¨le ou dataset = mÃªmes mÃ©thodes).\n",
        "*   RÃ©pond aux contraintes de performance (GPU limitÃ©, petit batch).\n",
        "*   Simplifie la traÃ§abilitÃ© des expÃ©riences (logs + Ã©valuation centralisÃ©e).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0V0DBnLmg-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "# model_modules.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from typing import Dict, Any\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "class ModelManager:\n",
        "    \"\"\"Gestion du modÃ¨le et de l'entraÃ®nement avec logs avancÃ©s\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.peft_model = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        \"\"\"Configuration du tokenizer\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        return self.tokenizer\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        \"\"\"Configuration du modÃ¨le avec LoRA\"\"\"\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=self.config.lora_r,\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_lin\", \"v_lin\"] if \"distilbert\" in self.config.model_name.lower() else [\"query\", \"value\"]\n",
        "        )\n",
        "\n",
        "        self.peft_model = get_peft_model(base_model, lora_config)\n",
        "        return self.peft_model\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"Tokenisation des exemples\"\"\"\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.config.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def setup_training_args(self, output_dir=\"outputs/runs\"):\n",
        "        \"\"\"Configuration complÃ¨te des TrainingArguments avec logs\"\"\"\n",
        "        return TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            logging_dir=output_dir + \"/logs\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"accuracy\",\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            gradient_checkpointing=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=2,\n",
        "            logging_steps=50,\n",
        "            eval_steps=1,\n",
        "            save_steps=1\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        \"\"\"Configuration du trainer avec arguments optimisÃ©s\"\"\"\n",
        "        training_args = self.setup_training_args()\n",
        "\n",
        "        self.trainer = Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics\n",
        "        )\n",
        "        return self.trainer\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_metrics(eval_pred):\n",
        "        \"\"\"Calcul des mÃ©triques\"\"\"\n",
        "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = torch.argmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, predictions),\n",
        "            \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
        "            \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
        "            \"precision\": precision_score(labels, predictions, average=\"weighted\"),\n",
        "            \"recall\": recall_score(labels, predictions, average=\"weighted\")\n",
        "        }"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9420b91d-ca3e-4a45-bda2-5a945f3e996b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enjeux et Challenges**"
      ],
      "metadata": {
        "id": "PFmi29tHhjpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Enjeux clÃ©s :**\n",
        "\n",
        "\n",
        "*   Adapter un LLM Ã  une tÃ¢che de classification sans explosion mÃ©moire (grÃ¢ce Ã  LoRA).\n",
        "*   Assurer la robustesse cross-model : prendre en charge DistilBERT, BERT, etc.\n",
        "*   Faciliter le scaling des expÃ©rimentations : simple changement de config = nouveau test reproductible.\n",
        "*   Optimiser la performance mÃ©tier via des mÃ©triques Ã©quilibrÃ©es (f1, accuracy...).\n",
        "\n",
        "\n",
        "**Challenges techniques :**\n",
        "\n",
        "\n",
        "*   Cibler correctement les couches internes Ã  adapter avec LoRA (ex. q_lin, v_lin, ou query, value).\n",
        "*   ParamÃ©trer TrainingArguments de faÃ§on Ã©quilibrÃ©e pour sâ€™adapter Ã  du low compute budget.\n",
        "*   Synchroniser tokenizer, dataset, et modÃ¨le, sans bugs de dimension ou padding.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i9i0oZiBhe7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GrÃ¢ce Ã  ce module :**\n",
        "\n",
        "Le modÃ¨le est LoRA-ready, avec gel des poids de base et focus sur lâ€™adaptation via matrices faibles rangs.\n",
        "\n",
        "Le tokenizer est automatiquement configurÃ©, avec pad_token gÃ©rÃ© pour Ã©viter les erreurs silencieuses.\n",
        "\n",
        "Les mÃ©triques clÃ©s (accuracy, F1, precision, recall) sont calculÃ©es systÃ©matiquement.\n",
        "\n",
        "Lâ€™utilisateur peut entraÃ®ner sur GPU/CPU, avec logging optimisÃ© pour analyse fine.\n",
        "\n",
        "Cette architecture modulaire permet de tester facilement plusieurs modÃ¨les (distilbert, roberta, albert) sans rÃ©Ã©criture de code, et offre un point d'entrÃ©e unique pour tout le pipeline de classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "V3Jiy7o6h9v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "model_modules.py est une colonne vertÃ©brale technique du projet. Il incarne lâ€™exigence de :\n",
        "\n",
        "\n",
        "\n",
        "*   Performance (LoRA),\n",
        "*   LisibilitÃ© (code modulaire),\n",
        "*   Robustesse (fallback CPU, configuration dynamique),\n",
        "*   Ã‰valuation mÃ©tier (mÃ©triques pertinentes dÃ¨s lâ€™entraÃ®nement).\n",
        "\n",
        "\n",
        "Il dÃ©montre comment une bonne ingÃ©nierie modÃ¨le permet Ã  une Ã©quipe IA de construire des prototypes puissants et scalables, tout en gardant la possibilitÃ© de passer rapidement en production ou en dÃ©monstration."
      ],
      "metadata": {
        "id": "SGqSAlpHiJ2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BÃ©nÃ©fice mÃ©tier :**\n",
        "Ce module simplifie le travail des Ã©quipes data et produit en rÃ©duisant drastiquement le coÃ»t de fine-tuning tout en maintenant des performances Ã©levÃ©es. Il est donc un levier stratÃ©gique pour dÃ©mocratiser lâ€™adaptation de LLMs sur des cas spÃ©cifiques (analyse de sentiment, classification thÃ©matique, etc.)."
      ],
      "metadata": {
        "id": "LikcKVbdielx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. **Visualization_modules.py**"
      ],
      "metadata": {
        "id": "qYJhFun3Mws5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le module visualization_modules.py est une composante clÃ© pour lâ€™analyse, lâ€™interprÃ©tation et la communication des rÃ©sultats du projet de fine-tuning dâ€™un LLM. Il centralise les outils de visualisation de la performance (courbes dâ€™entraÃ®nement, prÃ©cision, matrice de confusion, etc.) pour permettre Ã  lâ€™Ã©quipe data â€” mais aussi produit ou mÃ©tier â€” de comprendre comment le modÃ¨le apprend et oÃ¹ il se trompe."
      ],
      "metadata": {
        "id": "7TDCzMFujkX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intention et DÃ©marche**"
      ],
      "metadata": {
        "id": "nSxc1VLWjuua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Objectifs du VisualizationManager :**\n",
        "\n",
        "\n",
        "\n",
        "*   Rendre les courbes dâ€™entraÃ®nement lisibles et interactives via Streamlit.\n",
        "*   Offrir des analyses post-training, comme la matrice de confusion et le rapport de classification.\n",
        "*   Automatiser le suivi des performances, sans besoin de coder chaque fois les visualisations.\n",
        "*   \n",
        "Faciliter la comprÃ©hension mÃ©tier des rÃ©sultats via des graphiques clairs et structurÃ©s.\n",
        "\n",
        "\n",
        "**Pourquoi ce module est crucial ?**\n",
        "\n",
        "Dans un projet IA, le succÃ¨s ne se mesure pas uniquement par des chiffres. Les visualisations :\n",
        "*   rÃ©vÃ¨lent les biais d'apprentissage (ex. overfitting),\n",
        "*   permettent de communiquer efficacement entre profils techniques et non techniques,\n",
        "*   facilitent la prise de dÃ©cision (changement de modÃ¨le,ajustement hyperparamÃ¨tres...).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JuDE7Sq7jrvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestion des visualisations d'entraÃ®nement et d'Ã©valuation\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str):\n",
        "        \"\"\"Affiche les courbes d'entraÃ®nement depuis les logs\"\"\"\n",
        "        try:\n",
        "            # Chemin vers le fichier de logs\n",
        "            log_file = Path(log_dir) / \"trainer_state.json\"\n",
        "\n",
        "            if not log_file.exists():\n",
        "                st.warning(\"Fichier de logs non trouvÃ©\")\n",
        "                return\n",
        "\n",
        "            # Chargement des logs\n",
        "            with open(log_file, 'r') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            # Extraction des mÃ©triques\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"Aucune donnÃ©e d'entraÃ®nement trouvÃ©e\")\n",
        "                return\n",
        "\n",
        "            # PrÃ©paration des donnÃ©es\n",
        "            epochs = []\n",
        "            train_loss = []\n",
        "            eval_loss = []\n",
        "            eval_accuracy = []\n",
        "            learning_rates = []\n",
        "\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_accuracy.append(entry.get('eval_accuracy', 0))\n",
        "                    learning_rates.append(entry.get('learning_rate', 0))\n",
        "                elif 'loss' in entry:\n",
        "                    train_loss.append(entry.get('loss', 0))\n",
        "\n",
        "            # CrÃ©ation des graphiques\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle('ðŸ“ˆ Ã‰volution de l\\'entraÃ®nement', fontsize=16)\n",
        "\n",
        "            # Loss\n",
        "            if train_loss:\n",
        "                axes[0, 0].plot(range(len(train_loss)), train_loss, 'b-', label='Train Loss', marker='o')\n",
        "            if eval_loss:\n",
        "                axes[0, 0].plot(epochs[:len(eval_loss)], eval_loss, 'r-', label='Eval Loss', marker='s')\n",
        "            axes[0, 0].set_title('Perte (Loss)')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Accuracy\n",
        "            if eval_accuracy:\n",
        "                axes[0, 1].plot(epochs[:len(eval_accuracy)], eval_accuracy, 'g-', label='Accuracy', marker='^')\n",
        "            axes[0, 1].set_title('PrÃ©cision')\n",
        "            axes[0, 1].set_xlabel('Epoch')\n",
        "            axes[0, 1].set_ylabel('Accuracy')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # Learning Rate\n",
        "            if learning_rates:\n",
        "                axes[1, 0].plot(epochs[:len(learning_rates)], learning_rates, 'orange', marker='d')\n",
        "            axes[1, 0].set_title('Learning Rate')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('LR')\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # RÃ©sumÃ©\n",
        "            if eval_accuracy:\n",
        "                axes[1, 1].text(0.1, 0.5,\n",
        "                               f\"DerniÃ¨re prÃ©cision: {eval_accuracy[-1]:.4f}\\n\"\n",
        "                               f\"Meilleure prÃ©cision: {max(eval_accuracy):.4f}\\n\"\n",
        "                               f\"Epoch: {len(epochs)}\",\n",
        "                               fontsize=12, verticalalignment='center')\n",
        "            axes[1, 1].set_title('RÃ©sumÃ©')\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur lors de l'affichage des courbes: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names):\n",
        "        \"\"\"Affiche la matrice de confusion\"\"\"\n",
        "        try:\n",
        "            # PrÃ©dictions\n",
        "            preds_output = trainer.predict(test_dataset)\n",
        "            preds = preds_output.predictions.argmax(axis=1)\n",
        "            labels = preds_output.label_ids\n",
        "\n",
        "            # Calcul de la matrice\n",
        "            cm = confusion_matrix(labels, preds)\n",
        "\n",
        "            # Affichage avec seaborn\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax)\n",
        "            ax.set_title('Matrice de Confusion')\n",
        "            ax.set_xlabel('PrÃ©dictions')\n",
        "            ax.set_ylabel('Vraies Ã©tiquettes')\n",
        "\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Rapport de classification\n",
        "            st.subheader(\"ðŸ“Š Rapport de Classification\")\n",
        "            report = classification_report(labels, preds, target_names=label_names, output_dict=True)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.dataframe(report_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur matrice de confusion: {e}\")"
      ],
      "metadata": {
        "id": "oMZt-WwIMwez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70afbb08-4488-44d3-d91e-fe42523d201e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting visualization_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enjeux et Challenges**"
      ],
      "metadata": {
        "id": "_ycovcgRkfPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Enjeux principaux :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Rendre accessibles les logs de Hugging Face Ã  travers une UI conviviale.\n",
        "*   Permettre une analyse rapide et fiable des prÃ©dictions du modÃ¨le sur le jeu de test.\n",
        "*   CrÃ©er des visualisations robustes qui ne plantent pas si un fichier est absent ou mal formÃ©.\n",
        "\n",
        "\n",
        "**DifficultÃ©s techniques :**\n",
        "\n",
        "\n",
        "*   Lecture et structuration des logs (trainer_state.json) : il faut interprÃ©ter les Ã©tapes d'entraÃ®nement parfois dÃ©sy\n",
        "\n",
        "\n",
        "*   Synchronisation des mÃ©triques (loss, accuracy, learning\n",
        "rate) sur plusieurs epochs.\n",
        "\n",
        "*   Adaptation de matplotlib et seaborn Ã  lâ€™environnement Streamlit.\n",
        "*   Affichage conditionnel (fallbacks, gestion dâ€™erreurs utilisateur ou absence de logs).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SufjM84RkdQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GrÃ¢ce Ã  ce module :**\n",
        "\n",
        "Les courbes de loss et dâ€™accuracy montrent si le modÃ¨le converge correctement ou pas.\n",
        "\n",
        "Le learning rate peut Ãªtre analysÃ© pour dÃ©tecter un mauvais taux dâ€™apprentissage.\n",
        "\n",
        "La matrice de confusion permet dâ€™identifier les classes les plus confondues.\n",
        "\n",
        "Le rapport de classification offre une vue granulÃ©e sur la prÃ©cision, le rappel et le F1-score de chaque classe.\n",
        "\n",
        "Ces sorties visuelles sont essentielles pour diagnostiquer les faiblesses du modÃ¨le (ex. biais vers la classe majoritaire, difficultÃ© Ã  dÃ©tecter certaines Ã©motions/sentiments)."
      ],
      "metadata": {
        "id": "XoTMpu1rlJMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "[visualization_modules.py] agit comme un miroir du comportement du modÃ¨le. Câ€™est un pont entre lâ€™apprentissage automatique et la comprÃ©hension humaine.\n",
        "\n",
        "Il permet :\n",
        "\n",
        "\n",
        "*   un monitoring transparent de l'entraÃ®nement,\n",
        "*   une analyse fine des erreurs,\n",
        "*   et une prise de dÃ©cision Ã©clairÃ©e sur les prochaines Ã©tapes (plus de donnÃ©es, changement de modÃ¨le, etc.).\n",
        "\n",
        "\n",
        "\n",
        "En rendant les rÃ©sultats lisibles et interactifs, ce module transforme un projet IA technique en outil intelligible et valorisable, aussi bien pour des data scientists que pour des dÃ©cideurs.\n",
        "\n"
      ],
      "metadata": {
        "id": "5QsD2dF2lTSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IntÃ©rÃªt mÃ©tier :**\n",
        "Ce module permet aux parties prenantes non techniques (produit, marketing, direction) de comprendre la valeur et les limites du modÃ¨le, favorisant ainsi lâ€™adoption, la confiance et les dÃ©cisions stratÃ©giques basÃ©es sur la donnÃ©e.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1n1zZTClo0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. **qa_modules.py**"
      ],
      "metadata": {
        "id": "1HSXVYNkNc-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le module qa_modules.py constitue le pilier de la composante de recherche et de gÃ©nÃ©ration de rÃ©ponses contextuelles du projet. Il met en Å“uvre une recherche sÃ©mantique intelligente, permettant de rÃ©cupÃ©rer les documents les plus pertinents Ã  une requÃªte utilisateur. Câ€™est une brique essentielle pour assurer que lâ€™agent IA ne rÃ©ponde pas de maniÃ¨re gÃ©nÃ©rique, mais bien en sâ€™appuyant sur le contexte pertinent extrait des donnÃ©es."
      ],
      "metadata": {
        "id": "COTLVwUdm7ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intention et DÃ©marche**"
      ],
      "metadata": {
        "id": "xCZRxJpSCQ2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**L'objectif de ce module est double :**\n",
        "\n",
        "- Indexer un corpus de textes avec un modÃ¨le dâ€™embeddings (SentenceTransformer) de maniÃ¨re Ã  pouvoir retrouver rapidement les passages les plus proches sÃ©mantiquement dâ€™une question ou dâ€™un texte.\n",
        "\n",
        "- Fournir une interface simple pour interroger ce corpus, rÃ©cupÃ©rer les rÃ©sultats les plus pertinents, et les utiliser pour alimenter des prompts dans une Ã©tape de gÃ©nÃ©ration (RAG â€“ Retrieval-Augmented Generation).\n",
        "\n"
      ],
      "metadata": {
        "id": "n3XsARR6CNVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ã‰tapes clÃ©s de la dÃ©marche :**\n",
        "\n",
        "\n",
        "\n",
        "*   Chargement dâ€™un modÃ¨le lÃ©ger (all-MiniLM-L6-v2) pour encoder les textes.\n",
        "*   Indexation des documents via fit(), produisant un tableau dâ€™embeddings.\n",
        "*   Interrogation via query(), comparant un embedding de la question aux documents via cosine_similarity.\n",
        "*   Extraction des meilleurs rÃ©sultats (top_k) avec leurs labels et scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "7HCljPBZCU5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import streamlit as st\n",
        "\n",
        "class QAModule:\n",
        "    \"\"\"Module de recherche et Q&A basÃ© sur sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"Initialisation avec modÃ¨le d'embedding\"\"\"\n",
        "        try:\n",
        "            self.encoder = SentenceTransformer(model_name)\n",
        "            self.corpus_embeddings = None\n",
        "            self.corpus_texts = []\n",
        "            self.labels = []\n",
        "            self.model_name = model_name\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur chargement modÃ¨le Q&A: {e}\")\n",
        "            # Fallback\n",
        "            self.encoder = None\n",
        "            self.model_name = \"fallback\"\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        \"\"\"Indexe le dataset pour la recherche\"\"\"\n",
        "        if self.encoder is None:\n",
        "            st.warning(\"Module Q&A non disponible\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.corpus_texts = [item['text'] for item in dataset]\n",
        "            self.labels = [item['label_id'] for item in dataset]\n",
        "\n",
        "            with st.spinner(\"ðŸ“Š Indexation des donnÃ©es pour la recherche...\"):\n",
        "                self.corpus_embeddings = self.encoder.encode(\n",
        "                    self.corpus_texts,\n",
        "                    convert_to_tensor=False,\n",
        "                    show_progress_bar=True\n",
        "                )\n",
        "            st.success(f\"âœ… {len(self.corpus_texts)} Ã©lÃ©ments indexÃ©s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur indexation Q&A: {e}\")\n",
        "\n",
        "    def query(self, question: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Recherche les textes les plus similaires Ã  la question\"\"\"\n",
        "        if self.encoder is None or self.corpus_embeddings is None:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            question_embedding = self.encoder.encode([question], convert_to_tensor=False)\n",
        "            similarities = cosine_similarity(question_embedding, self.corpus_embeddings)[0]\n",
        "\n",
        "            # Top K indices\n",
        "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "            results = []\n",
        "            for idx in top_indices:\n",
        "                results.append({\n",
        "                    \"text\": self.corpus_texts[idx],\n",
        "                    \"label_id\": int(self.labels[idx]),\n",
        "                    \"score\": float(similarities[idx]),\n",
        "                    \"rank\": len(results) + 1\n",
        "                })\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur recherche Q&A: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Statistiques du module Q&A\"\"\"\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"indexed_items\": len(self.corpus_texts),\n",
        "            \"embedding_dim\": len(self.corpus_embeddings[0]) if self.corpus_embeddings else 0\n",
        "        }"
      ],
      "metadata": {
        "id": "CdRC_3s7Ncy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b582a6ea-a03b-4fe9-fdd5-98f0e0bb017a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting qa_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enjeux et Challenges**\n",
        "**Enjeux :**\n",
        "- Obtenir des rÃ©sultats contextuels de qualitÃ© pour amÃ©liorer la gÃ©nÃ©ration (rÃ©duction des hallucinations).\n",
        "\n",
        "- Permettre un accÃ¨s rapide Ã  lâ€™information Ã  partir dâ€™un corpus non structurÃ©.\n",
        "\n",
        "- IntÃ©grer un mÃ©canisme simple mais robuste de RAG, sans passer par des solutions coÃ»teuses ou complexes (ex : Pinecone).\n",
        "\n",
        "**Challenges :**\n",
        "- La qualitÃ© des embeddings : il faut un bon Ã©quilibre entre lÃ©gÃ¨retÃ© du modÃ¨le (pour la vitesse) et pertinence sÃ©mantique.\n",
        "\n",
        "- La gestion des grands corpus : encode() peut devenir lent ou gourmand si le dataset est volumineux.\n",
        "\n",
        "- La gestion des erreurs : si lâ€™encodage Ã©choue, tout le systÃ¨me de gÃ©nÃ©ration contextuelle est compromis.\n",
        "\n",
        "- Rendre le tout accessible via Streamlit sans alourdir lâ€™UX."
      ],
      "metadata": {
        "id": "voqcriP0C-co"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L'utilisation de ce module permet de :**\n",
        "\n",
        "- Identifier les documents les plus pertinents pour enrichir une requÃªte utilisateur.\n",
        "\n",
        "- GÃ©nÃ©rer une base contextuelle solide pour la gÃ©nÃ©ration de texte contrÃ´lÃ©e et informÃ©e.\n",
        "\n",
        "- Alimenter des tableaux de bord ou des fonctions de rÃ©ponse intelligente dans lâ€™app (comme un chatbot enrichi ou un moteur dâ€™aide).\n",
        "\n",
        "La mÃ©thode query() renvoie une liste classÃ©e de passages textuels, avec leur score de similaritÃ©, leur Ã©tiquette, et leur rang. Cette approche est bien plus efficace que de se baser uniquement sur des mots-clÃ©s ou des rÃ¨gles fixes.\n",
        "\n"
      ],
      "metadata": {
        "id": "erQoQCPSDRDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "`qa_modules.py` incarne lâ€™intelligence contextuelle du projet :\n",
        "il permet Ã  un modÃ¨le de gÃ©nÃ©ration de ne pas inventer des faits, mais de sâ€™appuyer sur des passages vÃ©rifiÃ©s, proches sÃ©mantiquement de la question.\n",
        "\n",
        "- Il est simple, modulaire et efficace :\n",
        "\n",
        "- Il ne dÃ©pend pas dâ€™un back-end lourd.\n",
        "\n",
        "- Il est facilement extensible (autres modÃ¨les dâ€™embeddings, base vectorielle plus avancÃ©eâ€¦).\n",
        "\n",
        "- Il augmente drastiquement la pertinence mÃ©tier des rÃ©ponses gÃ©nÃ©rÃ©es par le modÃ¨le LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "j1MBtYWuDc9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IntÃ©rÃªt mÃ©tier :**\n",
        "Ce module est particuliÃ¨rement utile dans des cas dâ€™usage tels que :\n",
        "\n",
        "- FAQ automatisÃ©es intelligentes\n",
        "\n",
        "- hatbots dâ€™assistance documentÃ©e\n",
        "\n",
        "- Recherche dâ€™information rapide pour agents internes\n",
        "\n",
        "- SystÃ¨mes de RAG (Retrieval-Augmented Generation) en production\n",
        "\n",
        "En clair, ce module transforme un modÃ¨le de gÃ©nÃ©ration standard en assistant intelligent, documentÃ© et adaptÃ© au contexte utilisateur."
      ],
      "metadata": {
        "id": "fGaqFer3EHQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. **Module Knowledge Base - knowledge_modules.py**"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le module `knowledge_modules.py` constitue une base de connaissances embarquÃ©e â€” une alternative lÃ©gÃ¨re aux mÃ©thodes classiques de retrieval par embeddings. Il est fondÃ© sur des techniques simples de recherche textuelle par similaritÃ© lexicale, permettant dâ€™enrichir les rÃ©ponses gÃ©nÃ©rÃ©es par un modÃ¨le LLM avec des faits clÃ©s issus dâ€™un corpus prÃ©dÃ©fini.\n",
        "\n"
      ],
      "metadata": {
        "id": "5Emsl3kTEyJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intention et DÃ©marche**"
      ],
      "metadata": {
        "id": "9sVCI7IpE-f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Ce module vise Ã  :**\n",
        "\n",
        "- Fournir une base de connaissances interprÃ©table, personnalisable et rapide Ã  intÃ©grer, sans dÃ©pendance Ã  des bibliothÃ¨ques externes lourdes.\n",
        "\n",
        "- RÃ©cupÃ©rer les faits les plus pertinents par rapport Ã  une requÃªte utilisateur, Ã  lâ€™aide dâ€™un score de similaritÃ© Jaccard basÃ© sur les mots communs.\n",
        "\n",
        "- Proposer un fallback utile dans des environnements Ã  faibles ressources (CPU only) ou sans accÃ¨s Internet.\n",
        "\n",
        "**DÃ©marche :**\n",
        "- Initialisation manuelle dâ€™une base de 10 faits scientifiques sur le climat (dans setup_knowledge_base()).\n",
        "\n",
        "- Analyse dâ€™une requÃªte utilisateur (via find_context()) par tokenisation simple + scoring Jaccard.\n",
        "\n",
        "- Tri des rÃ©sultats et renvoi des top_k plus pertinents si leur score dÃ©passe un seuil minimal.\n",
        "\n",
        "- Ajout dynamique possible de nouveaux faits (via add_knowledge()).\n",
        "\n"
      ],
      "metadata": {
        "id": "n_cytQ25E7JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le rÃ©chauffement climatique est principalement causÃ© par les Ã©missions de gaz Ã  effet de serre d'origine humaine.\",\n",
        "            \"Les Ã©nergies renouvelables comme le solaire et l'Ã©olien sont essentielles pour dÃ©carboner notre Ã©conomie.\",\n",
        "            \"La dÃ©forestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports reprÃ©sente environ 24% des Ã©missions mondiales de gaz Ã  effet de serre.\",\n",
        "            \"L'amÃ©lioration de l'efficacitÃ© Ã©nergÃ©tique des bÃ¢timents peut rÃ©duire jusqu'Ã  50% de leur consommation.\",\n",
        "            \"L'agriculture durable et rÃ©gÃ©nÃ©ratrice peut sÃ©questrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les ocÃ©ans absorbent 25% du CO2 atmosphÃ©rique mais s'acidifient, menaÃ§ant les Ã©cosystÃ¨mes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises Ã  rÃ©duire leurs Ã©missions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'attÃ©nuation des Ã©missions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralitÃ© carbone.\"\n",
        "        ]\n",
        "        print(\"âœ… Base de connaissances initialisÃ©e avec recherche par mots-clÃ©s\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similaritÃ© textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similaritÃ© basÃ© sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score dÃ©croissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"âœ… Nouvelle connaissance ajoutÃ©e: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb99d14e-56e0-4cb8-c5a1-9dd68ecc14a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enjeux et Challenges**"
      ],
      "metadata": {
        "id": "1erCbaJtFkoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Enjeux :**\n",
        "- RÃ©pondre rapidement Ã  des questions frÃ©quentes Ã  partir dâ€™un corpus contrÃ´lÃ©.\n",
        "\n",
        "- Permettre Ã  un LLM de gÃ©nÃ©rer du texte documentÃ© et prÃ©cis, mÃªme sans pipeline de RAG complet.\n",
        "\n",
        "- Ã‰viter les hallucinations en injectant des faits fiables dans les prompts.\n",
        "\n",
        "**Challenges :**\n",
        "- La limitation du matching lexical : sans embeddings, la pertinence sÃ©mantique est rÃ©duite.\n",
        "\n",
        "- Lâ€™approche ne gÃ¨re pas la polysÃ©mie ou la synonymie, ce qui peut faire rater des rÃ©sultats importants.\n",
        "\n",
        "- La base doit Ãªtre structurÃ©e Ã  la main, ce qui peut devenir contraignant Ã  grande Ã©chelle.\n",
        "\n",
        "- Le seuil de pertinence (score > 0.1) est empirique et nÃ©cessite un ajustement selon les cas.\n",
        "\n"
      ],
      "metadata": {
        "id": "TEsFefgFFhKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce module renvoie une liste triÃ©e des documents les plus proches dâ€™une requÃªte, ce qui peut Ãªtre utilisÃ© pour :\n",
        "\n",
        "- Construire un prompt enrichi dans un agent conversationnel.\n",
        "\n",
        "- Justifier une rÃ©ponse automatique (avec la source en annexe).\n",
        "\n",
        "- Alimenter un systÃ¨me de recommandation ou un moteur de suggestion.\n",
        "\n",
        "Il fournit Ã©galement des statistiques sur la base, comme le nombre total de faits et la longueur moyenne des documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "HoNYOARjF4M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "- `knowledge_modules.py` illustre un compromis ingÃ©nieux entre simplicitÃ©, interprÃ©tabilitÃ© et efficacitÃ© :\n",
        "\n",
        "- Il Ã©vite le recours Ã  des modÃ¨les lourds (pas de FAISS, pas de vector store).\n",
        "\n",
        "- Il est parfaitement adaptÃ© pour des MVP, des environnements dÃ©connectÃ©s, ou des prototypes embarquÃ©s.\n",
        "\n",
        "- Il offre une base extensible via lâ€™ajout dynamique de connaissances.\n",
        "\n"
      ],
      "metadata": {
        "id": "WUwKlVPsGD6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**InterprÃ©tation mÃ©tier :**\n",
        "Pour un utilisateur mÃ©tier (ex. : responsable RSE, chargÃ© de mission climat), ce module permet :\n",
        "\n",
        "- De construire un rÃ©fÃ©rentiel de faits vÃ©rifiables, facilement enrichissable.\n",
        "\n",
        "- De garantir que les modÃ¨les gÃ©nÃ©ratifs sâ€™appuient sur des informations validÃ©es, donc cohÃ©rentes avec la politique de lâ€™organisation.\n",
        "\n",
        "- De servir de fallback contextuel si la recherche par embeddings est indisponible.\n",
        "\n",
        "En somme, ce module agit comme un filet de sÃ©curitÃ© cognitif pour lâ€™IA gÃ©nÃ©rative."
      ],
      "metadata": {
        "id": "VDyCNTKZGYo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. **Module Streamlit - streamlit_app.py**"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le fichier streamlit_app.py est l'orchestrateur final du projet Climate Sentiment Analyzer, une application interactive dÃ©veloppÃ©e avec Streamlit qui permet de visualiser, entraÃ®ner, tester et interprÃ©ter un modÃ¨le de classification de sentiments appliquÃ© Ã  des textes sur le climat, tout en intÃ©grant des modules de gÃ©nÃ©ration de contexte et de question/rÃ©ponse."
      ],
      "metadata": {
        "id": "u9v1uCM0G-oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intention et DÃ©marche**"
      ],
      "metadata": {
        "id": "lRoy3D5KHVBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Lâ€™objectif de ce module est dâ€™intÃ©grer de maniÃ¨re fluide tous les composants prÃ©cÃ©demment dÃ©veloppÃ©s (prÃ©traitement, entraÃ®nement, visualisation, prÃ©diction, Q&A, etc.) dans une interface utilisateur accessible, sans compÃ©tences techniques requises.\n",
        "\n",
        "**Les Ã©tapes-clÃ©s de la dÃ©marche :**\n",
        "1. Structuration en classes :\n",
        "\n",
        "- `ClimateAnalyzerApp` : gÃ¨re lâ€™interface utilisateur, les interactions et les affichages.\n",
        "\n",
        "- `PipelineOrchestrator` : sert de point dâ€™entrÃ©e principal du script.\n",
        "\n",
        "2. Navigation par onglets (via selectbox) :\n",
        "\n",
        "- Pipeline Complet\n",
        "\n",
        "- Traitement des donnÃ©es\n",
        "\n",
        "- Gestion du modÃ¨le\n",
        "\n",
        "- Analyse de texte\n",
        "\n",
        "- Q&A intelligent\n",
        "\n",
        "- Visualisations\n",
        "\n",
        "3. Personnalisation UX/UI :\n",
        "\n",
        "- CSS intÃ©grÃ© pour amÃ©liorer lâ€™apparence (gradient, cards, layout responsive).\n",
        "\n",
        "- Affichage dynamique des Ã©tapes, erreurs, barres de progression.\n",
        "\n",
        "4. Optimisation de lâ€™expÃ©rience :\n",
        "\n",
        "- Upload de CSV pour lancer le pipeline.\n",
        "\n",
        "- Lancement Ã  la demande de lâ€™entraÃ®nement.\n",
        "\n",
        "- Visualisation des courbes dâ€™apprentissage.\n",
        "\n",
        "- Interface de question-rÃ©ponse par similaritÃ©.\n",
        "\n",
        "- Analyse fine de texte en temps rÃ©el avec contexte."
      ],
      "metadata": {
        "id": "-G1Q-RMjHSzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "\n",
        "# streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Import des modules\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig, PredictionResult\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from knowledge_modules import KnowledgeBase\n",
        "from visualization_modules import VisualizationManager\n",
        "from qa_modules import QAModule\n",
        "\n",
        "# Configuration Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"ðŸŒ Climate Analyzer - Pipeline Complet\",\n",
        "    page_icon=\"ðŸŒ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# CSS personnalisÃ©\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        padding: 2rem;\n",
        "        border-radius: 15px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "        box-shadow: 0 10px 30px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .metric-card {\n",
        "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 12px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        margin: 1rem 0;\n",
        "        box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n",
        "    }\n",
        "    .stage-card {\n",
        "        background: #f8f9fa;\n",
        "        border-left: 4px solid #007bff;\n",
        "        padding: 1rem;\n",
        "        border-radius: 8px;\n",
        "        margin: 1rem 0;\n",
        "    }\n",
        "    .success-box {\n",
        "        background: #d4edda;\n",
        "        border: 1px solid #c3e6cb;\n",
        "        border-radius: 8px;\n",
        "        padding: 1rem;\n",
        "        margin: 1rem 0;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    \"\"\"Application Streamlit principale\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "        self.knowledge_base = KnowledgeBase()\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.qa_module = QAModule()\n",
        "        self.trained = False\n",
        "        self.trainer = None\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"ExÃ©cution principale de l'application\"\"\"\n",
        "\n",
        "        # Header\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"main-header\">\n",
        "            <h1>ðŸŒ Climate Sentiment Analyzer</h1>\n",
        "            <h3>Pipeline Complet avec Visualisations et Q&A</h3>\n",
        "            <p>Analyse de sentiment climatique avec entraÃ®nement, courbes et recherche intelligente</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Sidebar\n",
        "        st.sidebar.header(\"ðŸŽ›ï¸ Navigation\")\n",
        "\n",
        "        # Navigation principale\n",
        "        app_mode = st.sidebar.selectbox(\n",
        "            \"Mode d'application\",\n",
        "            [\"ðŸš€ Pipeline Complet\", \"ðŸ“Š Data Processing\", \"ðŸ¤– ModÃ¨le\", \"ðŸ” Analyse\", \"â“ Q&A\", \"ðŸ“ˆ Visualisations\"]\n",
        "        )\n",
        "\n",
        "        # Configuration rapide\n",
        "        with st.sidebar.expander(\"âš™ï¸ Configuration\"):\n",
        "            sample_size = st.slider(\"Taille Ã©chantillon\", 1000, 10000, 4000)\n",
        "            epochs = st.slider(\"Epochs\", 1, 5, 3)\n",
        "            show_details = st.checkbox(\"DÃ©tails d'exÃ©cution\", True)\n",
        "\n",
        "        # ExÃ©cution selon le mode\n",
        "        if app_mode == \"ðŸš€ Pipeline Complet\":\n",
        "            self.run_complete_pipeline(sample_size, epochs, show_details)\n",
        "        elif app_mode == \"ðŸ“Š Data Processing\":\n",
        "            self.run_data_processing()\n",
        "        elif app_mode == \"ðŸ¤– ModÃ¨le\":\n",
        "            self.run_model_management()\n",
        "        elif app_mode == \"ðŸ” Analyse\":\n",
        "            self.run_analysis()\n",
        "        elif app_mode == \"â“ Q&A\":\n",
        "            self.run_qa_interface()\n",
        "        elif app_mode == \"ðŸ“ˆ Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self, sample_size, epochs, show_details):\n",
        "        \"\"\"Pipeline complet avec toutes les fonctionnalitÃ©s\"\"\"\n",
        "\n",
        "        st.header(\"ðŸš€ Pipeline Complet avec EntraÃ®nement RÃ©el\")\n",
        "\n",
        "        # Configuration des epochs\n",
        "        self.config.epochs = epochs\n",
        "\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Choisissez un fichier CSV\",\n",
        "            type=['csv'],\n",
        "            key=\"pipeline_upload\"\n",
        "        )\n",
        "\n",
        "        if uploaded_file:\n",
        "            try:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "                st.success(f\"âœ… Fichier chargÃ©: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
        "\n",
        "                with st.expander(\"ðŸ‘€ AperÃ§u des donnÃ©es\"):\n",
        "                    st.dataframe(df.head())\n",
        "\n",
        "                if st.button(\"ðŸš€ Lancer l'entraÃ®nement complet\", type=\"primary\"):\n",
        "                    self.run_real_training(df, sample_size)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"âŒ Erreur lors du chargement: {e}\")\n",
        "\n",
        "    def run_real_training(self, df, sample_size):\n",
        "        \"\"\"EntraÃ®nement rÃ©el avec toutes les fonctionnalitÃ©s\"\"\"\n",
        "\n",
        "        progress_bar = st.progress(0)\n",
        "        status_text = st.empty()\n",
        "\n",
        "        try:\n",
        "            # Ã‰tape 1: Data Processing\n",
        "            status_text.text(\"ðŸ“Š PrÃ©paration des donnÃ©es...\")\n",
        "            train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "            progress_bar.progress(20)\n",
        "\n",
        "            # Ã‰tape 2: Configuration modÃ¨le\n",
        "            status_text.text(\"ðŸ¤– Configuration du modÃ¨le...\")\n",
        "            self.model_manager.setup_tokenizer()\n",
        "            self.model_manager.setup_model(len(self.data_processor.label_mapping))\n",
        "            progress_bar.progress(40)\n",
        "\n",
        "            # Ã‰tape 3: EntraÃ®nement\n",
        "            status_text.text(\"ðŸŽ¯ EntraÃ®nement en cours...\")\n",
        "            self.trainer = self.model_manager.setup_trainer(train_ds, val_ds)\n",
        "            self.trainer.train()\n",
        "            progress_bar.progress(70)\n",
        "\n",
        "            # Ã‰tape 4: Ã‰valuation\n",
        "            status_text.text(\"ðŸ“Š Ã‰valuation du modÃ¨le...\")\n",
        "            metrics = self.trainer.evaluate(test_ds)\n",
        "            progress_bar.progress(85)\n",
        "\n",
        "            # Ã‰tape 5: Configuration Q&A\n",
        "            status_text.text(\"ðŸ” Configuration Q&A...\")\n",
        "            self.qa_module.fit(train_ds)\n",
        "            progress_bar.progress(100)\n",
        "\n",
        "            # Affichage des rÃ©sultats\n",
        "            self.display_training_results(metrics, test_ds)\n",
        "            self.trained = True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"âŒ Erreur dans le pipeline: {str(e)}\")\n",
        "            st.code(str(e))\n",
        "\n",
        "    def display_training_results(self, metrics, test_dataset):\n",
        "        \"\"\"Affichage des rÃ©sultats d'entraÃ®nement\"\"\"\n",
        "\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"success-box\">\n",
        "            <h3>ðŸŽ‰ EntraÃ®nement terminÃ© avec succÃ¨s!</h3>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # MÃ©triques principales\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "        with col1:\n",
        "            st.metric(\"ðŸ“Š Accuracy\", f\"{metrics['eval_accuracy']:.4f}\")\n",
        "        with col2:\n",
        "            st.metric(\"ðŸŽ¯ F1-Score\", f\"{metrics['eval_f1_weighted']:.4f}\")\n",
        "        with col3:\n",
        "            st.metric(\"ðŸ“ Precision\", f\"{metrics['eval_precision']:.4f}\")\n",
        "        with col4:\n",
        "            st.metric(\"ðŸ” Recall\", f\"{metrics['eval_recall']:.4f}\")\n",
        "\n",
        "        # Visualisations\n",
        "        st.subheader(\"ðŸ“ˆ Courbes d'entraÃ®nement\")\n",
        "        self.visualizer.plot_training_curves(\"outputs/runs\")\n",
        "\n",
        "        # Matrice de confusion\n",
        "        label_names = list(self.data_processor.label_mapping.keys())\n",
        "        st.subheader(\"ðŸ” Matrice de Confusion\")\n",
        "        self.visualizer.show_confusion_matrix(self.trainer, test_dataset, label_names)\n",
        "\n",
        "        # Sauvegarde\n",
        "        model_path = \"outputs/final_model\"\n",
        "        self.trainer.save_model(model_path)\n",
        "        st.success(f\"ðŸ“¦ ModÃ¨le sauvegardÃ© dans `{model_path}`\")\n",
        "\n",
        "    def run_qa_interface(self):\n",
        "        \"\"\"Interface de recherche Q&A\"\"\"\n",
        "        st.header(\"â“ Module Questions-RÃ©ponses\")\n",
        "\n",
        "        if not self.trained:\n",
        "            st.warning(\"âš ï¸ Veuillez d'abord entraÃ®ner un modÃ¨le pour utiliser la recherche Q&A\")\n",
        "            return\n",
        "\n",
        "        question = st.text_input(\n",
        "            \"Posez votre question sur le climat :\",\n",
        "            placeholder=\"Ex: Quels sont les impacts du rÃ©chauffement climatique ?\"\n",
        "        )\n",
        "\n",
        "        if question:\n",
        "            results = self.qa_module.query(question, top_k=5)\n",
        "\n",
        "            if results:\n",
        "                st.markdown(\"### ðŸ” RÃ©sultats les plus pertinents :\")\n",
        "                for i, res in enumerate(results, 1):\n",
        "                    st.markdown(f\"\"\"\n",
        "                    <div class=\"stage-card\">\n",
        "                        <h4>{i}. Score: {res['score']:.3f}</h4>\n",
        "                        <p><strong>Texte:</strong> {res['text'][:300]}...</p>\n",
        "                        <p><strong>Label:</strong> {res['label_id']}</p>\n",
        "                    </div>\n",
        "                    \"\"\", unsafe_allow_html=True)\n",
        "            else:\n",
        "                st.info(\"Aucun rÃ©sultat trouvÃ©. Essayez avec des mots-clÃ©s diffÃ©rents.\")\n",
        "\n",
        "    def run_data_processing(self):\n",
        "        \"\"\"Interface de data processing\"\"\"\n",
        "        st.header(\"ðŸ“Š Module Data Processing\")\n",
        "\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Choisissez un fichier CSV\",\n",
        "            type=['csv'],\n",
        "            key=\"data_upload\"\n",
        "        )\n",
        "\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"ðŸ“Š DonnÃ©es chargÃ©es: {df.shape}\")\n",
        "\n",
        "            # DÃ©tection automatique\n",
        "            text_col, label_col = self.data_processor.detect_columns(df)\n",
        "            st.info(f\"ðŸ” Colonnes dÃ©tectÃ©es: Texte='{text_col}', Label='{label_col}'\")\n",
        "\n",
        "            # PrÃ©visualisation\n",
        "            with st.expander(\"ðŸ‘€ AperÃ§u des donnÃ©es\"):\n",
        "                st.dataframe(df.head(10))\n",
        "\n",
        "            if st.button(\"ðŸ“Š PrÃ©parer les donnÃ©es\"):\n",
        "                with st.spinner(\"PrÃ©paration en cours...\"):\n",
        "                    train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df)\n",
        "\n",
        "                    col1, col2, col3 = st.columns(3)\n",
        "                    with col1:\n",
        "                        st.metric(\"ðŸŽ¯ Train\", len(train_ds))\n",
        "                    with col2:\n",
        "                        st.metric(\"ðŸ” Validation\", len(val_ds))\n",
        "                    with col3:\n",
        "                        st.metric(\"ðŸ“Š Test\", len(test_ds))\n",
        "\n",
        "                    # Distribution des labels\n",
        "                    st.subheader(\"ðŸ“ˆ Distribution des labels\")\n",
        "                    label_dist = pd.Series([item['label_id'] for item in train_ds]).value_counts()\n",
        "                    st.bar_chart(label_dist)\n",
        "\n",
        "    def run_model_management(self):\n",
        "        \"\"\"Interface de gestion du modÃ¨le\"\"\"\n",
        "        st.header(\"ðŸ¤– Module Gestion ModÃ¨le\")\n",
        "\n",
        "        if st.button(\"ðŸ”§ Configurer le modÃ¨le\"):\n",
        "            with st.spinner(\"Configuration...\"):\n",
        "                self.model_manager.setup_tokenizer()\n",
        "\n",
        "                st.success(\"âœ… Tokenizer configurÃ©!\")\n",
        "\n",
        "                col1, col2 = st.columns(2)\n",
        "\n",
        "                with col1:\n",
        "                    st.subheader(\"ðŸ“‹ Configuration\")\n",
        "                    config_dict = self.config.to_dict()\n",
        "                    st.json(config_dict)\n",
        "\n",
        "                with col2:\n",
        "                    st.subheader(\"ðŸ” Informations\")\n",
        "                    st.info(\"ðŸ¤– ModÃ¨le: \" + self.config.model_name)\n",
        "                    st.info(\"ðŸ“ Longueur max: \" + str(self.config.max_length))\n",
        "                    st.info(\"ðŸŽ¯ Batch size: \" + str(self.config.batch_size))\n",
        "\n",
        "    def run_analysis(self):\n",
        "        \"\"\"Interface d'analyse\"\"\"\n",
        "        st.header(\"ðŸ” Module Analyse\")\n",
        "\n",
        "        if not self.trained:\n",
        "            st.warning(\"âš ï¸ Veuillez d'abord entraÃ®ner un modÃ¨le\")\n",
        "            return\n",
        "\n",
        "        # Charger le modÃ¨le sauvegardÃ©\n",
        "        model_path = \"outputs/final_model\"\n",
        "        if os.path.exists(model_path):\n",
        "            self.model_manager.peft_model = self.model_manager.setup_model(len(self.data_processor.label_mapping))\n",
        "            self.model_manager.peft_model.load_adapter(model_path)\n",
        "\n",
        "        text_input = st.text_area(\n",
        "            \"Texte Ã  analyser:\",\n",
        "            height=100,\n",
        "            placeholder=\"Entrez votre texte climatique ici...\",\n",
        "            value=\"Le rÃ©chauffement climatique est un problÃ¨me urgent qui nÃ©cessite une action immÃ©diate.\"\n",
        "        )\n",
        "\n",
        "        if st.button(\"ðŸ” PrÃ©dire\", type=\"primary\"):\n",
        "            if text_input.strip():\n",
        "                with st.spinner(\"Analyse en cours...\"):\n",
        "                    result = self.predict_with_saved_model(text_input)\n",
        "                    self.display_analysis_result(result)\n",
        "\n",
        "    def predict_with_saved_model(self, text: str) -> PredictionResult:\n",
        "        \"\"\"PrÃ©diction avec le modÃ¨le entraÃ®nÃ©\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Tokenisation\n",
        "        inputs = self.model_manager.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "        # PrÃ©diction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_manager.peft_model(**inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "            confidence = probs[0][predicted_class].item()\n",
        "\n",
        "        # Mapping inverse\n",
        "        inv_label_map = {v: k for k, v in self.data_processor.label_mapping.items()}\n",
        "        predicted_label = inv_label_map.get(predicted_class, f\"Classe {predicted_class}\")\n",
        "\n",
        "        # Tous les scores\n",
        "        all_scores = {inv_label_map.get(i, f\"Class {i}\"): float(prob) for i, prob in enumerate(probs[0])}\n",
        "\n",
        "        # Contexte\n",
        "        context = self.knowledge_base.find_context(text)\n",
        "\n",
        "        return PredictionResult(\n",
        "            text=text,\n",
        "            predicted_label=predicted_label,\n",
        "            confidence=confidence,\n",
        "            all_scores=all_scores,\n",
        "            context=context,\n",
        "            processing_time=time.time() - start_time\n",
        "        )\n",
        "\n",
        "    def display_analysis_result(self, result: PredictionResult):\n",
        "        \"\"\"Affichage des rÃ©sultats d'analyse\"\"\"\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>ðŸŽ¯ Sentiment</h3>\n",
        "                <h2>{result.predicted_label.title()}</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        with col2:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>ðŸ“Š Confiance</h3>\n",
        "                <h2>{result.confidence:.1%}</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        with col3:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>â±ï¸ Temps</h3>\n",
        "                <h2>{result.processing_time:.2f}s</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Scores dÃ©taillÃ©s\n",
        "        st.subheader(\"ðŸ“Š Scores dÃ©taillÃ©s\")\n",
        "        scores_df = pd.DataFrame([\n",
        "            {\"Sentiment\": k.title(), \"Score\": f\"{v:.2%}\"}\n",
        "            for k, v in result.all_scores.items()\n",
        "        ])\n",
        "        st.dataframe(scores_df, use_container_width=True)\n",
        "\n",
        "        # Contexte\n",
        "        if result.context:\n",
        "            st.subheader(\"ðŸ’¡ Contexte Pertinent\")\n",
        "            for i, ctx in enumerate(result.context, 1):\n",
        "                st.markdown(f\"**{i}.** {ctx}\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        \"\"\"Interface de visualisations\"\"\"\n",
        "        st.header(\"ðŸ“ˆ Module Visualisations\")\n",
        "\n",
        "        if not self.trained:\n",
        "            st.warning(\"âš ï¸ Veuillez d'abord entraÃ®ner un modÃ¨le pour voir les visualisations\")\n",
        "            return\n",
        "\n",
        "        # Visualisations disponibles\n",
        "        viz_option = st.selectbox(\n",
        "            \"Choisir une visualisation\",\n",
        "            [\"ðŸ“ˆ Courbes d'entraÃ®nement\", \"ðŸ” Matrice de confusion\", \"ðŸ“Š MÃ©triques\"]\n",
        "        )\n",
        "\n",
        "        if viz_option == \"ðŸ“ˆ Courbes d'entraÃ®nement\":\n",
        "            self.visualizer.plot_training_curves(\"outputs/runs\")\n",
        "        elif viz_option == \"ðŸ” Matrice de confusion\":\n",
        "            label_names = list(self.data_processor.label_mapping.keys())\n",
        "            self.visualizer.show_confusion_matrix(self.trainer, self.trainer.eval_dataset, label_names)\n",
        "        elif viz_option == \"ðŸ“Š MÃ©triques\":\n",
        "            if self.trainer:\n",
        "                metrics = self.trainer.evaluate()\n",
        "                st.json(metrics)\n",
        "\n",
        "# ========== ORCHESTRATEUR PRINCIPAL ==========\n",
        "class PipelineOrchestrator:\n",
        "    \"\"\"Orchestrateur principal du pipeline\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.app = ClimateAnalyzerApp()\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Lancement de l'application\"\"\"\n",
        "        self.app.run()\n",
        "\n",
        "# ========== UTILISATION ==========\n",
        "if __name__ == \"__main__\":\n",
        "    orchestrator = PipelineOrchestrator()\n",
        "    orchestrator.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e759bc8-53bd-48f6-f4ee-cd46a6d7798d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enjeux et Challenges**"
      ],
      "metadata": {
        "id": "UIuOqTiBH9EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Enjeux :**\n",
        "- Rendre le projet utilisable par tous : chercheurs, dÃ©cideurs, Ã©tudiants...\n",
        "\n",
        "- Permettre un test rapide du modÃ¨le sur des donnÃ©es rÃ©elles.\n",
        "\n",
        "- Fournir une expÃ©rience fluide et complÃ¨te du cycle ML : de la data Ã  la prÃ©diction.\n",
        "\n",
        "- GÃ©rer robustement les erreurs, les exceptions et les chemins absents.\n",
        "\n",
        "**Challenges :**\n",
        "- IntÃ©grer des composants hÃ©tÃ©rogÃ¨nes (PyTorch, HuggingFace, PEFT, Streamlit, Pandasâ€¦).\n",
        "\n",
        "- Garder une structure lisible, modulaire et maintenable malgrÃ© la complexitÃ© croissante.\n",
        "\n",
        "- Assurer une compatibilitÃ© GPU/CPU sans casser le pipeline.\n",
        "\n",
        "- Offrir une expÃ©rience utilisateur intuitive, sans sacrifier la puissance technique."
      ],
      "metadata": {
        "id": "vvurmPkhH7Ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce module donne naissance Ã  une web app de NLP complÃ¨te, dans laquelle on peut :\n",
        "\n",
        "- Charger ses propres donnÃ©es climatiques textuelles.\n",
        "\n",
        "- Lancer un entraÃ®nement LoRA allÃ©gÃ©.\n",
        "\n",
        "- Observer les courbes dâ€™apprentissage (loss, accuracy, LR).\n",
        "\n",
        "- Obtenir des mÃ©triques (f1, precision, recall).\n",
        "\n",
        "- Interroger le modÃ¨le pour des prÃ©dictions individuelles.\n",
        "\n",
        "- Rechercher des Ã©lÃ©ments similaires grÃ¢ce au module Q&A.\n",
        "\n",
        "- Obtenir un contexte sÃ©mantique enrichi via une base de connaissances intÃ©grÃ©e.\n",
        "\n",
        "Le tout sans toucher Ã  une seule ligne de code Python, ce qui en fait un vÃ©ritable outil mÃ©tier."
      ],
      "metadata": {
        "id": "NSszDp0iIHcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "\n",
        "streamlit_app.py est l'aboutissement de tout le projet : unifier les briques IA, orchestrer les Ã©tapes, simplifier lâ€™expÃ©rience utilisateur. Câ€™est ce module qui transforme un ensemble de scripts en produit prÃªt Ã  lâ€™emploi, dÃ©montrant toute la puissance dâ€™un prototype Low-Code + IA pour des enjeux climatiques.\n",
        "\n"
      ],
      "metadata": {
        "id": "p7_mTK6LIUCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**InterprÃ©tation MÃ©tiers**\n",
        "Pour un utilisateur mÃ©tier (chercheur, communicant, analyste RSE) :\n",
        "\n",
        "- Cette interface devient un laboratoire numÃ©rique interactif : il peut expÃ©rimenter avec des jeux de donnÃ©es, tester ses hypothÃ¨ses, comprendre la tonalitÃ© de certains discours.\n",
        "\n",
        "- Elle offre un moyen rapide de valider la perception des messages liÃ©s au climat.\n",
        "\n",
        "- Le module de Q&A permet une exploration sÃ©mantique fine, utile pour identifier les reprÃ©sentations collectives, tensions, ou idÃ©es dominantes."
      ],
      "metadata": {
        "id": "lGrc4-MnIbMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #8. **Script d'Installation - setup_pipeline.py**"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce module est conÃ§u pour automatiser l'installation de toutes les dÃ©pendances nÃ©cessaires au bon fonctionnement du projet d'analyse de sentiment climatique basÃ© sur un pipeline IA complet. L'objectif est de garantir que tout utilisateur puisse configurer son environnement sans erreurs ni oublis, avec une commande unique."
      ],
      "metadata": {
        "id": "qKJOPexFJTP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DÃ©marche technique**\n",
        "Liste des dÃ©pendances critiques :\n",
        "- Le fichier contient une liste structurÃ©e des bibliothÃ¨ques indispensables :\n",
        "\n",
        "- transformers, datasets, torch â†’ pour le fine-tuning des LLMs.\n",
        "\n",
        "- peft, sentence-transformers, faiss-cpu â†’ pour le LoRA et la recherche sÃ©mantique.\n",
        "\n",
        "- streamlit, plotly, matplotlib, seaborn â†’ pour lâ€™interface utilisateur et les visualisations.\n",
        "\n",
        "- scikit-learn, pandas, numpy â†’ pour les mÃ©triques, le traitement de donnÃ©es et les structures fondamentales.\n",
        "\n",
        "Installation dynamique avec subprocess :\n",
        "\n",
        "- Le script utilise subprocess.check_call pour lancer des commandes pip install de faÃ§on indÃ©pendante, module par module.\n",
        "\n",
        "- Si une erreur survient, elle est capturÃ©e, signalÃ©e sans arrÃªter lâ€™installation des autres paquets (try/except).\n",
        "\n",
        "ExÃ©cution autonome :\n",
        "\n",
        "- Le if __name__ == \"__main__\" permet de lancer lâ€™installation avec une seule commande :\n",
        "\n",
        "`python setup_pipeline.py`\n"
      ],
      "metadata": {
        "id": "fysRkseqJbuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_pipeline.py\n",
        "# setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installation complÃ¨te des dÃ©pendances\"\"\"\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"âœ… {package} installÃ©\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"âš ï¸ Erreur avec {package}: {e}\")\n",
        "\n",
        "    print(\"âœ… Installation complÃ¨te terminÃ©e!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56a998e-63d6-4679-b2b9-1b17d2c726c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enjeux et challenges**\n",
        "- Synchronisation des versions : Il est crucial de figer les versions compatibles pour Ã©viter les conflits ou des comportements instables.\n",
        "\n",
        "- Robustesse multiplateforme : Utiliser subprocess rend le script plus portable que des solutions type requirements.txt, surtout en contexte programmatique ou en notebook.\n",
        "\n",
        "- ExpÃ©rience utilisateur : Le module Ã©vite aux utilisateurs dâ€™avoir Ã  gÃ©rer manuellement lâ€™installation, source dâ€™erreurs frÃ©quentes dans les projets IA complexes."
      ],
      "metadata": {
        "id": "BUaKEEjmQEQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**InterprÃ©tation des rÃ©sultats**\n",
        "- Chaque dÃ©pendance installÃ©e avec succÃ¨s affiche un âœ….\n",
        "\n",
        "- En cas de souci (par exemple, dÃ©pendance manquante, connexion, conflit), le message est explicite avec âš ï¸.\n",
        "\n",
        "- Une fois le processus terminÃ©, un message de confirmation final s'affiche :\n",
        "âœ… Installation complÃ¨te terminÃ©e!"
      ],
      "metadata": {
        "id": "UDuxTb0yQLBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "\n",
        "Ce module incarne une meilleure pratique de dÃ©ploiement dans tout projet IA : il centralise, fiabilise et simplifie la mise en place de lâ€™environnement technique. GrÃ¢ce Ã  ce script, toute personne ou Ã©quipe peut rÃ©pliquer l'environnement de dÃ©veloppement en un clic, favorisant la collaboration, la portabilitÃ© et la reproductibilitÃ© des rÃ©sultats."
      ],
      "metadata": {
        "id": "yWQqNG7zQU-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2284ade-2370-4ded-a780-2b7ca78fb0d7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "âœ… transformers>=4.36.0 installÃ©\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "âœ… datasets>=2.16.0 installÃ©\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "âœ… torch>=2.1.0 installÃ©\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "âœ… peft>=0.7.0 installÃ©\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.33.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "âœ… sentence-transformers>=2.2.0 installÃ©\n",
            "Requirement already satisfied: faiss-cpu>=1.7.0 in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "âœ… faiss-cpu>=1.7.0 installÃ©\n",
            "Requirement already satisfied: streamlit>=1.29.0 in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "âœ… streamlit>=1.29.0 installÃ©\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "âœ… plotly>=5.17.0 installÃ©\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "âœ… scikit-learn>=1.3.0 installÃ©\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "âœ… matplotlib>=3.7.0 installÃ©\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "âœ… seaborn>=0.12.0 installÃ©\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "âœ… pandas>=1.5.0 installÃ©\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "âœ… numpy>=1.24.0 installÃ©\n",
            "âœ… Installation complÃ¨te terminÃ©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MEnnZWsOuL5",
        "outputId": "fb259169-f68c-448e-c412-0d04c750a702"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3wskx_fZLEp",
        "outputId": "7591afe9-7690-4cd5-ead7-4906b9467799"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Streamlit + ngrok**"
      ],
      "metadata": {
        "id": "-pZxSn9_Q9kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce script sert Ã  dÃ©marrer une application Streamlit en local et Ã  la rendre accessible via un lien public grÃ¢ce Ã  ngrok, un outil de tunneling trÃ¨s utile en dÃ©veloppement collaboratif, dÃ©monstration ou test depuis un cloud (comme Colab, GCP ou un serveur distant)."
      ],
      "metadata": {
        "id": "Q5BckmEJREOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|  Avantage                        |  Explication                                                            |\n",
        "| --------------------------------- | ------------------------------------------------------------------------- |\n",
        "| **Accessible depuis Internet**    | Vous pouvez tester ou faire une dÃ©mo Ã  distance, mÃªme depuis un notebook. |\n",
        "| **Simple et rapide**              | Pas besoin de configurer un serveur web ou de modifier les DNS.           |\n",
        "| **Compatible Colab**              | Fonctionne parfaitement depuis un environnement Google Colab ou serveur.  |\n",
        "| **Pas besoin d'ouvrir des ports** | IdÃ©al en rÃ©seau dâ€™entreprise ou cloud privÃ© (ports souvent bloquÃ©s).      |\n"
      ],
      "metadata": {
        "id": "NkbykT4rRM29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”§ Lancement Streamlit + ngrok (version corrigÃ©e)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1ï¸âƒ£ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2ï¸âƒ£ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3ï¸âƒ£ Attendre et crÃ©er le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"ðŸš€ Interface Streamlit disponible Ã  :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFyKJzjISjWG",
        "outputId": "14bfeaa0-a3ff-4c2a-f0ad-00eab3da7e37"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "ðŸš€ Interface Streamlit disponible Ã  :\n",
            "NgrokTunnel: \"https://0bfadc390cb7.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette dÃ©marche est indispensable pour dÃ©ployer rapidement et temporairement une interface Streamlit sur le web, sans infrastructure complexe. Elle facilite les dÃ©mos en direct, les tests collaboratifs ou les livraisons rapides de POC (Proof of Concept) IA."
      ],
      "metadata": {
        "id": "_Qoix6etRWpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONCLUSION GLOBALE**"
      ],
      "metadata": {
        "id": "L1DOx7hrSKCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce projet propose un pipeline complet et modulaire pour lâ€™analyse de sentiments climatiques Ã  partir de textes. Il intÃ¨gre plusieurs Ã©tapes clÃ©s : le prÃ©traitement intelligent des donnÃ©es (avec dÃ©tection automatique des colonnes), la modÃ©lisation optimisÃ©e avec LoRA pour fine-tuning allÃ©gÃ©, la visualisation des performances, une interface de prÃ©diction et de contexte, ainsi quâ€™un moteur de questions-rÃ©ponses sÃ©mantiques. Chaque module a Ã©tÃ© pensÃ© pour Ãªtre robuste, rÃ©utilisable et facilement dÃ©ployable grÃ¢ce Ã  Streamlit et ngrok. Les principaux dÃ©fis rencontrÃ©s concernent la gestion des donnÃ©es hÃ©tÃ©rogÃ¨nes, la configuration efficace du modÃ¨le, et l'interprÃ©tabilitÃ© des rÃ©sultats. GrÃ¢ce Ã  des choix techniques adaptÃ©s (PEFT, Streamlit, embeddings), ce projet rend lâ€™intelligence artificielle accessible et interactive, avec des bÃ©nÃ©fices immÃ©diats pour l'exploration, la sensibilisation ou lâ€™analyse d'opinion sur des enjeux environnementaux."
      ],
      "metadata": {
        "id": "T7T_H3XbSOIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et si demain, ces outils devenaient des assistants de dÃ©cision pour la transition Ã©cologique, jusquâ€™oÃ¹ pourrions-nous aller collectivement ?"
      ],
      "metadata": {
        "id": "Nh-bUUfHSQYy"
      }
    }
  ]
}