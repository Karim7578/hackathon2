{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Module Core - core_modules.py"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    \"\"\"Structure pour les r√©sultats de pr√©diction\"\"\"\n",
        "    text: str\n",
        "    predicted_label: str\n",
        "    confidence: float\n",
        "    all_scores: Dict[str, float]\n",
        "    context: Optional[List[str]] = None\n",
        "    processing_time: float = 0.0\n",
        "\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralis√©e\"\"\"\n",
        "    def __init__(self):\n",
        "        self.model_name = \"distilbert-base-uncased\"\n",
        "        self.max_length = 256\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 2e-4\n",
        "        self.epochs = 3\n",
        "        self.lora_r = 16\n",
        "        self.lora_alpha = 32\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            'model_name': self.model_name,\n",
        "            'max_length': self.max_length,\n",
        "            'batch_size': self.batch_size,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'epochs': self.epochs,\n",
        "            'device': str(self.device),\n",
        "            'lora_config': {'r': self.lora_r, 'alpha': self.lora_alpha}\n",
        "        }"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71da521d-c3a9-44bb-9c19-7cf6dbba61e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Module Data Processing - data_modules.py"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "\n",
        "# data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Gestion centralis√©e du traitement des donn√©es avec gestion robuste des erreurs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        \"\"\"D√©tection automatique des colonnes texte et label avec validation\"\"\"\n",
        "        print(f\"üîç D√©tection des colonnes sur {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
        "        print(f\"üìã Colonnes disponibles: {list(df.columns)}\")\n",
        "\n",
        "        text_keywords = ['self_text', 'text', 'content', 'message', 'comment', 'body', 'description']\n",
        "        label_keywords = ['comment_sentiment', 'sentiment', 'label', 'category', 'class', 'target']\n",
        "\n",
        "        # Recherche intelligente\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        # Recherche par mots-cl√©s\n",
        "        for col in df.columns:\n",
        "            col_lower = str(col).lower()\n",
        "\n",
        "            # Recherche colonne texte\n",
        "            if not text_col:\n",
        "                for keyword in text_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        text_col = col\n",
        "                        break\n",
        "\n",
        "            # Recherche colonne label\n",
        "            if not label_col:\n",
        "                for keyword in label_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        label_col = col\n",
        "                        break\n",
        "\n",
        "        # Fallback intelligent pour la colonne texte\n",
        "        if not text_col:\n",
        "            string_cols = []\n",
        "            for col in df.columns:\n",
        "                try:\n",
        "                    # V√©rifier si la colonne contient principalement du texte\n",
        "                    sample = df[col].dropna().head(100)\n",
        "                    if len(sample) > 0:\n",
        "                        # Convertir en string et calculer la longueur moyenne\n",
        "                        sample_str = sample.astype(str)\n",
        "                        avg_length = sample_str.str.len().mean()\n",
        "                        if avg_length > 10:  # Textes probablement plus longs que 10 caract√®res\n",
        "                            string_cols.append((col, avg_length))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if string_cols:\n",
        "                # Prendre la colonne avec le texte le plus long en moyenne\n",
        "                text_col = max(string_cols, key=lambda x: x[1])[0]\n",
        "            else:\n",
        "                # Last resort: premi√®re colonne object\n",
        "                object_cols = df.select_dtypes(include=['object']).columns\n",
        "                if len(object_cols) > 0:\n",
        "                    text_col = object_cols[0]\n",
        "\n",
        "        # Fallback pour la colonne label\n",
        "        if not label_col:\n",
        "            # Chercher une colonne avec peu de valeurs uniques (potentiel label)\n",
        "            for col in df.columns:\n",
        "                if col != text_col:\n",
        "                    try:\n",
        "                        unique_count = df[col].nunique()\n",
        "                        total_count = len(df[col].dropna())\n",
        "                        if total_count > 0 and unique_count < min(20, total_count * 0.1):\n",
        "                            label_col = col\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Si toujours pas trouv√©, prendre la derni√®re colonne\n",
        "            if not label_col:\n",
        "                label_col = df.columns[-1]\n",
        "\n",
        "        print(f\"‚úÖ Colonnes d√©tect√©es: Text='{text_col}', Label='{label_col}'\")\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text_column(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Nettoyage robuste d'une colonne texte\"\"\"\n",
        "        try:\n",
        "            # Convertir en string d'abord\n",
        "            cleaned = series.astype(str)\n",
        "\n",
        "            # Remplacer les valeurs probl√©matiques\n",
        "            cleaned = cleaned.replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "            # Supprimer les espaces\n",
        "            cleaned = cleaned.str.strip()\n",
        "\n",
        "            # Remplacer les cha√Ænes vides par NaN\n",
        "            cleaned = cleaned.replace('', pd.NA)\n",
        "\n",
        "            return cleaned\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur nettoyage texte: {e}\")\n",
        "            # Fallback: conversion simple\n",
        "            return series.astype(str)\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        \"\"\"Pr√©paration des datasets avec validation robuste\"\"\"\n",
        "\n",
        "        print(f\"üìä Pr√©paration des datasets - Taille originale: {df.shape}\")\n",
        "\n",
        "        # D√©tection des colonnes\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "\n",
        "        if not self.text_col or not self.label_col:\n",
        "            raise ValueError(f\"‚ùå Impossible de d√©tecter les colonnes: text='{self.text_col}', label='{self.label_col}'\")\n",
        "\n",
        "        # Extraction et copie des colonnes n√©cessaires\n",
        "        try:\n",
        "            df_work = df[[self.text_col, self.label_col]].copy()\n",
        "        except KeyError as e:\n",
        "            print(f\"‚ùå Colonnes manquantes: {e}\")\n",
        "            print(f\"Colonnes disponibles: {list(df.columns)}\")\n",
        "            raise\n",
        "\n",
        "        # Renommer les colonnes\n",
        "        df_work.columns = ['text', 'label']\n",
        "\n",
        "        print(f\"üìã Avant nettoyage: {len(df_work)} lignes\")\n",
        "\n",
        "        # Nettoyage robuste des donn√©es\n",
        "        # 1. Nettoyage de la colonne texte\n",
        "        df_work['text'] = self.clean_text_column(df_work['text'])\n",
        "\n",
        "        # 2. Nettoyage de la colonne label\n",
        "        df_work['label'] = df_work['label'].astype(str).str.strip()\n",
        "        df_work['label'] = df_work['label'].replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "        # 3. Suppression des lignes avec des valeurs manquantes\n",
        "        initial_size = len(df_work)\n",
        "        df_work = df_work.dropna()\n",
        "        print(f\"üßπ Apr√®s suppression des NaN: {len(df_work)} lignes (supprim√©: {initial_size - len(df_work)})\")\n",
        "\n",
        "        # 4. Filtrage des textes trop courts (de mani√®re s√©curis√©e)\n",
        "        try:\n",
        "            # V√©rifier que nous avons bien des strings\n",
        "            df_work['text'] = df_work['text'].astype(str)\n",
        "\n",
        "            # Filtrer les textes trop courts\n",
        "            mask = df_work['text'].str.len() > 5\n",
        "            df_work = df_work[mask]\n",
        "            print(f\"üìù Apr√®s filtrage textes courts: {len(df_work)} lignes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du filtrage des textes: {e}\")\n",
        "            # Continuer sans filtrage si erreur\n",
        "\n",
        "        # V√©rification finale\n",
        "        if len(df_work) == 0:\n",
        "            raise ValueError(\"‚ùå Aucune donn√©e valide apr√®s nettoyage!\")\n",
        "\n",
        "        # 5. √âchantillonnage si n√©cessaire\n",
        "        if len(df_work) > sample_size:\n",
        "            df_work = df_work.sample(n=sample_size, random_state=42)\n",
        "            print(f\"üéØ √âchantillonnage √† {sample_size} lignes\")\n",
        "\n",
        "        # 6. Mapping des labels\n",
        "        unique_labels = sorted(df_work['label'].unique())\n",
        "        print(f\"üè∑Ô∏è Labels uniques trouv√©s: {unique_labels}\")\n",
        "\n",
        "        self.label_mapping = {str(label): idx for idx, label in enumerate(unique_labels)}\n",
        "        df_work['label_id'] = df_work['label'].astype(str).map(self.label_mapping)\n",
        "\n",
        "        # V√©rification du mapping\n",
        "        if df_work['label_id'].isna().any():\n",
        "            print(\"‚ö†Ô∏è Probl√®me de mapping des labels d√©tect√©\")\n",
        "            print(f\"Labels non mapp√©s: {df_work[df_work['label_id'].isna()]['label'].unique()}\")\n",
        "\n",
        "        print(f\"üìä Mapping des labels: {self.label_mapping}\")\n",
        "\n",
        "        # 7. Splits stratifi√©s\n",
        "        try:\n",
        "            # V√©rifier si on peut faire une stratification\n",
        "            if len(unique_labels) > 1 and all(df_work['label_id'].value_counts() >= 2):\n",
        "                stratify_col = df_work['label_id']\n",
        "                print(\"‚úÖ Stratification activ√©e\")\n",
        "            else:\n",
        "                stratify_col = None\n",
        "                print(\"‚ö†Ô∏è Pas de stratification (pas assez d'exemples par classe)\")\n",
        "\n",
        "            # Premier split: train vs (val + test)\n",
        "            train_df, temp_df = train_test_split(\n",
        "                df_work,\n",
        "                test_size=0.4,\n",
        "                random_state=42,\n",
        "                stratify=stratify_col if stratify_col is not None else None\n",
        "            )\n",
        "\n",
        "            # Deuxi√®me split: val vs test\n",
        "            if stratify_col is not None:\n",
        "                temp_stratify = temp_df['label_id']\n",
        "            else:\n",
        "                temp_stratify = None\n",
        "\n",
        "            val_df, test_df = train_test_split(\n",
        "                temp_df,\n",
        "                test_size=0.5,\n",
        "                random_state=42,\n",
        "                stratify=temp_stratify if temp_stratify is not None else None\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du split: {e}\")\n",
        "            # Fallback: split simple\n",
        "            train_size = int(0.6 * len(df_work))\n",
        "            val_size = int(0.2 * len(df_work))\n",
        "\n",
        "            train_df = df_work[:train_size]\n",
        "            val_df = df_work[train_size:train_size+val_size]\n",
        "            test_df = df_work[train_size+val_size:]\n",
        "\n",
        "        print(f\"üìä Splits finaux: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "        # 8. Conversion en Dataset\n",
        "        try:\n",
        "            train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']].reset_index(drop=True))\n",
        "\n",
        "            print(\"‚úÖ Datasets cr√©√©s avec succ√®s\")\n",
        "\n",
        "            return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation des datasets: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques du processeur de donn√©es\"\"\"\n",
        "        return {\n",
        "            \"text_column\": self.text_col,\n",
        "            \"label_column\": self.label_col,\n",
        "            \"label_mapping\": self.label_mapping,\n",
        "            \"num_labels\": len(self.label_mapping)\n",
        "        }\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Validation d'un DataFrame\"\"\"\n",
        "        try:\n",
        "            if df is None or df.empty:\n",
        "                print(\"‚ùå DataFrame vide ou None\")\n",
        "                return False\n",
        "\n",
        "            if len(df.columns) < 2:\n",
        "                print(\"‚ùå DataFrame doit avoir au moins 2 colonnes\")\n",
        "                return False\n",
        "\n",
        "            print(f\"‚úÖ DataFrame valide: {df.shape}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur validation DataFrame: {e}\")\n",
        "            return False"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3a7942-6844-4f10-c889-d16411460c6d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Module Mod√®le - model_modules.py"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "# model_modules.py\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        \"\"\"Charge et configure le tokenizer avec v√©rifications.\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                if self.tokenizer.eos_token is not None:\n",
        "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "                else:\n",
        "                    self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "            print(f\"‚úÖ Tokenizer charg√© : {self.config.model_name}\")\n",
        "            return self.tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors du chargement du tokenizer : {e}\")\n",
        "            raise\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        \"\"\"Charge le mod√®le de base et applique LoRA.\"\"\"\n",
        "        if num_labels < 2:\n",
        "            raise ValueError(\"‚ùå num_labels doit √™tre ‚â• 2 pour la classification.\")\n",
        "\n",
        "        try:\n",
        "            print(f\"üîß Chargement du mod√®le pour {num_labels} classes...\")\n",
        "\n",
        "            base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                self.config.model_name,\n",
        "                num_labels=num_labels,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                problem_type=\"single_label_classification\",\n",
        "            )\n",
        "\n",
        "            target_modules = self.get_target_modules()\n",
        "\n",
        "            lora_config = LoraConfig(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                r=self.config.lora_r,\n",
        "                lora_alpha=self.config.lora_alpha,\n",
        "                lora_dropout=0.1,\n",
        "                target_modules=target_modules,\n",
        "                bias=\"none\",\n",
        "            )\n",
        "\n",
        "            self.peft_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "            trainable_params = sum(p.numel() for p in self.peft_model.parameters() if p.requires_grad)\n",
        "            total_params = sum(p.numel() for p in self.peft_model.parameters())\n",
        "            print(f\"üìä Param√®tres entra√Ænables : {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "            return self.peft_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors du chargement du mod√®le : {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_target_modules(self):\n",
        "        \"\"\"Retourne les modules cibles LoRA selon l'architecture.\"\"\"\n",
        "        model_name_lower = self.config.model_name.lower()\n",
        "\n",
        "        if \"distilbert\" in model_name_lower:\n",
        "            return [\"q_lin\", \"v_lin\"]\n",
        "        elif \"bert\" in model_name_lower:\n",
        "            return [\"query\", \"value\"]\n",
        "        elif \"roberta\" in model_name_lower:\n",
        "            return [\"query\", \"value\"]\n",
        "        else:\n",
        "            warnings.warn(\"‚ö†Ô∏è Mod√®le non reconnu : fallback sur des modules g√©n√©riques.\")\n",
        "            return [\"query\", \"value\", \"dense\"]\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"Tokenisation avec padding dynamique.\"\"\"\n",
        "        if \"text\" not in examples:\n",
        "            raise KeyError(\"‚ùå Cl√© 'text' manquante dans les exemples.\")\n",
        "\n",
        "        assert self.config.max_length > 0, \"‚ùå max_length doit √™tre > 0.\"\n",
        "\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,  # Padding dynamique via DataCollator\n",
        "            max_length=self.config.max_length,\n",
        "        )\n",
        "\n",
        "    def setup_training_args(self, output_dir=\"outputs/runs\"):\n",
        "        \"\"\"Arguments d'entra√Ænement optimis√©s.\"\"\"\n",
        "        return TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            logging_dir=f\"{output_dir}/logs\",\n",
        "\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            gradient_checkpointing=True,\n",
        "            dataloader_num_workers=2,\n",
        "\n",
        "            save_total_limit=2,\n",
        "            save_steps=500,\n",
        "\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            push_to_hub=False,\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        \"\"\"Configure le Trainer avec m√©triques et callbacks.\"\"\"\n",
        "        try:\n",
        "            training_args = self.setup_training_args()\n",
        "            data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "\n",
        "            def compute_metrics(eval_pred):\n",
        "                predictions, labels = eval_pred\n",
        "                preds = np.argmax(predictions, axis=1)\n",
        "                return {\n",
        "                    \"accuracy\": accuracy_score(labels, preds),\n",
        "                    \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                    \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
        "                    \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                    \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                }\n",
        "\n",
        "            callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "            self.trainer = Trainer(\n",
        "                model=self.peft_model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=val_dataset,\n",
        "                tokenizer=self.tokenizer,\n",
        "                compute_metrics=compute_metrics,\n",
        "                data_collator=data_collator,\n",
        "                callbacks=callbacks,\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ Trainer configur√© avec succ√®s\")\n",
        "            return self.trainer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors de la configuration du Trainer : {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778a211b-e2a1-4a64-e3d3-31801fb686ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. visualization_modules.py"
      ],
      "metadata": {
        "id": "qYJhFun3Mws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestion des visualisations d'entra√Ænement et d'√©valuation\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str):\n",
        "        \"\"\"Affiche les courbes d'entra√Ænement depuis les logs\"\"\"\n",
        "        try:\n",
        "            # Chemin vers le fichier de logs\n",
        "            log_file = Path(log_dir) / \"trainer_state.json\"\n",
        "\n",
        "            if not log_file.exists():\n",
        "                st.warning(\"Fichier de logs non trouv√©\")\n",
        "                return\n",
        "\n",
        "            # Chargement des logs\n",
        "            with open(log_file, 'r') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            # Extraction des m√©triques\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"Aucune donn√©e d'entra√Ænement trouv√©e\")\n",
        "                return\n",
        "\n",
        "            # Pr√©paration des donn√©es\n",
        "            epochs = []\n",
        "            train_loss = []\n",
        "            eval_loss = []\n",
        "            eval_accuracy = []\n",
        "            learning_rates = []\n",
        "\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_accuracy.append(entry.get('eval_accuracy', 0))\n",
        "                    learning_rates.append(entry.get('learning_rate', 0))\n",
        "                elif 'loss' in entry:\n",
        "                    train_loss.append(entry.get('loss', 0))\n",
        "\n",
        "            # Cr√©ation des graphiques\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle('üìà √âvolution de l\\'entra√Ænement', fontsize=16)\n",
        "\n",
        "            # Loss\n",
        "            if train_loss:\n",
        "                axes[0, 0].plot(range(len(train_loss)), train_loss, 'b-', label='Train Loss', marker='o')\n",
        "            if eval_loss:\n",
        "                axes[0, 0].plot(epochs[:len(eval_loss)], eval_loss, 'r-', label='Eval Loss', marker='s')\n",
        "            axes[0, 0].set_title('Perte (Loss)')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Accuracy\n",
        "            if eval_accuracy:\n",
        "                axes[0, 1].plot(epochs[:len(eval_accuracy)], eval_accuracy, 'g-', label='Accuracy', marker='^')\n",
        "            axes[0, 1].set_title('Pr√©cision')\n",
        "            axes[0, 1].set_xlabel('Epoch')\n",
        "            axes[0, 1].set_ylabel('Accuracy')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # Learning Rate\n",
        "            if learning_rates:\n",
        "                axes[1, 0].plot(epochs[:len(learning_rates)], learning_rates, 'orange', marker='d')\n",
        "            axes[1, 0].set_title('Learning Rate')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('LR')\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # R√©sum√©\n",
        "            if eval_accuracy:\n",
        "                axes[1, 1].text(0.1, 0.5,\n",
        "                               f\"Derni√®re pr√©cision: {eval_accuracy[-1]:.4f}\\n\"\n",
        "                               f\"Meilleure pr√©cision: {max(eval_accuracy):.4f}\\n\"\n",
        "                               f\"Epoch: {len(epochs)}\",\n",
        "                               fontsize=12, verticalalignment='center')\n",
        "            axes[1, 1].set_title('R√©sum√©')\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur lors de l'affichage des courbes: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names):\n",
        "        \"\"\"Affiche la matrice de confusion\"\"\"\n",
        "        try:\n",
        "            # Pr√©dictions\n",
        "            preds_output = trainer.predict(test_dataset)\n",
        "            preds = preds_output.predictions.argmax(axis=1)\n",
        "            labels = preds_output.label_ids\n",
        "\n",
        "            # Calcul de la matrice\n",
        "            cm = confusion_matrix(labels, preds)\n",
        "\n",
        "            # Affichage avec seaborn\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax)\n",
        "            ax.set_title('Matrice de Confusion')\n",
        "            ax.set_xlabel('Pr√©dictions')\n",
        "            ax.set_ylabel('Vraies √©tiquettes')\n",
        "\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Rapport de classification\n",
        "            st.subheader(\"üìä Rapport de Classification\")\n",
        "            report = classification_report(labels, preds, target_names=label_names, output_dict=True)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.dataframe(report_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur matrice de confusion: {e}\")"
      ],
      "metadata": {
        "id": "oMZt-WwIMwez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1432eb1-d69c-40b4-9f03-e100e223ca3c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting visualization_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. qa_modules.py"
      ],
      "metadata": {
        "id": "1HSXVYNkNc-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import streamlit as st\n",
        "\n",
        "class QAModule:\n",
        "    \"\"\"Module de recherche et Q&A bas√© sur sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"Initialisation avec mod√®le d'embedding\"\"\"\n",
        "        try:\n",
        "            self.encoder = SentenceTransformer(model_name)\n",
        "            self.corpus_embeddings = None\n",
        "            self.corpus_texts = []\n",
        "            self.labels = []\n",
        "            self.model_name = model_name\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur chargement mod√®le Q&A: {e}\")\n",
        "            # Fallback\n",
        "            self.encoder = None\n",
        "            self.model_name = \"fallback\"\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        \"\"\"Indexe le dataset pour la recherche\"\"\"\n",
        "        if self.encoder is None:\n",
        "            st.warning(\"Module Q&A non disponible\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.corpus_texts = [item['text'] for item in dataset]\n",
        "            self.labels = [item['label_id'] for item in dataset]\n",
        "\n",
        "            with st.spinner(\"üìä Indexation des donn√©es pour la recherche...\"):\n",
        "                self.corpus_embeddings = self.encoder.encode(\n",
        "                    self.corpus_texts,\n",
        "                    convert_to_tensor=False,\n",
        "                    show_progress_bar=True\n",
        "                )\n",
        "            st.success(f\"‚úÖ {len(self.corpus_texts)} √©l√©ments index√©s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur indexation Q&A: {e}\")\n",
        "\n",
        "    def query(self, question: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Recherche les textes les plus similaires √† la question\"\"\"\n",
        "        if self.encoder is None or self.corpus_embeddings is None:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            question_embedding = self.encoder.encode([question], convert_to_tensor=False)\n",
        "            similarities = cosine_similarity(question_embedding, self.corpus_embeddings)[0]\n",
        "\n",
        "            # Top K indices\n",
        "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "            results = []\n",
        "            for idx in top_indices:\n",
        "                results.append({\n",
        "                    \"text\": self.corpus_texts[idx],\n",
        "                    \"label_id\": int(self.labels[idx]),\n",
        "                    \"score\": float(similarities[idx]),\n",
        "                    \"rank\": len(results) + 1\n",
        "                })\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur recherche Q&A: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Statistiques du module Q&A\"\"\"\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"indexed_items\": len(self.corpus_texts),\n",
        "            \"embedding_dim\": len(self.corpus_embeddings[0]) if self.corpus_embeddings else 0\n",
        "        }"
      ],
      "metadata": {
        "id": "CdRC_3s7Ncy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3370e1f6-aa81-471b-aa52-a9f68318bcd9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting qa_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Module Knowledge Base - knowledge_modules.py"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le r√©chauffement climatique est principalement caus√© par les √©missions de gaz √† effet de serre d'origine humaine.\",\n",
        "            \"Les √©nergies renouvelables comme le solaire et l'√©olien sont essentielles pour d√©carboner notre √©conomie.\",\n",
        "            \"La d√©forestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports repr√©sente environ 24% des √©missions mondiales de gaz √† effet de serre.\",\n",
        "            \"L'am√©lioration de l'efficacit√© √©nerg√©tique des b√¢timents peut r√©duire jusqu'√† 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et r√©g√©n√©ratrice peut s√©questrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les oc√©ans absorbent 25% du CO2 atmosph√©rique mais s'acidifient, mena√ßant les √©cosyst√®mes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises √† r√©duire leurs √©missions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'att√©nuation des √©missions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralit√© carbone.\"\n",
        "        ]\n",
        "        print(\"‚úÖ Base de connaissances initialis√©e avec recherche par mots-cl√©s\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarit√© textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarit√© bas√© sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score d√©croissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"‚úÖ Nouvelle connaissance ajout√©e: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b16390-6d39-45ed-8220-b00e5fa20dfa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5Ô∏è‚É£ Module Streamlit - streamlit_app.py"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "# streamlit_app.py\n",
        "# streamlit_app_fixed.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Ajout du chemin\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig, PredictionResult\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from knowledge_modules import KnowledgeBase\n",
        "from visualization_modules import VisualizationManager\n",
        "from qa_modules import QAModule\n",
        "\n",
        "st.set_page_config(page_title=\"üåç Climate Analyzer ‚Äì Complet\", page_icon=\"üåç\", layout=\"wide\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>.main-header{background:linear-gradient(135deg,#667eea,#764ba2);padding:2rem;border-radius:15px;color:white;text-align:center}</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "        self.knowledge_base = KnowledgeBase()\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.qa_module = QAModule()\n",
        "        self.trained = False\n",
        "        self.trainer = None\n",
        "        # Stocker les datasets pour √©viter les erreurs NoneType\n",
        "        self.train_ds = None\n",
        "        self.val_ds = None\n",
        "        self.test_ds = None\n",
        "\n",
        "    def run(self):\n",
        "        st.markdown('<div class=\"main-header\"><h1>üåç Climate Sentiment Analyzer</h1><h3>Pipeline Complet</h3></div>', unsafe_allow_html=True)\n",
        "        mode = st.sidebar.selectbox(\"Mode\", [\"üöÄ Pipeline Complet\", \"üìä Data Processing\", \"‚ùì Q&A\", \"üìà Visualisations\"])\n",
        "\n",
        "        if mode == \"üöÄ Pipeline Complet\":\n",
        "            self.run_complete_pipeline()\n",
        "        elif mode == \"üìä Data Processing\":\n",
        "            self.run_data_processing()\n",
        "        elif mode == \"‚ùì Q&A\":\n",
        "            self.run_qa_interface()\n",
        "        elif mode == \"üìà Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        st.header(\"üöÄ Pipeline Complet\")\n",
        "        uploaded_file = st.file_uploader(\"T√©l√©chargez votre fichier CSV\", type=[\"csv\"])\n",
        "\n",
        "        if uploaded_file:\n",
        "            try:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "                st.success(f\"‚úÖ Fichier charg√© : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
        "                st.dataframe(df.head())\n",
        "\n",
        "                # Afficher les colonnes d√©tect√©es\n",
        "                if hasattr(self.data_processor, 'text_col') and self.data_processor.text_col:\n",
        "                    st.info(f\"üìù Colonne texte d√©tect√©e: {self.data_processor.text_col}\")\n",
        "                    st.info(f\"üè∑Ô∏è Colonne label d√©tect√©e: {self.data_processor.label_col}\")\n",
        "\n",
        "                sample_size = st.slider(\"Taille √©chantillon\", 1000, 10000, 4000)\n",
        "                epochs = st.slider(\"Epochs\", 1, 5, 3)\n",
        "                self.config.epochs = epochs\n",
        "\n",
        "                if st.button(\"üöÄ Lancer l'entra√Ænement\", type=\"primary\"):\n",
        "                    self.run_real_training(df, sample_size)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå Erreur lors du chargement du fichier: {e}\")\n",
        "\n",
        "    def run_real_training(self, df, sample_size):\n",
        "        \"\"\"Version corrig√©e de l'entra√Ænement\"\"\"\n",
        "        progress = st.progress(0)\n",
        "        status = st.empty()\n",
        "\n",
        "        try:\n",
        "            # 1. Pr√©paration des donn√©es\n",
        "            status.text(\"üìä Pr√©paration des donn√©es...\")\n",
        "            self.train_ds, self.val_ds, self.test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "            progress.progress(20)\n",
        "\n",
        "            st.success(f\"‚úÖ Donn√©es pr√©par√©es: Train={len(self.train_ds)}, Val={len(self.val_ds)}, Test={len(self.test_ds)}\")\n",
        "\n",
        "            # 2. Configuration du mod√®le\n",
        "            status.text(\"ü§ñ Configuration du mod√®le...\")\n",
        "            self.model_manager.setup_tokenizer()\n",
        "            num_labels = len(self.data_processor.label_mapping)\n",
        "            self.model_manager.setup_model(num_labels)\n",
        "            progress.progress(40)\n",
        "\n",
        "            # 3. Tokenisation (CORRECTION MAJEURE)\n",
        "            status.text(\"üî§ Tokenisation des donn√©es...\")\n",
        "\n",
        "            # Tokenisation avec r√©assignation correcte\n",
        "            self.train_ds = self.train_ds.map(\n",
        "                self.model_manager.tokenize_function,\n",
        "                batched=True,\n",
        "                remove_columns=['text']  # Supprimer l'ancienne colonne texte\n",
        "            )\n",
        "            self.val_ds = self.val_ds.map(\n",
        "                self.model_manager.tokenize_function,\n",
        "                batched=True,\n",
        "                remove_columns=['text']\n",
        "            )\n",
        "            self.test_ds = self.test_ds.map(\n",
        "                self.model_manager.tokenize_function,\n",
        "                batched=True,\n",
        "                remove_columns=['text']\n",
        "            )\n",
        "\n",
        "            # Renommer la colonne label_id en labels\n",
        "            self.train_ds = self.train_ds.rename_column(\"label_id\", \"labels\")\n",
        "            self.val_ds = self.val_ds.rename_column(\"label_id\", \"labels\")\n",
        "            self.test_ds = self.test_ds.rename_column(\"label_id\", \"labels\")\n",
        "\n",
        "            # D√©finir le format PyTorch\n",
        "            self.train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "            self.val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "            self.test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "            progress.progress(60)\n",
        "\n",
        "            # 4. Configuration du trainer\n",
        "            status.text(\"‚öôÔ∏è Configuration du trainer...\")\n",
        "            self.trainer = self.model_manager.setup_trainer(self.train_ds, self.val_ds)\n",
        "            progress.progress(70)\n",
        "\n",
        "            # 5. Entra√Ænement\n",
        "            status.text(\"üéØ Entra√Ænement en cours...\")\n",
        "            with st.spinner(\"Entra√Ænement du mod√®le...\"):\n",
        "                self.trainer.train()\n",
        "            progress.progress(90)\n",
        "\n",
        "            # 6. √âvaluation\n",
        "            status.text(\"üìä √âvaluation...\")\n",
        "            metrics = self.trainer.evaluate(self.test_ds)\n",
        "\n",
        "            # 7. Configuration Q&A\n",
        "            try:\n",
        "                # Convertir le dataset pour Q&A (sans tokenisation)\n",
        "                qa_data = []\n",
        "                for item in self.data_processor.prepare_datasets(df, sample_size)[0]:  # Utiliser train original\n",
        "                    qa_data.append({\n",
        "                        'text': item['text'],\n",
        "                        'label_id': item['label_id']\n",
        "                    })\n",
        "                self.qa_module.fit(qa_data)\n",
        "            except Exception as e:\n",
        "                st.warning(f\"‚ö†Ô∏è Q&A non disponible: {e}\")\n",
        "\n",
        "            progress.progress(100)\n",
        "            status.text(\"‚úÖ Entra√Ænement termin√©!\")\n",
        "\n",
        "            # 8. Affichage des r√©sultats\n",
        "            self.display_training_results(metrics)\n",
        "            self.trained = True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur d'entra√Ænement: {e}\")\n",
        "            st.code(f\"D√©tails de l'erreur:\\n{str(e)}\")\n",
        "            import traceback\n",
        "            st.code(traceback.format_exc())\n",
        "\n",
        "    def display_training_results(self, metrics):\n",
        "        \"\"\"Affichage des r√©sultats d'entra√Ænement\"\"\"\n",
        "        st.success(\"üéâ Entra√Ænement termin√© avec succ√®s!\")\n",
        "\n",
        "        # M√©triques\n",
        "        cols = st.columns(4)\n",
        "        cols[0].metric(\"Accuracy\", f\"{metrics.get('eval_accuracy', 0):.4f}\")\n",
        "        cols[1].metric(\"F1\", f\"{metrics.get('eval_f1_weighted', 0):.4f}\")\n",
        "        cols[2].metric(\"Precision\", f\"{metrics.get('eval_precision', 0):.4f}\")\n",
        "        cols[3].metric(\"Recall\", f\"{metrics.get('eval_recall', 0):.4f}\")\n",
        "\n",
        "        # Visualisations\n",
        "        try:\n",
        "            self.visualizer.plot_training_curves(\"outputs/runs/logs\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"‚ö†Ô∏è Impossible d'afficher les courbes: {e}\")\n",
        "\n",
        "        # Matrice de confusion\n",
        "        try:\n",
        "            labels = list(self.data_processor.label_mapping.keys())\n",
        "            self.visualizer.show_confusion_matrix(self.trainer, self.test_ds, labels)\n",
        "        except Exception as e:\n",
        "            st.warning(f\"‚ö†Ô∏è Impossible d'afficher la matrice: {e}\")\n",
        "\n",
        "        # Sauvegarde\n",
        "        try:\n",
        "            model_path = \"outputs/final_model\"\n",
        "            os.makedirs(model_path, exist_ok=True)\n",
        "            self.trainer.save_model(model_path)\n",
        "            st.success(f\"üì¶ Mod√®le sauvegard√© dans `{model_path}`\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"‚ö†Ô∏è Erreur de sauvegarde: {e}\")\n",
        "\n",
        "    def run_data_processing(self):\n",
        "        st.header(\"üìä Data Processing\")\n",
        "        uploaded_file = st.file_uploader(\"T√©l√©chargez votre fichier CSV pour analyse\", type=[\"csv\"])\n",
        "\n",
        "        if uploaded_file:\n",
        "            try:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "                st.success(f\"‚úÖ Fichier charg√©: {df.shape}\")\n",
        "\n",
        "                # Aper√ßu des donn√©es\n",
        "                st.subheader(\"Aper√ßu des donn√©es\")\n",
        "                st.dataframe(df.head())\n",
        "\n",
        "                # Pr√©paration des datasets\n",
        "                train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df)\n",
        "\n",
        "                # Statistiques\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                col1.metric(\"Train\", len(train_ds))\n",
        "                col2.metric(\"Validation\", len(val_ds))\n",
        "                col3.metric(\"Test\", len(test_ds))\n",
        "\n",
        "                # Informations sur les colonnes\n",
        "                stats = self.data_processor.get_stats()\n",
        "                st.json(stats)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå Erreur de traitement: {e}\")\n",
        "\n",
        "    def run_qa_interface(self):\n",
        "        st.header(\"‚ùì Interface Q&A\")\n",
        "\n",
        "        if not self.trained:\n",
        "            st.warning(\"‚ö†Ô∏è Veuillez d'abord entra√Æner un mod√®le dans le mode 'Pipeline Complet'.\")\n",
        "            return\n",
        "\n",
        "        st.info(\"üí° Posez une question sur le climat ou l'environnement\")\n",
        "\n",
        "        question = st.text_input(\"Votre question:\", placeholder=\"Ex: Quelles sont les causes du r√©chauffement climatique?\")\n",
        "\n",
        "        if question:\n",
        "            with st.spinner(\"Recherche en cours...\"):\n",
        "                try:\n",
        "                    results = self.qa_module.query(question, top_k=5)\n",
        "\n",
        "                    if results:\n",
        "                        st.subheader(\"üìã R√©sultats de recherche\")\n",
        "                        for i, result in enumerate(results, 1):\n",
        "                            with st.expander(f\"R√©sultat {i} - Score: {result['score']:.3f}\"):\n",
        "                                st.write(f\"**Texte:** {result['text']}\")\n",
        "                                st.write(f\"**Label ID:** {result['label_id']}\")\n",
        "                    else:\n",
        "                        st.warning(\"Aucun r√©sultat trouv√© pour votre question.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"‚ùå Erreur Q&A: {e}\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        st.header(\"üìà Visualisations\")\n",
        "\n",
        "        if not self.trained:\n",
        "            st.warning(\"‚ö†Ô∏è Aucune donn√©e d'entra√Ænement disponible. Entra√Ænez d'abord un mod√®le.\")\n",
        "            return\n",
        "\n",
        "        viz_option = st.selectbox(\n",
        "            \"Choisir le type de visualisation:\",\n",
        "            [\"Courbes d'entra√Ænement\", \"Matrice de confusion\", \"M√©triques d√©taill√©es\"]\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            if viz_option == \"Courbes d'entra√Ænement\":\n",
        "                self.visualizer.plot_training_curves(\"outputs/runs/logs\")\n",
        "\n",
        "            elif viz_option == \"Matrice de confusion\" and self.test_ds:\n",
        "                labels = list(self.data_processor.label_mapping.keys())\n",
        "                self.visualizer.show_confusion_matrix(self.trainer, self.test_ds, labels)\n",
        "\n",
        "            elif viz_option == \"M√©triques d√©taill√©es\":\n",
        "                if self.trainer and self.test_ds:\n",
        "                    metrics = self.trainer.evaluate(self.test_ds)\n",
        "                    st.json(metrics)\n",
        "                else:\n",
        "                    st.warning(\"Donn√©es d'√©valuation non disponibles\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur de visualisation: {e}\")\n",
        "\n",
        "class PipelineOrchestrator:\n",
        "    def __init__(self):\n",
        "        self.app = ClimateAnalyzerApp()\n",
        "\n",
        "    def run(self):\n",
        "        self.app.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    orchestrator = PipelineOrchestrator()\n",
        "    orchestrator.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a267692c-dce9-44cf-e909-7fd47ef6d369"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6Ô∏è‚É£ Script d'Installation - setup_pipeline.py"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_pipeline.py\n",
        "# setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installation compl√®te des d√©pendances\"\"\"\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"‚úÖ {package} install√©\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur avec {package}: {e}\")\n",
        "\n",
        "    print(\"‚úÖ Installation compl√®te termin√©e!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25aa3d9-5a24-4953-801d-003d7b62257c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167f80e4-a8d7-4143-e35d-2bea3f0e28f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "‚úÖ transformers>=4.36.0 install√©\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "‚úÖ datasets>=2.16.0 install√©\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "‚úÖ torch>=2.1.0 install√©\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "‚úÖ peft>=0.7.0 install√©\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.33.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "‚úÖ sentence-transformers>=2.2.0 install√©\n",
            "Requirement already satisfied: faiss-cpu>=1.7.0 in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "‚úÖ faiss-cpu>=1.7.0 install√©\n",
            "Requirement already satisfied: streamlit>=1.29.0 in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "‚úÖ streamlit>=1.29.0 install√©\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "‚úÖ plotly>=5.17.0 install√©\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "‚úÖ scikit-learn>=1.3.0 install√©\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "‚úÖ matplotlib>=3.7.0 install√©\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "‚úÖ seaborn>=0.12.0 install√©\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "‚úÖ pandas>=1.5.0 install√©\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "‚úÖ numpy>=1.24.0 install√©\n",
            "‚úÖ Installation compl√®te termin√©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MEnnZWsOuL5",
        "outputId": "4e42647f-d14d-4315-98d2-6c22f059d838"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3wskx_fZLEp",
        "outputId": "2357d833-be78-4feb-d262-91720e4e56b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Lancement Streamlit + ngrok (version corrig√©e)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1Ô∏è‚É£ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2Ô∏è‚É£ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Attendre et cr√©er le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üöÄ Interface Streamlit disponible √† :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFyKJzjISjWG",
        "outputId": "de60a287-19b7-4636-ba6b-06e74f1dcf2b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "üöÄ Interface Streamlit disponible √† :\n",
            "NgrokTunnel: \"https://9170c262d24a.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}